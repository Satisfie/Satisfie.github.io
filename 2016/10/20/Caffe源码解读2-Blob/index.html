<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="caffe," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。在Blob中主要定义了关于数据data_和梯度diff_以及相关的一系列方法,使用的变量都是SyncedMemory的智能指针，所以在解读Blob之前，需要先看上一篇的SyncedMemory解读
私有保护变量1234567protected: shared_ptr&amp;lt;SyncedMemory&amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="Caffe源码解读2--Blob">
<meta property="og:url" content="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/index.html">
<meta property="og:site_name" content="Keson's blog">
<meta property="og:description" content="Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。在Blob中主要定义了关于数据data_和梯度diff_以及相关的一系列方法,使用的变量都是SyncedMemory的智能指针，所以在解读Blob之前，需要先看上一篇的SyncedMemory解读
私有保护变量1234567protected: shared_ptr&amp;lt;SyncedMemory&amp;">
<meta property="og:updated_time" content="2016-12-15T13:42:50.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Caffe源码解读2--Blob">
<meta name="twitter:description" content="Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。在Blob中主要定义了关于数据data_和梯度diff_以及相关的一系列方法,使用的变量都是SyncedMemory的智能指针，所以在解读Blob之前，需要先看上一篇的SyncedMemory解读
私有保护变量1234567protected: shared_ptr&amp;lt;SyncedMemory&amp;">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/"/>





  <title> Caffe源码解读2--Blob | Keson's blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Keson's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Caffe源码解读2--Blob
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-20T16:50:01+08:00">
              2016-10-20
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-15T21:42:50+08:00">
              2016-12-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/20/Caffe源码解读2-Blob/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/10/20/Caffe源码解读2-Blob/" class="leancloud_visitors" data-flag-title="Caffe源码解读2--Blob">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。在Blob中主要定义了关于数据<code>data_</code>和梯度<code>diff_</code>以及相关的一系列方法,使用的变量都是<code>SyncedMemory</code>的智能指针，所以在解读Blob之前，需要先看上一篇的<a href="">SyncedMemory解读</a></p>
<h1 id="私有保护变量"><a href="#私有保护变量" class="headerlink" title="私有保护变量"></a>私有保护变量</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_;</div><div class="line"> <span class="keyword">int</span> count_;</div><div class="line"> <span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure>
<p>shared_ptr是智能指针，使用引用计数，当计数为0时，自动释放内存。其中</p>
<ul>
<li>data_ : 用来存放正向传播时的权重系数</li>
<li>diff_ : 用来存放反向传播时的梯度差</li>
<li>shape_data_: 用来存储Blob的形状数据的</li>
<li>shape_ : 用来存储Blob的形状数据</li>
<li>count_ : 表示Blob中的元素个数，等于 $num \times channels \times height \times width$</li>
<li>capacity_ : 表示Blob的容量</li>
</ul>
<h1 id="Reshape函数"><a href="#Reshape函数" class="headerlink" title="Reshape函数"></a>Reshape函数</h1><p><code>Reshape</code>能够被调用用来实现 （1）内存的初始化分配；（2）在<code>Layer::Reshape</code>或者<code>Layer::Forward</code>时 用来调节top blob的维度。 当改变blob的size时，只有当原来的内存已经不够了才会重新分配，而创建后多余的内存是不会释放的。需要注意的是，当对输入blob进行<code>reshape</code>时，不能马上调用<code>Net::Backward</code>,因为需要在进行<br><code>reshape</code>后需要调用<code>Net::Forward</code>或者<code>Net::Reshape</code>将新的输入shape传递到更高的层。</p>
<p>Reshape成员函数有4种:</p>
<ul>
<li><code>void Reshape(const int num, const int channels, const int height,const int width);</code></li>
<li><code>void Reshape(const vector&lt;int&gt;&amp; shape);</code></li>
<li><code>void Reshape(const BlobShape&amp; shape);</code></li>
<li><code>void ReshapeLike(const Blob&amp; other);</code></li>
</ul>
<h2 id="Reshape成员函数1"><a href="#Reshape成员函数1" class="headerlink" title="Reshape成员函数1"></a>Reshape成员函数1</h2><p> <code>Reshape(const int num, const int channels, const int height,const int width)</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接用num,channels,height,width来完成reshape,调用了Reshape(const vector&lt;int&gt; &amp;shape)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width) &#123;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape(<span class="number">4</span>);</div><div class="line">  shape[<span class="number">0</span>] = num;</div><div class="line">  shape[<span class="number">1</span>] = channels;</div><div class="line">  shape[<span class="number">2</span>] = height;</div><div class="line">  shape[<span class="number">3</span>] = width;</div><div class="line">  Reshape(shape);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数2"><a href="#Reshape成员函数2" class="headerlink" title="Reshape成员函数2"></a>Reshape成员函数2</h2><p><code>Reshape(const vector&lt;int&gt;&amp; shape)</code>，重点Reshape函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape) &#123;</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * CHECK_LE 位于logging.h文件 767行</div><div class="line">   * 定义为：</div><div class="line">   * #define CHECK_LE(val1, val2) CHECK_OP(_LE, &lt;=, val1, val2)</div><div class="line">   * 用来检查val1&lt;=val2, 用到了GLOG日志库</div><div class="line">   */</div><div class="line">  CHECK_LE(shape.size(), kMaxBlobAxes); <span class="comment">//kMaxBlobAxes定义为shape参数最大的个数，设定为32</span></div><div class="line">  count_ = <span class="number">1</span>; <span class="comment">//开始初始化时赋值为1，因为后面要乘以shape_中的每个元素值</span></div><div class="line">  shape_.resize(shape.size());<span class="comment">//shape_ 开始初始化</span></div><div class="line"> </div><div class="line">  <span class="comment">//shape_data_的初始化和赋值，它是一个SyncedMemory类指针</span></div><div class="line">  <span class="keyword">if</span> (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)) &#123;</div><div class="line">    shape_data_.reset(<span class="keyword">new</span> SyncedMemory(shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">int</span>* shape_data = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>*&gt;(shape_data_-&gt;mutable_cpu_data());<span class="comment">//获得shape_data_的cpu内存地址</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.size(); ++i) &#123;</div><div class="line">    CHECK_GE(shape[i], <span class="number">0</span>); <span class="comment">//检查shape中该参数是否为0</span></div><div class="line">    <span class="keyword">if</span> (count_ != <span class="number">0</span>) &#123;    </div><div class="line">      <span class="comment">//检查乘以shape[i]后，count_是否会超过INT_MAX</span></div><div class="line">      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; <span class="string">"blob size exceeds INT_MAX"</span>; </div><div class="line">    &#125;</div><div class="line">    count_ *= shape[i]; <span class="comment">//统计Blob元素个数= num*channels*height*width</span></div><div class="line">    shape_[i] = shape[i]; <span class="comment">//给成员变量shape_ 赋值</span></div><div class="line">    shape_data[i] = shape[i]; <span class="comment">//给shape_data_赋值</span></div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">//超过容量，设定容量为count_</span></div><div class="line">  <span class="keyword">if</span> (count_ &gt; capacity_) &#123;</div><div class="line">    capacity_ = count_;</div><div class="line">    data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//data_ 进行内存分配</span></div><div class="line">    diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//diff_ 进行内存分配</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数3"><a href="#Reshape成员函数3" class="headerlink" title="Reshape成员函数3"></a>Reshape成员函数3</h2><p><code>Reshape(const BlobShape&amp; shape)</code> 用<code>BlobShape</code>来进行Reshape, <code>BlobShape</code>是在caffe.proto中定义的，用来定义Blob的shape维度的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> BlobShape&amp; shape) &#123;</div><div class="line">  CHECK_LE(shape.dim_size(), kMaxBlobAxes);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_vec(shape.dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.dim_size(); ++i) &#123;</div><div class="line">    shape_vec[i] = shape.dim(i);</div><div class="line">  &#125;</div><div class="line">  Reshape(shape_vec);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数4"><a href="#Reshape成员函数4" class="headerlink" title="Reshape成员函数4"></a>Reshape成员函数4</h2><p><code>ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)</code>，用其他的Blob参数进行Reshape</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ReshapeLike(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; other) &#123;</div><div class="line">  Reshape(other.shape());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="shape数据输出函数"><a href="#shape数据输出函数" class="headerlink" title="shape数据输出函数"></a>shape数据输出函数</h2><p>两个内敛函数用来输出shape的形状数据</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="built_in">string</span> <span class="title">shape_string</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="built_in">ostringstream</span> stream;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</div><div class="line">    stream &lt;&lt; shape_[i] &lt;&lt; <span class="string">" "</span>;</div><div class="line">  &#125;</div><div class="line">  stream &lt;&lt; <span class="string">"("</span> &lt;&lt; count_ &lt;&lt; <span class="string">")"</span>;</div><div class="line">  <span class="keyword">return</span> stream.str();</div><div class="line">&#125;</div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape() <span class="keyword">const</span> &#123; <span class="keyword">return</span> shape_; &#125;</div></pre></td></tr></table></figure>
<h1 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h1><p>总共声明了3种构造函数：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Blob(): data_(), diff_(), count_(<span class="number">0</span>), capacity_(<span class="number">0</span>) &#123;&#125;</div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width);   </div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</div></pre></td></tr></table></figure>
<p>explict关键字可以防止构造函数的隐式转换,构造函数的实现主要是调用了<code>Reshape</code>函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(num, channels, height, width);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(shape);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="一些变量访问函数"><a href="#一些变量访问函数" class="headerlink" title="一些变量访问函数"></a>一些变量访问函数</h1><ul>
<li><code>num_axes()</code>: 返回shape_ 的size</li>
<li><code>count()</code>:返回count_</li>
<li><code>CanonicalAxisIndex(int axis_index)</code>：返回规范化的坐标，支持负坐标的访问</li>
<li><code>shape(int index)</code>:返回shape_索引处的值</li>
<li><code>LegacyShape(int index)</code>:内部调用shape(intdex)，多了一些合法性检查</li>
<li><code>num()</code>,<code>channels()</code>,<code>height()</code>,<code>width()</code>:分别返回对应的值</li>
</ul>
<h2 id="num-axes"><a href="#num-axes" class="headerlink" title="num_axes()"></a><code>num_axes()</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_axes</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> shape_.size(); &#125;</div></pre></td></tr></table></figure>
<h2 id="count"><a href="#count" class="headerlink" title="count()"></a><code>count()</code></h2><p>这个函数有多个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接返回count_</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> count_; &#125; </div><div class="line"><span class="comment">//计算一个片内的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis, <span class="keyword">int</span> end_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">CHECK_LE(start_axis, end_axis);</div><div class="line">CHECK_GE(start_axis, <span class="number">0</span>);</div><div class="line">CHECK_GE(end_axis, <span class="number">0</span>);</div><div class="line">CHECK_LE(start_axis, num_axes());</div><div class="line">CHECK_LE(end_axis, num_axes());</div><div class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = start_axis; i &lt; end_axis; ++i) &#123;</div><div class="line">  count *= shape(i);</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> count;</div><div class="line">&#125;</div><div class="line"><span class="comment">//给定一个起始，计算到最后片的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">   <span class="keyword">return</span> count(start_axis, num_axes());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="CanonicalAxisIndex"><a href="#CanonicalAxisIndex" class="headerlink" title="CanonicalAxisIndex()"></a><code>CanonicalAxisIndex()</code></h2><p>用来进行坐标的规范化，和Python一样，支持负数的访问。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">CanonicalAxisIndex</span><span class="params">(<span class="keyword">int</span> axis_index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_GE(axis_index, -num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  CHECK_LT(axis_index, num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  <span class="keyword">if</span> (axis_index &lt; <span class="number">0</span>) &#123;</div><div class="line">    <span class="keyword">return</span> axis_index + num_axes();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> axis_index;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="int-shape-int-intdex"><a href="#int-shape-int-intdex" class="headerlink" title="int shape(int intdex)"></a><code>int shape(int intdex)</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">shape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> shape_[CanonicalAxisIndex(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="LegacyShape-int-index"><a href="#LegacyShape-int-index" class="headerlink" title="LegacyShape(int index)"></a><code>LegacyShape(int index)</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">LegacyShape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(num_axes(), <span class="number">4</span>)</div><div class="line">      &lt;&lt; <span class="string">"Cannot use legacy accessors on Blobs with &gt; 4 axes."</span>;</div><div class="line">  CHECK_LT(index, <span class="number">4</span>);</div><div class="line">  CHECK_GE(index, <span class="number">-4</span>);</div><div class="line">  <span class="keyword">if</span> (index &gt;= num_axes() || index &lt; -num_axes()) &#123;</div><div class="line">    <span class="comment">// Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse</span></div><div class="line">    <span class="comment">// indexing) -- this special case simulates the one-padding used to fill</span></div><div class="line">    <span class="comment">// extraneous axes of legacy blobs.</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape(index);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="num-channels-height-width"><a href="#num-channels-height-width" class="headerlink" title="num(),channels(),height(),width()"></a><code>num()</code>,<code>channels()</code>,<code>height()</code>,<code>width()</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor num: use shape(0) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">0</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor channels: use shape(1) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">channels</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">1</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor height: use shape(2) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">2</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor width: use shape(3) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">width</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">3</span>); &#125;</div></pre></td></tr></table></figure>
<h1 id="CPU和GPU中数据的获得"><a href="#CPU和GPU中数据的获得" class="headerlink" title="CPU和GPU中数据的获得"></a>CPU和GPU中数据的获得</h1><p>以下是一些get和set函数</p>
<ul>
<li><code>const Dtype* cpu_data() const;</code></li>
<li><code>void set_cpu_data(Dtype* data);</code></li>
<li><code>const int* gpu_shape() const;</code></li>
<li><code>const Dtype* gpu_data() const;</code></li>
<li><code>const Dtype* cpu_diff() const;</code></li>
<li><code>const Dtype* gpu_diff() const;</code></li>
<li><code>Dtype* mutable_cpu_data();</code></li>
<li><code>Dtype* mutable_gpu_data();</code></li>
<li><code>Dtype* mutable_cpu_diff();</code></li>
<li><code>Dtype* mutable_gpu_diff();</code></li>
</ul>
<h2 id="cpu-data"><a href="#cpu-data" class="headerlink" title="cpu_data()"></a><code>cpu_data()</code></h2><p>返回数据<code>data_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="set-cpu-data"><a href="#set-cpu-data" class="headerlink" title="set_cpu_data()"></a><code>set_cpu_data()</code></h2><p>设定<code>data_</code>在cpu中的数据，直接用指针替换的方式</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) &#123;</div><div class="line">  CHECK(data);</div><div class="line">  data_-&gt;set_cpu_data(data); <span class="comment">//在Syscedmem.cpp中实现，用cpu_ptr_ = data实现替换</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-shape"><a href="#gpu-shape" class="headerlink" title="gpu_shape"></a><code>gpu_shape</code></h2><p>返回的是shape_data_中存储的gpu数据</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span>* Blob&lt;Dtype&gt;::gpu_shape() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(shape_data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">int</span>*)shape_data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-data"><a href="#gpu-data" class="headerlink" title="gpu_data()"></a><code>gpu_data()</code></h2><p>返回数据<code>data_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="cpu-diff"><a href="#cpu-diff" class="headerlink" title="cpu_diff()"></a><code>cpu_diff()</code></h2><p>返回梯度<code>diff_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-diff"><a href="#gpu-diff" class="headerlink" title="gpu_diff()"></a><code>gpu_diff()</code></h2><p>返回梯度<code>diff_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="mutable版本的"><a href="#mutable版本的" class="headerlink" title="mutable版本的"></a>mutable版本的</h2><p>下面这4个函数与上面类似，不同之处在于mutable，可以对其进行修改，而当面的返回形式是const，不可修改的</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="具体offset位置处的访问"><a href="#具体offset位置处的访问" class="headerlink" title="具体offset位置处的访问"></a>具体offset位置处的访问</h1><p>先需要用<code>offset()</code>函数计算具体的位置index，然后对<code>data_</code>和<code>diff_</code>具体index处进行访问,主要的函数有</p>
<ul>
<li><code>offset(const int n, const int c = 0, const int h = 0, const int w = 0)</code></li>
<li><code>offset(const vector&lt;int&gt;&amp; indices)</code></li>
<li><code>data_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>diff_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>data_at(const vector&lt;int&gt;&amp; index)</code></li>
<li><code>diff_at(const vector&lt;int&gt;&amp; index)</code></li>
</ul>
<h2 id="offset"><a href="#offset" class="headerlink" title="offset()"></a><code>offset()</code></h2><p><code>offset()</code>函数有两个实现的版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c = <span class="number">0</span>, <span class="keyword">const</span> <span class="keyword">int</span> h = <span class="number">0</span>,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w = <span class="number">0</span>) <span class="keyword">const</span> &#123;</div><div class="line">  CHECK_GE(n, <span class="number">0</span>);</div><div class="line">  CHECK_LE(n, num());</div><div class="line">  CHECK_GE(channels(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(c, channels());</div><div class="line">  CHECK_GE(height(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(h, height());</div><div class="line">  CHECK_GE(width(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(w, width());</div><div class="line">  <span class="keyword">return</span> ((n * channels() + c) * height() + h) * width() + w;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//用[n,c,h,w]的vector向量实现</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; indices)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(indices.size(), num_axes());</div><div class="line">  <span class="keyword">int</span> offset = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_axes(); ++i) &#123;</div><div class="line">    offset *= shape(i);</div><div class="line">    <span class="keyword">if</span> (indices.size() &gt; i) &#123;</div><div class="line">      CHECK_GE(indices[i], <span class="number">0</span>);</div><div class="line">      CHECK_LT(indices[i], shape(i));</div><div class="line">      offset += indices[i]; </div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> offset;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="data-at-和diff-data-函数的访问"><a href="#data-at-和diff-data-函数的访问" class="headerlink" title="data_at()和diff_data()函数的访问"></a><code>data_at()</code>和<code>diff_data()</code>函数的访问</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(index)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="返回data-和diff-指针"><a href="#返回data-和diff-指针" class="headerlink" title="返回data_和diff_指针"></a>返回<code>data_</code>和<code>diff_</code>指针</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> data_;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> diff_;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="data-的更新"><a href="#data-的更新" class="headerlink" title="data_的更新"></a><code>data_</code>的更新</h1><p>对于<code>data_</code>的更新，一般是对其减去反向传播的<code>diff_</code>乘以相应的系数，这里主要用到的函数有</p>
<ul>
<li><code>void Update()</code>: 用来对<code>data_</code>进行更新</li>
<li><code>Dtype asum_data() const</code>: 对<code>data_</code>求绝对值之和，即L1范数</li>
<li><code>Dtype asum_diff() const</code>: 对<code>diff_</code>求L1范数</li>
<li><code>Dtype sumsq_data() const</code>:对<code>data_</code>求平方和之和，即L2范数</li>
<li><code>Dtype sumsq_diff() const</code>:对<code>diff_</code>求L2范数</li>
<li><code>void scale_data(Dtype scale_factor)</code>:对<code>data_</code>乘以相应的标量</li>
<li><code>void scale_diff(Dtype scale_factor)</code>:对<code>diff_</code>乘以相应的标量</li>
</ul>
<h2 id="Update-方法"><a href="#Update-方法" class="headerlink" title="Update()方法"></a><code>Update()</code>方法</h2><p><code>Updata()</code>方法组要是用来对<code>data_</code>进行<code>diff_</code>的更新，主要是封装了cblas和cublas中的版本，其中里面分别有针对<code>float</code>和<code>double</code>版本的。同样，<code>Updata()</code>方法是针对Blob<float>和Blob<double>版本的，因此没有实现<code>int</code>版本和<code>unsigned int</code>版本</double></float></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Update() &#123;</div><div class="line">  <span class="comment">// We will perform update based on where the data is located.</span></div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:  <span class="comment">//对于cpu中的数据,调用caffe_axpy()</span></div><div class="line">    <span class="comment">// perform computation on CPU</span></div><div class="line">    caffe_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;cpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY                     </span></div><div class="line">    <span class="comment">// perform computation on GPU  //对于gpu中的数据调用caffe_gpu_axpy()</span></div><div class="line">    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;gpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;     <span class="comment">//log报错                   </span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Syncedmem not initialized."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在<code>Update</code>函数中调用了<code>caffe_axpy</code>,分别封装了cpu实现版本<code>cblas_saxpy</code>,主要是调用了cblas中的函数；<br>gpu实现版本<code>caffe_gpu_axpy</code>，主要是调用了cublas中的函数。</p>
<p>两者的声明在/caffe/util/math_function.hpp中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div></pre></td></tr></table></figure>
<p>cpu版本的实现只有一种，位于/caffe/util/math_function.cpp中</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123; cblas_saxpy(N, alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>); &#125;</div></pre></td></tr></table></figure>
<p>在<code>cblas_saxpy</code>函数中，N是这个向量的元素个数，在Blob中就是<code>count_</code>。alpha是X前面的系数，X,Y 是输入的矢量。其中的1和1分别是X,Y的步进，这里每个元素都要更新，所以是1。函数实现的功能是</p>
<p>\begin{equation}<br>Y=alpha * X+Y<br>\end{equation}</p>
<p>gpu版本的实现有两种,位于/caffe/util/math_function.cu中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">const</span> <span class="keyword">double</span>* X,</div><div class="line">    <span class="keyword">double</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="求L1范数"><a href="#求L1范数" class="headerlink" title="求L1范数"></a>求L1范数</h2><p><code>asum_data()</code>函数用来计算data数据的绝对值之和(L1范数),<code>asum_diff()</code>用来计算梯度数据diff的L1范数。<br>主要是调用了cpu版本的<code>caffe_cpu_asum</code>和gpu版本的<code>caffe_gpu_asum</code>，这两个函数同样分别对cblas和cublas中的函数进行了封装。</p>
<p><code>asum_data()</code>的实现如下，<code>asum_diff()</code>的实现类似，无非是将<code>data_</code>指针换成了<code>diff_</code>指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    <span class="keyword">return</span> caffe_cpu_asum(count_, cpu_data());</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  &#123;</div><div class="line">    Dtype asum;</div><div class="line">    caffe_gpu_asum(count_, gpu_data(), &amp;asum);</div><div class="line">    <span class="keyword">return</span> asum;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中,<code>caffe_cpu_asum</code>和<code>caffe_gpu_asum</code>的声明如下，位于/caffe/util/math_function.hpp</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, Dtype* y)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_asum</code>的实现在/caffe/util/math_function.cpp中,针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sasum(n, x, <span class="number">1</span>);<span class="comment">//用来计算向量x的和，共有n个元素，1是stride,每个元素都用所以取1</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_dasum(n, x, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_asum</code>的实现在/caffe/util/math_function.cu中，针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//gpu的计算使用cublas</span></div><div class="line"><span class="keyword">template</span> &lt;&gt; </div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">float</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">double</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h2><p><code>sumq_data()</code>函数用来计算data数据的平方和(L2范数),<code>sumsq_diff()</code> 用来计算梯度数据diff的L2范数。<br>主要是调用了cpu版本的<code>caffe_cpu_dout</code>和gpu版本的<code>caffe_gpu_dot</code>函数。</p>
<p><code>sumq_data()</code>的实现如下，<code>sumsq_diff()</code>与之类似：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  Dtype sumsq;</div><div class="line">  <span class="keyword">const</span> Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = cpu_data();</div><div class="line">    sumsq = caffe_cpu_dot(count_, data, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = gpu_data();</div><div class="line">    caffe_gpu_dot(count_, data, data, &amp;sumsq); <span class="comment">//gpu版本</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> sumsq;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中<code>caffe_cpu_dout</code>和<code>caffe_gpu_dot</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_dot</code>的实现调用了</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> caffe_cpu_strided_dot(n, x, <span class="number">1</span>, y, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_strided_dot</code>是一个模板函数，对<code>cblas_sdot</code>和<code>cblas_ddot</code>进行了封装，其声明和实现分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_strided_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</span></span></div><div class="line">    <span class="keyword">const</span> Dtype* y, <span class="keyword">const</span> <span class="keyword">int</span> incy);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_strided_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sdot(n, x, incx, y, incy);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_strided_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> incx, <span class="keyword">const</span> <span class="keyword">double</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_ddot(n, x, incx, y, incy);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_dot</code>是一个模板函数，它的声明如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p>该函数有<code>float</code>和<code>double</code>两个实现版本,位于math_functions.cu中，实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">float</span>* y,</div><div class="line">    <span class="keyword">float</span>* out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">const</span> <span class="keyword">double</span>* y,</div><div class="line">    <span class="keyword">double</span> * out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="scale-data和scale-diff"><a href="#scale-data和scale-diff" class="headerlink" title="scale_data和scale_diff"></a><code>scale_data</code>和<code>scale_diff</code></h2><p><code>scale_data</code>函数和<code>scale_diff</code>函数主要是对Blob内的<code>data_</code>向量或者<code>diff_</code>向量乘以一个标量。<br>主要调用的两个函数cpu版本的<code>caffe_scal()</code>和gpu版本的<code>caffe_gpu_scal()</code></p>
<p><code>scale_data</code>的实现如下,<code>scale_diff</code>与之类似。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::scale_data(<span class="keyword">unsigned</span> <span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::scale_data(<span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) &#123;</div><div class="line">  Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = mutable_cpu_data();</div><div class="line">    caffe_scal(count_, scale_factor, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = mutable_gpu_data();</div><div class="line">    caffe_gpu_scal(count_, scale_factor, data); <span class="comment">//gpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_scal</code>的模板函数声明为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p>对应的<code>float</code>和<code>double</code>实现版本如下，分别是对<code>cblas_sscal</code>和<code>cblas_dscal</code>函数的调用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  cblas_sscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  cblas_dscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的实现同样有两个版本：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="其他的一些函数"><a href="#其他的一些函数" class="headerlink" title="其他的一些函数"></a>其他的一些函数</h1><ul>
<li><code>ShareData</code></li>
<li><code>ShareDiff</code></li>
<li><code>ShapeEquals</code></li>
<li><code>CopyFrom</code></li>
<li><code>FromProto</code></li>
<li><code>ToProto</code></li>
</ul>
<h2 id="ShareData和ShareDiff"><a href="#ShareData和ShareDiff" class="headerlink" title="ShareData和ShareDiff"></a><code>ShareData</code>和<code>ShareDiff</code></h2><p><code>ShareData</code>和<code>ShareDiff</code>实现方式是直接将<code>data_</code>和<code>diff_</code>的指针替换成其他类中的指针。这可以简化Layer中前向传递时只是简单的copy的情况。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareData(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  data_ = other.data();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareDiff(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  diff_ = other.diff();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="ShapeEquals"><a href="#ShapeEquals" class="headerlink" title="ShapeEquals"></a><code>ShapeEquals</code></h2><p>Blob的shape是否相同的检查</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">bool</span> Blob&lt;Dtype&gt;::ShapeEquals(<span class="keyword">const</span> BlobProto&amp; other) &#123;</div><div class="line">  <span class="keyword">if</span> (other.has_num() || other.has_channels() ||</div><div class="line">      other.has_height() || other.has_width()) &#123;</div><div class="line">    <span class="comment">// Using deprecated 4D Blob dimensions --</span></div><div class="line">    <span class="comment">// shape is (num, channels, height, width).</span></div><div class="line">    <span class="comment">// Note: we do not use the normal Blob::num(), Blob::channels(), etc.</span></div><div class="line">    <span class="comment">// methods as these index from the beginning of the blob shape, where legacy</span></div><div class="line">    <span class="comment">// parameter blobs were indexed from the end of the blob shape (e.g., bias</span></div><div class="line">    <span class="comment">// Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).</span></div><div class="line">    <span class="keyword">return</span> shape_.size() &lt;= <span class="number">4</span> &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-4</span>) == other.num() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-3</span>) == other.channels() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-2</span>) == other.height() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-1</span>) == other.width();</div><div class="line">  &#125;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; other_shape(other.shape().dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; other.shape().dim_size(); ++i) &#123;</div><div class="line">    other_shape[i] = other.shape().dim(i);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape_ == other_shape;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="CopyFrom"><a href="#CopyFrom" class="headerlink" title="CopyFrom"></a><code>CopyFrom</code></h2><p><code>CopyFrom</code>的声明如下，其中<code>copy_diff</code>为<code>false</code>,则拷贝的是data,否则拷贝diff</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></div><div class="line">    <span class="keyword">bool</span> reshape = <span class="literal">false</span>);</div></pre></td></tr></table></figure>
<p>实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, <span class="keyword">bool</span> reshape) &#123;</div><div class="line">  <span class="comment">//先做size检查</span></div><div class="line">  <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</div><div class="line">    <span class="keyword">if</span> (reshape) &#123;</div><div class="line">      ReshapeLike(source);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Trying to copy blobs of different sizes."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.gpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.gpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.cpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.cpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中，<code>copy_copy</code>封装了cpu内存之间的内存copy和gpu内存的copy</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_copy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (X != Y) &#123;</div><div class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">      <span class="comment">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></div><div class="line">      CUDA_CHECK(cudaMemcpy(Y, X, <span class="keyword">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">      NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">memcpy</span>(Y, X, <span class="keyword">sizeof</span>(Dtype) * N);  <span class="comment">// NOLINT(caffe/alt_fn)</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="FromProto和ToProto"><a href="#FromProto和ToProto" class="headerlink" title="FromProto和ToProto"></a><code>FromProto</code>和<code>ToProto</code></h2><p><code>FromProto</code>用proto文件来实现Blob的初始化,<code>ToProto</code>是将Blob的内容写入到proto文件</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总而言之，Blob相对而言是一个比较简单的类，在看懂<code>SyncedMemory</code>以后，就比较容易看懂这个类了，里面的各种调用数学函数的声明都是在/caffe/util/math_functions.hpp中，实现分别是在math_functions.cpp和math_functions.cu中。</p>
<p>在类的最后可以看到 <code>DISABLE_COPY_AND_ASSIGN(Blob)</code>,这是一个宏函数，可以看到它的实现,主要是禁止这个类的拷贝和赋值操作。是为了防止两个大型数据结构内容的复制和赋值，如果要使用，应该是使用指针和引用来指向。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Disable the copy and assignment operator for a class.</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> DISABLE_COPY_AND_ASSIGN(classname) \</span></div><div class="line">private:\</div><div class="line">  classname(const classname&amp;);\</div><div class="line">  classname&amp; operator=(const classname&amp;)</div></pre></td></tr></table></figure>
<h4 id="接下去将对Layer进行分析…"><a href="#接下去将对Layer进行分析…" class="headerlink" title="接下去将对Layer进行分析….."></a>接下去将对Layer进行分析…..</h4>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/caffe/" rel="tag"># caffe</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/10/15/Caffe源码解读1——SyncedMemory/" rel="next" title="Caffe源码解读1——SyncedMemory">
                <i class="fa fa-chevron-left"></i> Caffe源码解读1——SyncedMemory
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/11/19/Caffe源码解读-im2col/" rel="prev" title="Caffe源码解读--im2col">
                Caffe源码解读--im2col <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="fb-comments"
           data-href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/"
           data-numposts="10"
           data-width="100%"
           data-colorscheme="light">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="John Doe" />
          <p class="site-author-name" itemprop="name">John Doe</p>
          <p class="site-description motion-element" itemprop="description">Stay hungry</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">10</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#私有保护变量"><span class="nav-number">1.</span> <span class="nav-text">私有保护变量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reshape函数"><span class="nav-number">2.</span> <span class="nav-text">Reshape函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape成员函数1"><span class="nav-number">2.1.</span> <span class="nav-text">Reshape成员函数1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape成员函数2"><span class="nav-number">2.2.</span> <span class="nav-text">Reshape成员函数2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape成员函数3"><span class="nav-number">2.3.</span> <span class="nav-text">Reshape成员函数3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape成员函数4"><span class="nav-number">2.4.</span> <span class="nav-text">Reshape成员函数4</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shape数据输出函数"><span class="nav-number">2.5.</span> <span class="nav-text">shape数据输出函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#构造函数"><span class="nav-number">3.</span> <span class="nav-text">构造函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一些变量访问函数"><span class="nav-number">4.</span> <span class="nav-text">一些变量访问函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#num-axes"><span class="nav-number">4.1.</span> <span class="nav-text">num_axes()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#count"><span class="nav-number">4.2.</span> <span class="nav-text">count()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CanonicalAxisIndex"><span class="nav-number">4.3.</span> <span class="nav-text">CanonicalAxisIndex()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#int-shape-int-intdex"><span class="nav-number">4.4.</span> <span class="nav-text">int shape(int intdex)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LegacyShape-int-index"><span class="nav-number">4.5.</span> <span class="nav-text">LegacyShape(int index)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#num-channels-height-width"><span class="nav-number">4.6.</span> <span class="nav-text">num(),channels(),height(),width()</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CPU和GPU中数据的获得"><span class="nav-number">5.</span> <span class="nav-text">CPU和GPU中数据的获得</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cpu-data"><span class="nav-number">5.1.</span> <span class="nav-text">cpu_data()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#set-cpu-data"><span class="nav-number">5.2.</span> <span class="nav-text">set_cpu_data()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu-shape"><span class="nav-number">5.3.</span> <span class="nav-text">gpu_shape</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu-data"><span class="nav-number">5.4.</span> <span class="nav-text">gpu_data()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cpu-diff"><span class="nav-number">5.5.</span> <span class="nav-text">cpu_diff()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu-diff"><span class="nav-number">5.6.</span> <span class="nav-text">gpu_diff()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mutable版本的"><span class="nav-number">5.7.</span> <span class="nav-text">mutable版本的</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#具体offset位置处的访问"><span class="nav-number">6.</span> <span class="nav-text">具体offset位置处的访问</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#offset"><span class="nav-number">6.1.</span> <span class="nav-text">offset()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data-at-和diff-data-函数的访问"><span class="nav-number">6.2.</span> <span class="nav-text">data_at()和diff_data()函数的访问</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#返回data-和diff-指针"><span class="nav-number">6.3.</span> <span class="nav-text">返回data_和diff_指针</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#data-的更新"><span class="nav-number">7.</span> <span class="nav-text">data_的更新</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-方法"><span class="nav-number">7.1.</span> <span class="nav-text">Update()方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#求L1范数"><span class="nav-number">7.2.</span> <span class="nav-text">求L1范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L2范数"><span class="nav-number">7.3.</span> <span class="nav-text">L2范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scale-data和scale-diff"><span class="nav-number">7.4.</span> <span class="nav-text">scale_data和scale_diff</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他的一些函数"><span class="nav-number">8.</span> <span class="nav-text">其他的一些函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ShareData和ShareDiff"><span class="nav-number">8.1.</span> <span class="nav-text">ShareData和ShareDiff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShapeEquals"><span class="nav-number">8.2.</span> <span class="nav-text">ShapeEquals</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CopyFrom"><span class="nav-number">8.3.</span> <span class="nav-text">CopyFrom</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FromProto和ToProto"><span class="nav-number">8.4.</span> <span class="nav-text">FromProto和ToProto</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">9.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#接下去将对Layer进行分析…"><span class="nav-number">9.0.0.1.</span> <span class="nav-text">接下去将对Layer进行分析…..</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>







        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
</html>
