<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="learn, think" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Stay hungry">
<meta property="og:type" content="website">
<meta property="og:title" content="Keson's blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Keson's blog">
<meta property="og:description" content="Stay hungry">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keson's blog">
<meta name="twitter:description" content="Stay hungry">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Keson's blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Keson's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/27/Caffe-Layer-学习/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/27/Caffe-Layer-学习/" itemprop="url">
                  Caffe Layer 学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-27T16:13:13+08:00">
              2016-11-27
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-02-27T16:13:43+08:00">
              2017-02-27
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/27/Caffe-Layer-学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/27/Caffe-Layer-学习/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/27/Caffe-Layer-学习/" class="leancloud_visitors" data-flag-title="Caffe Layer 学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/26/Jetson-TX1-训练darknet/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/26/Jetson-TX1-训练darknet/" itemprop="url">
                  Jetson TX1 训练darknet
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-26T10:35:13+08:00">
              2016-11-26
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-06T10:36:00+08:00">
              2017-03-06
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/26/Jetson-TX1-训练darknet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/26/Jetson-TX1-训练darknet/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/26/Jetson-TX1-训练darknet/" class="leancloud_visitors" data-flag-title="Jetson TX1 训练darknet">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="在Jetson-TX1-上训练darknet"><a href="#在Jetson-TX1-上训练darknet" class="headerlink" title="在Jetson TX1 上训练darknet"></a>在Jetson TX1 上训练darknet</h1><p>本文介绍在Jetson TX1上用VOC2007数据集训练YOLO。</p>
<p>Reference:</p>
<p><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">YOLO主页</a></p>
<h2 id="1-下载数据集"><a href="#1-下载数据集" class="headerlink" title="1 下载数据集"></a>1 下载数据集</h2><p>Pascal VOC 数据集是一个用来进行目标检测的数据集，官网链接<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="external">pascal voc</a></p>
<p>从上面下载VOC2007 的数据集，解压后放到 /darknet/scripts  </p>
<h2 id="2-修改"><a href="#2-修改" class="headerlink" title="2 修改"></a>2 修改</h2><p>scripts下的voc_label.py可以用来将voc数据集转换成yolo格式的数据集，对voc_label.py进行修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sets=[(<span class="string">'2007'</span>,<span class="string">'train'</span>),(<span class="string">'2007'</span>,<span class="string">'val'</span>)] //这里只用了VOC2007里的train和validation数据集</div></pre></td></tr></table></figure>
<p>如果是自己的数据集，需要相应的修改成VOC格式，在进行修改，<br>可以参考这里<a href="http://blog.csdn.net/sinat_30071459/article/details/53100791" target="_blank" rel="external">YOLO(v1)用自己的数据集训练模型</a><br><a href="http://blog.csdn.net/sinat_30071459/article/details/50723212" target="_blank" rel="external">做自己的VOC2007数据集</a></p>
<p>运行pyhon voc_label.py后可以发现/VOCdevkit/VOC2007 下多了一个labels文件夹。</p>
<h2 id="修改YOLO-源码"><a href="#修改YOLO-源码" class="headerlink" title="修改YOLO 源码"></a>修改YOLO 源码</h2><p>然后修改/darknet/src/yolo.c中的源码</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">char</span> *train_images=<span class="string">"/home/ubuntu/darknet/scripts/2007_train.txt"</span> </div><div class="line"><span class="keyword">char</span> *backup_directory= <span class="string">"/home/ubuntu/darknet/backup"</span> <span class="comment">//用来保存中间的权重</span></div></pre></td></tr></table></figure>
<h3 id="重新训练"><a href="#重新训练" class="headerlink" title="重新训练"></a>重新训练</h3><p>make编译后，可以重新训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./darknet yolo train cfg/tiny-yolo.cfg darknet.conv.weights</div><div class="line">``` </div><div class="line">darknet.conv.weights 是卷积网络在分类网络上预训练的权重，在此基础上训练，</div><div class="line">可以从这里下载[darknet.conv.weights](http://download.csdn.net/detail/kesonyk/9707462)</div><div class="line"></div><div class="line">下载后放到/darknet目录即可</div><div class="line"></div><div class="line">### 在Jetson TX1 上训练时的技巧</div><div class="line">由于TX1 只有4G的共用内存和显存，因此batch需要重新设置，更改tiny-yolo.cfg的配置</div></pre></td></tr></table></figure>
<p>batch=64<br>subdivisions=8<br>```<br>其中batch是64，指的是64张图片后更新权重，subdivions是为了适应GPU有限的资源，在这里GPU每次只处理 8(64/8)张图片.所以是进行8个新的子batch后更新权重。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/20/Caffe源码解读4-激活函数/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/20/Caffe源码解读4-激活函数/" itemprop="url">
                  Caffe源码解读4--激活函数
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-20T14:40:31+08:00">
              2016-11-20
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-20T14:42:49+08:00">
              2016-12-20
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/20/Caffe源码解读4-激活函数/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/20/Caffe源码解读4-激活函数/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/20/Caffe源码解读4-激活函数/" class="leancloud_visitors" data-flag-title="Caffe源码解读4--激活函数">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="未完待续-先看ReLU-Leaky-ReLU和PReLU"><a href="#未完待续-先看ReLU-Leaky-ReLU和PReLU" class="headerlink" title="未完待续~~ 先看ReLU,Leaky-ReLU和PReLU"></a>未完待续~~ 先看<code>ReLU</code>,<code>Leaky-ReLU</code>和<code>PReLU</code></h4><h1 id="激活函数ReLU和Leaky-ReLU"><a href="#激活函数ReLU和Leaky-ReLU" class="headerlink" title="激活函数ReLU和Leaky-ReLU"></a>激活函数ReLU和Leaky-ReLU</h1><p><code>ReLU</code>函数实现的功能是$y=\max(0,x)$,<br><code>Leaky-ReLU</code>实现的功能是$y = \max(0, x) + \nu \min(0, x)$</p>
<ul>
<li>Layer type: ReLU</li>
<li>CPU implementation: ./src/caffe/layers/relu_layer.cpp</li>
<li>CUDA GPU implementation: ./src/caffe/layers/relu_layer.cu</li>
<li>Parameters (ReLUParameter relu_param)<ul>
<li>OPtional<ul>
<li>negative_slope [default 0]: specifies whether to leak the negative part by multiplying it with the slope value rather than setting it to 0.</li>
</ul>
</li>
</ul>
</li>
<li>Sample</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"relu1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">  bottom: <span class="string">"conv1"</span></div><div class="line">  top: <span class="string">"conv1"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">   name: <span class="string">"relu1"</span></div><div class="line">   type: <span class="string">"ReLU"</span></div><div class="line">   bottom: <span class="string">"conv1"</span></div><div class="line">   top: <span class="string">"conv1"</span></div><div class="line">   relu_param&#123;</div><div class="line">      negative_slope: <span class="number">0.1</span></div><div class="line">   &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h2 id="梯度求导"><a href="#梯度求导" class="headerlink" title="梯度求导"></a>梯度求导</h2><h3 id="对于ReLU"><a href="#对于ReLU" class="headerlink" title="对于ReLU"></a>对于<code>ReLU</code></h3><p>$$<br>           \frac{\partial E}{\partial x} = \left\{<br>           \begin{array}{lr}<br>               0 &amp; \mathrm{if} \; x \le 0 \\<br>               \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0<br>           \end{array} \right.<br>$$</p>
<h3 id="对于Leaky-ReLU"><a href="#对于Leaky-ReLU" class="headerlink" title="对于Leaky-ReLU"></a>对于<code>Leaky-ReLU</code></h3><p>$$<br>          \frac{\partial E}{\partial x} = \left\{<br>           \begin{array}{lr}<br>               \nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\<br>               \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0<br>           \end{array} \right.<br>$$</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p><code>Forward_cpu</code>的实现</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">  Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</div><div class="line">    top_data[i] = <span class="built_in">std</span>::max(bottom_data[i], Dtype(<span class="number">0</span>))</div><div class="line">        + negative_slope * <span class="built_in">std</span>::min(bottom_data[i], Dtype(<span class="number">0</span>));</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Forward_gpu</code>的实现</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">ReLUForward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in, Dtype* out,</span></span></div><div class="line">    Dtype negative_slope) &#123;</div><div class="line">  CUDA_KERNEL_LOOP(index, n) &#123;</div><div class="line">    out[index] = in[index] &gt; <span class="number">0</span> ? in[index] : in[index] * negative_slope;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;gpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_gpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">  Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">  <span class="comment">// NOLINT_NEXT_LINE(whitespace/operators)</span></div><div class="line">  ReLUForward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</div><div class="line">      count, bottom_data, top_data, negative_slope);</div><div class="line">  CUDA_POST_KERNEL_CHECK;</div><div class="line">  <span class="comment">// &lt;&lt; " count: " &lt;&lt; count &lt;&lt; " bottom_data: "</span></div><div class="line">  <span class="comment">//     &lt;&lt; (unsigned long)bottom_data</span></div><div class="line">  <span class="comment">//     &lt;&lt; " top_data: " &lt;&lt; (unsigned long)top_data</span></div><div class="line">  <span class="comment">//     &lt;&lt; " blocks: " &lt;&lt; CAFFE_GET_BLOCKS(count)</span></div><div class="line">  <span class="comment">//     &lt;&lt; " threads: " &lt;&lt; CAFFE_CUDA_NUM_THREADS;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Backward_cpu</code>实现</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</div><div class="line">      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">          + negative_slope * (bottom_data[i] &lt;= <span class="number">0</span>));</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Backward_gpu</code>实现</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">ReLUBackward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* in_diff,</span></span></div><div class="line">    <span class="keyword">const</span> Dtype* in_data, Dtype* out_diff, Dtype negative_slope) &#123;</div><div class="line">  CUDA_KERNEL_LOOP(index, n) &#123;</div><div class="line">    out_diff[index] = in_diff[index] * ((in_data[index] &gt; <span class="number">0</span>)</div><div class="line">        + (in_data[index] &lt;= <span class="number">0</span>) * negative_slope);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> ReLULayer&lt;Dtype&gt;::Backward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">    <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;gpu_data();</div><div class="line">    <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;gpu_diff();</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_gpu_diff();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">    Dtype negative_slope = <span class="keyword">this</span>-&gt;layer_param_.relu_param().negative_slope();</div><div class="line">    <span class="comment">// NOLINT_NEXT_LINE(whitespace/operators)</span></div><div class="line">    ReLUBackward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</div><div class="line">        count, top_diff, bottom_data, bottom_diff, negative_slope);</div><div class="line">    CUDA_POST_KERNEL_CHECK;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="PReLU函数"><a href="#PReLU函数" class="headerlink" title="PReLU函数"></a>PReLU函数</h1><p><code>PReLU</code>函数实现的功能是$y_i = \max(0, x_i) + a_i \min(0, x_i)$</p>
<p>跟<code>ReLU</code>函数有两个不同之处：</p>
<p>（1）负数前面的斜率$a_i$是可以通过反向传播学习的。</p>
<p>（2）不同通道之间的负数斜率$a_i$可以不同。</p>
<h2 id="Forward-cpu"><a href="#Forward-cpu" class="headerlink" title="Forward_cpu"></a><code>Forward_cpu</code></h2><p><code>Forward_cpu</code>的实现,这里需要注意的地方是，需要计算属于哪个channels的index。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> PReLULayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">  <span class="comment">//计算H*W的维度大小</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>);</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> channels = bottom[<span class="number">0</span>]-&gt;channels();</div><div class="line">  <span class="keyword">const</span> Dtype* slope_data = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line"></div><div class="line">  <span class="comment">// For in-place computation</span></div><div class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>] == top[<span class="number">0</span>]) &#123;</div><div class="line">    caffe_copy(count, bottom_data, bottom_memory_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// if channel_shared, channel index in the following computation becomes</span></div><div class="line">  <span class="comment">// always zero.</span></div><div class="line">  <span class="comment">//若channels是共享的，则下面计算的c肯定为0，则只有一个斜率参数。</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> div_factor = channel_shared_ ? channels : <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</div><div class="line">    <span class="comment">//在这里计算属于哪个channels的index</span></div><div class="line">    <span class="keyword">int</span> c = (i / dim) % channels / div_factor;</div><div class="line">    top_data[i] = <span class="built_in">std</span>::max(bottom_data[i], Dtype(<span class="number">0</span>))</div><div class="line">        + slope_data[c] * <span class="built_in">std</span>::min(bottom_data[i], Dtype(<span class="number">0</span>));</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Forward-gpu"><a href="#Forward-gpu" class="headerlink" title="Forward_gpu"></a><code>Forward_gpu</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// CUDA kernele for forward</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">PReLUForward</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> dim,</span></span></div><div class="line">    <span class="keyword">const</span> Dtype* in, Dtype* out, <span class="keyword">const</span> Dtype* slope_data,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> div_factor) &#123;</div><div class="line">  CUDA_KERNEL_LOOP(index, n) &#123;</div><div class="line">    <span class="keyword">int</span> c = (index / dim) % channels / div_factor;</div><div class="line">    out[index] = in[index] &gt; <span class="number">0</span> ? in[index] : in[index] * slope_data[c];</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> PReLULayer&lt;Dtype&gt;::Forward_gpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;gpu_data();</div><div class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_gpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>);</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> channels = bottom[<span class="number">0</span>]-&gt;channels();</div><div class="line">  <span class="keyword">const</span> Dtype* slope_data = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;gpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> div_factor = channel_shared_ ? channels : <span class="number">1</span>;</div><div class="line"></div><div class="line">  <span class="comment">// For in-place computation</span></div><div class="line">  <span class="keyword">if</span> (top[<span class="number">0</span>] == bottom[<span class="number">0</span>]) &#123;</div><div class="line">    caffe_copy(count, bottom_data, bottom_memory_.mutable_gpu_data());</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// NOLINT_NEXT_LINE(whitespace/operators)</span></div><div class="line">  PReLUForward&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(</div><div class="line">      count, channels, dim, bottom_data, top_data, slope_data, div_factor);</div><div class="line">  CUDA_POST_KERNEL_CHECK;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Backward-cpu"><a href="#Backward-cpu" class="headerlink" title="Backward_cpu"></a><code>Backward_cpu</code></h2><p>针对 $y_i = \max(0, x_i) + a_i \min(0, x_i)$，则 $\frac{\partial E}{\partial a_i}=x_i,if x_i&lt;0$</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> PReLULayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">const</span> Dtype* slope_data = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">const</span> Dtype* top_diff = top[<span class="number">0</span>]-&gt;cpu_diff();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> count = bottom[<span class="number">0</span>]-&gt;count();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> dim = bottom[<span class="number">0</span>]-&gt;count(<span class="number">2</span>);</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> channels = bottom[<span class="number">0</span>]-&gt;channels();</div><div class="line"></div><div class="line">  <span class="comment">// For in-place computation</span></div><div class="line">  <span class="keyword">if</span> (top[<span class="number">0</span>] == bottom[<span class="number">0</span>]) &#123;</div><div class="line">    bottom_data = bottom_memory_.cpu_data();</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// if channel_shared, channel index in the following computation becomes</span></div><div class="line">  <span class="comment">// always zero.</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> div_factor = channel_shared_ ? channels : <span class="number">1</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Propagte to param</span></div><div class="line">  <span class="comment">// Since to write bottom diff will affect top diff if top and bottom blobs</span></div><div class="line">  <span class="comment">// are identical (in-place computaion), we first compute param backward to</span></div><div class="line">  <span class="comment">// keep top_diff unchanged.</span></div><div class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;param_propagate_down_[<span class="number">0</span>]) &#123;</div><div class="line">    Dtype* slope_diff = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</div><div class="line">      <span class="keyword">int</span> c = (i / dim) % channels / div_factor;</div><div class="line">      </div><div class="line">      <span class="comment">//这里是斜率的梯度更新</span></div><div class="line">      slope_diff[c] += top_diff[i] * bottom_data[i] * (bottom_data[i] &lt;= <span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Propagate to bottom</span></div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; ++i) &#123;</div><div class="line">      <span class="keyword">int</span> c = (i / dim) % channels / div_factor;</div><div class="line">      <span class="comment">//这里更新bottom的梯度</span></div><div class="line">      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; <span class="number">0</span>)</div><div class="line">          + slope_data[c] * (bottom_data[i] &lt;= <span class="number">0</span>));</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" itemprop="url">
                  Caffe源码解读6--BaseConvolutionLayer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-20T14:27:12+08:00">
              2016-11-20
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-20T14:28:35+08:00">
              2016-12-20
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" class="leancloud_visitors" data-flag-title="Caffe源码解读6--BaseConvolutionLayer">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Message-ConvolutionParameter"><a href="#Message-ConvolutionParameter" class="headerlink" title="Message ConvolutionParameter"></a>Message ConvolutionParameter</h1><p>分析caffe中的每个层，应该先看caffe.proto中关于该层的参数定义，<code>message ConvolutionParameter</code>的定义如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">message ConvolutionParameter &#123;</div><div class="line">  optional uint32 num_output = <span class="number">1</span>; <span class="comment">// The number of outputs for the layer</span></div><div class="line">  optional <span class="keyword">bool</span> bias_term = <span class="number">2</span> [<span class="keyword">default</span> = <span class="literal">true</span>]; <span class="comment">// whether to have bias terms</span></div><div class="line"></div><div class="line">  <span class="comment">// Pad, kernel size, and stride are all given as a single value for equal</span></div><div class="line">  <span class="comment">// dimensions in all spatial dimensions, or once per spatial dimension.</span></div><div class="line">  repeated uint32 pad = <span class="number">3</span>; <span class="comment">// The padding size; defaults to 0</span></div><div class="line">  repeated uint32 kernel_size = <span class="number">4</span>; <span class="comment">// The kernel size</span></div><div class="line">  repeated uint32 stride = <span class="number">6</span>; <span class="comment">// The stride; defaults to 1</span></div><div class="line">  <span class="comment">// Factor used to dilate the kernel, (implicitly) zero-filling the resulting</span></div><div class="line">  <span class="comment">// holes. (Kernel dilation is sometimes referred to by its use in the</span></div><div class="line">  <span class="comment">// algorithme à trous from Holschneider et al. 1987.)</span></div><div class="line">  repeated uint32 dilation = <span class="number">18</span>; <span class="comment">// The dilation; defaults to 1</span></div><div class="line"></div><div class="line">  <span class="comment">// For 2D convolution only, the *_h and *_w versions may also be used to</span></div><div class="line">  <span class="comment">// specify both spatial dimensions.</span></div><div class="line">  optional uint32 pad_h = <span class="number">9</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding height (2D only)</span></div><div class="line">  optional uint32 pad_w = <span class="number">10</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding width (2D only)</span></div><div class="line">  optional uint32 kernel_h = <span class="number">11</span>; <span class="comment">// The kernel height (2D only)</span></div><div class="line">  optional uint32 kernel_w = <span class="number">12</span>; <span class="comment">// The kernel width (2D only)</span></div><div class="line">  optional uint32 stride_h = <span class="number">13</span>; <span class="comment">// The stride height (2D only)</span></div><div class="line">  optional uint32 stride_w = <span class="number">14</span>; <span class="comment">// The stride width (2D only)</span></div><div class="line"></div><div class="line">  optional uint32 group = <span class="number">5</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// The group size for group conv</span></div><div class="line"></div><div class="line">  optional FillerParameter weight_filler = <span class="number">7</span>; <span class="comment">// The filler for the weight</span></div><div class="line">  optional FillerParameter bias_filler = <span class="number">8</span>; <span class="comment">// The filler for the bias</span></div><div class="line">  <span class="keyword">enum</span> Engine &#123;</div><div class="line">    DEFAULT = <span class="number">0</span>;</div><div class="line">    CAFFE = <span class="number">1</span>;</div><div class="line">    CUDNN = <span class="number">2</span>;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = <span class="number">15</span> [<span class="keyword">default</span> = DEFAULT];</div><div class="line"></div><div class="line">  <span class="comment">// The axis to interpret as "channels" when performing convolution.</span></div><div class="line">  <span class="comment">// Preceding dimensions are treated as independent inputs;</span></div><div class="line">  <span class="comment">// succeeding dimensions are treated as "spatial".</span></div><div class="line">  <span class="comment">// With (N, C, H, W) inputs, and axis == 1 (the default), we perform</span></div><div class="line">  <span class="comment">// N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</span></div><div class="line">  <span class="comment">// groups g&gt;1) filters across the spatial axes (H, W) of the input.</span></div><div class="line">  <span class="comment">// With (N, C, D, H, W) inputs, and axis == 1, we perform</span></div><div class="line">  <span class="comment">// N independent 3D convolutions, sliding (C/g)-channels</span></div><div class="line">  <span class="comment">// filters across the spatial axes (D, H, W) of the input.</span></div><div class="line">  optional int32 axis = <span class="number">16</span> [<span class="keyword">default</span> = <span class="number">1</span>];</div><div class="line"></div><div class="line">  <span class="comment">// Whether to force use of the general ND convolution, even if a specific</span></div><div class="line">  <span class="comment">// implementation for blobs of the appropriate number of spatial dimensions</span></div><div class="line">  <span class="comment">// is available. (Currently, there is only a 2D-specific convolution</span></div><div class="line">  <span class="comment">// implementation; for input blobs with num_axes != 2, this option is</span></div><div class="line">  <span class="comment">// ignored and the ND implementation will be used.)</span></div><div class="line">  optional <span class="keyword">bool</span> force_nd_im2col = <span class="number">17</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Message生成的ConvolutionParameter类"><a href="#Message生成的ConvolutionParameter类" class="headerlink" title="Message生成的ConvolutionParameter类"></a>Message生成的ConvolutionParameter类</h1><h2 id="对于Message中的optional字段"><a href="#对于Message中的optional字段" class="headerlink" title="对于Message中的optional字段"></a>对于Message中的<code>optional</code>字段</h2><p>例如对于<code>optional uint32 num_output = 1</code>会生成以下对应的accessors</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// optional uint32 num_output = 1;</span></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">has_num_output</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_num_output</span><span class="params">()</span></span>;</div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> kNumOutputFieldNumber = <span class="number">1</span>;</div><div class="line">::google::protobuf::<span class="function">uint32 <span class="title">num_output</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_num_output</span><span class="params">(::google::protobuf::uint32 value)</span></span>;</div></pre></td></tr></table></figure>
<p>同时会产生如下私有函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_has_num_output</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_has_num_output</span><span class="params">()</span></span>;</div></pre></td></tr></table></figure>
<h2 id="对于Message中的repeated字段"><a href="#对于Message中的repeated字段" class="headerlink" title="对于Message中的repeated字段"></a>对于Message中的<code>repeated</code>字段</h2><p>会产生如下的accessors</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// repeated uint32 pad = 3;</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">pad_size</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_pad</span><span class="params">()</span></span>;</div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> kPadFieldNumber = <span class="number">3</span>;</div><div class="line">::google::protobuf::<span class="function">uint32 <span class="title">pad</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_pad</span><span class="params">(<span class="keyword">int</span> index, ::google::protobuf::uint32 value)</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_pad</span><span class="params">(::google::protobuf::uint32 value)</span></span>;</div><div class="line"><span class="keyword">const</span> ::google::protobuf::RepeatedField&lt; ::google::protobuf::uint32 &gt;&amp;</div><div class="line">    pad() <span class="keyword">const</span>;</div><div class="line">::google::protobuf::RepeatedField&lt; ::google::protobuf::uint32 &gt;*</div><div class="line">    mutable_pad();</div></pre></td></tr></table></figure>
<h1 id="BaseConvolutionLayer类"><a href="#BaseConvolutionLayer类" class="headerlink" title="BaseConvolutionLayer类"></a><code>BaseConvolutionLayer</code>类</h1><p>该类继承<code>Layer</code>,也是<code>ConvolutionLayer</code>和<code>DeconvolutionLayer</code>的抽象类。</p>
<h2 id="protected成员变量和private变量"><a href="#protected成员变量和private变量" class="headerlink" title="protected成员变量和private变量"></a><code>protected</code>成员变量和<code>private</code>变量</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="comment">/// @brief The spatial dimensions of a filter kernel.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; kernel_shape_;  <span class="comment">//卷积核形状</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the stride.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; stride_;        <span class="comment">//步进</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the padding.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; pad_;           <span class="comment">//补充</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the dilation.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; dilation_;      <span class="comment">//膨胀系数</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the convolution input.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; conv_input_shape_; <span class="comment">//卷积的输入形状</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the col_buffer.</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; col_buffer_shape_; <span class="comment">//</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the output.</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; output_shape_;     <span class="comment">//输出的形状</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;* bottom_shape_; <span class="comment">//</span></div><div class="line"></div><div class="line"> <span class="keyword">int</span> num_spatial_axes_;  <span class="comment">//空间的轴个数</span></div><div class="line"> <span class="keyword">int</span> bottom_dim_;        <span class="comment">//输入维度</span></div><div class="line"> <span class="keyword">int</span> top_dim_;           <span class="comment">//输出维度</span></div><div class="line"></div><div class="line"> <span class="keyword">int</span> channel_axis_;      <span class="comment">//通道轴的索引</span></div><div class="line"> <span class="keyword">int</span> num_;               <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> channels_;          <span class="comment">//输入的通道数</span></div><div class="line"> <span class="keyword">int</span> group_;             <span class="comment">//卷积组的大小</span></div><div class="line"> <span class="keyword">int</span> out_spatial_dim_;   <span class="comment">//输出的空间维度</span></div><div class="line"> <span class="keyword">int</span> weight_offset_;     <span class="comment">//使用卷积组时用到的</span></div><div class="line"> <span class="keyword">int</span> num_output_;        <span class="comment">//卷积后的通道数</span></div><div class="line"> <span class="keyword">bool</span> bias_term_;        <span class="comment">//是否使用偏置</span></div><div class="line"> <span class="keyword">bool</span> is_1x1_;           <span class="comment">//是不是1*1的卷积</span></div><div class="line"> <span class="keyword">bool</span> force_nd_im2col_;  <span class="comment">//强制使用n维通用卷积</span></div><div class="line"> </div><div class="line"> <span class="keyword">private</span>:</div><div class="line"> </div><div class="line"> <span class="keyword">int</span> num_kernels_im2col_; <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> num_kernels_col2im_;</div><div class="line"> <span class="keyword">int</span> conv_out_channels_;    <span class="comment">//卷积的输出通道数</span></div><div class="line"> <span class="keyword">int</span> conv_in_channels_;     <span class="comment">//卷积的输入通道数</span></div><div class="line"> <span class="keyword">int</span> conv_out_spatial_dim_; <span class="comment">//卷积的输出空间维度=卷积后的h*w</span></div><div class="line"> <span class="keyword">int</span> kernel_dim_;           <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> col_offset_;           <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> output_offset_;</div><div class="line"></div><div class="line"> Blob&lt;Dtype&gt; col_buffer_;   <span class="comment">//使用im2col时使用的存储空间</span></div><div class="line"> Blob&lt;Dtype&gt; bias_multiplier_;</div></pre></td></tr></table></figure>
<h2 id="LayerSetUp"><a href="#LayerSetUp" class="headerlink" title="LayerSetUp"></a><code>LayerSetUp</code></h2><p>在<code>LayerSetUp</code>中，首先是对kernel size，padding,stride和输入的配置</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">// Configure the kernel size, padding, stride, and inputs.</span></div><div class="line">  ConvolutionParameter conv_param = <span class="keyword">this</span>-&gt;layer_param_.convolution_param(); <span class="comment">//配置卷积参数</span></div><div class="line">  force_nd_im2col_ = conv_param.force_nd_im2col(); <span class="comment">//是否强制使用n维im2col</span></div><div class="line">  channel_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(conv_param.axis());<span class="comment">//获取channel的axis</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> first_spatial_axis = channel_axis_ + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_axes = bottom[<span class="number">0</span>]-&gt;num_axes();     <span class="comment">//数据的axis数量</span></div><div class="line">  num_spatial_axes_ = num_axes - first_spatial_axis;</div><div class="line">  CHECK_GE(num_spatial_axes_, <span class="number">0</span>);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bottom_dim_blob_shape(<span class="number">1</span>, num_spatial_axes_ + <span class="number">1</span>);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; spatial_dim_blob_shape(<span class="number">1</span>, <span class="built_in">std</span>::max(num_spatial_axes_, <span class="number">1</span>));</div><div class="line">  </div><div class="line">  <span class="comment">// Setup filter kernel dimensions (kernel_shape_). 开始卷积核的设置</span></div><div class="line">  kernel_shape_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* kernel_shape_data = kernel_shape_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_kernel_h() || conv_param.has_kernel_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"kernel_h &amp; kernel_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.kernel_size_size())</div><div class="line">        &lt;&lt; <span class="string">"Either kernel_size or kernel_h/w should be specified; not both."</span>;</div><div class="line">    kernel_shape_data[<span class="number">0</span>] = conv_param.kernel_h();  <span class="comment">//给kernel_shape_data 赋值高度</span></div><div class="line">    kernel_shape_data[<span class="number">1</span>] = conv_param.kernel_w();  <span class="comment">//给kernel_shape_data 赋值宽度</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_kernel_dims = conv_param.kernel_size_size(); <span class="comment">//卷积核的数目</span></div><div class="line">    CHECK(num_kernel_dims == <span class="number">1</span> || num_kernel_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"kernel_size must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(kernel_size specified "</span> &lt;&lt; num_kernel_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">        kernel_shape_data[i] =</div><div class="line">            conv_param.kernel_size((num_kernel_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    CHECK_GT(kernel_shape_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Filter dimensions must be nonzero."</span>;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup stride dimensions (stride_). 开始步进的设置</span></div><div class="line">  stride_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* stride_data = stride_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_stride_h() || conv_param.has_stride_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"stride_h &amp; stride_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.stride_size())</div><div class="line">        &lt;&lt; <span class="string">"Either stride or stride_h/w should be specified; not both."</span>;</div><div class="line">    stride_data[<span class="number">0</span>] = conv_param.stride_h(); <span class="comment">//给步进赋值</span></div><div class="line">    stride_data[<span class="number">1</span>] = conv_param.stride_w();</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_stride_dims = conv_param.stride_size();</div><div class="line">    CHECK(num_stride_dims == <span class="number">0</span> || num_stride_dims == <span class="number">1</span> ||</div><div class="line">          num_stride_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"stride must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(stride specified "</span> &lt;&lt; num_stride_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultStride = <span class="number">1</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">      stride_data[i] = (num_stride_dims == <span class="number">0</span>) ? kDefaultStride :</div><div class="line">          conv_param.stride((num_stride_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">      CHECK_GT(stride_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Stride dimensions must be nonzero."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup pad dimensions (pad_).  开始padding的配置</span></div><div class="line">  pad_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* pad_data = pad_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_pad_h() || conv_param.has_pad_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"pad_h &amp; pad_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.pad_size())</div><div class="line">        &lt;&lt; <span class="string">"Either pad or pad_h/w should be specified; not both."</span>;</div><div class="line">    pad_data[<span class="number">0</span>] = conv_param.pad_h();</div><div class="line">    pad_data[<span class="number">1</span>] = conv_param.pad_w();</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_pad_dims = conv_param.pad_size();</div><div class="line">    CHECK(num_pad_dims == <span class="number">0</span> || num_pad_dims == <span class="number">1</span> ||</div><div class="line">          num_pad_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"pad must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(pad specified "</span> &lt;&lt; num_pad_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultPad = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">      pad_data[i] = (num_pad_dims == <span class="number">0</span>) ? kDefaultPad :</div><div class="line">          conv_param.pad((num_pad_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup dilation dimensions (dilation_). 开始膨胀系数的配置</span></div><div class="line">  dilation_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* dilation_data = dilation_.mutable_cpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_dilation_dims = conv_param.dilation_size();</div><div class="line">  CHECK(num_dilation_dims == <span class="number">0</span> || num_dilation_dims == <span class="number">1</span> ||</div><div class="line">        num_dilation_dims == num_spatial_axes_)</div><div class="line">      &lt;&lt; <span class="string">"dilation must be specified once, or once per spatial dimension "</span></div><div class="line">      &lt;&lt; <span class="string">"(dilation specified "</span> &lt;&lt; num_dilation_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">      &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kDefaultDilation = <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    dilation_data[i] = (num_dilation_dims == <span class="number">0</span>) ? kDefaultDilation :</div><div class="line">                       conv_param.dilation((num_dilation_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Special case: im2col is the identity for 1x1 convolution with stride 1</span></div><div class="line">  <span class="comment">// and no padding, so flag for skipping the buffer and transformation.</span></div><div class="line">  is_1x1_ = <span class="literal">true</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    is_1x1_ &amp;=</div><div class="line">        kernel_shape_data[i] == <span class="number">1</span> &amp;&amp; stride_data[i] == <span class="number">1</span> &amp;&amp; pad_data[i] == <span class="number">0</span>;</div><div class="line">    <span class="keyword">if</span> (!is_1x1_) &#123; <span class="keyword">break</span>; &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Configure output channels and groups. 开始配置输出的通道数和卷积组</span></div><div class="line">  channels_ = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_);</div><div class="line">  num_output_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().num_output();</div><div class="line">  CHECK_GT(num_output_, <span class="number">0</span>);</div><div class="line">  group_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().group();</div><div class="line">  CHECK_EQ(channels_ % group_, <span class="number">0</span>);</div><div class="line">  CHECK_EQ(num_output_ % group_, <span class="number">0</span>)</div><div class="line">      &lt;&lt; <span class="string">"Number of output should be multiples of group."</span>;</div><div class="line">  <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">    conv_out_channels_ = channels_;</div><div class="line">    conv_in_channels_ = num_output_;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    conv_out_channels_ = num_output_;</div><div class="line">    conv_in_channels_ = channels_;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Handle the parameters: weights and biases. 开始处理权重参数</span></div><div class="line">  <span class="comment">// - blobs_[0] holds the filter weights       权重参数</span></div><div class="line">  <span class="comment">// - blobs_[1] holds the biases (optional)    偏置参数</span></div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weight_shape(<span class="number">2</span>);</div><div class="line">  weight_shape[<span class="number">0</span>] = conv_out_channels_;          <span class="comment">//输出的通道数</span></div><div class="line">  weight_shape[<span class="number">1</span>] = conv_in_channels_ / group_;  <span class="comment">//输入的通道数，分组卷积，针对多GPU的问题</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    weight_shape.push_back(kernel_shape_data[i]);</div><div class="line">  &#125;</div><div class="line">  bias_term_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().bias_term();</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bias_shape(bias_term_, num_output_);</div><div class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">    CHECK_EQ(<span class="number">1</span> + bias_term_, <span class="keyword">this</span>-&gt;blobs_.size())</div><div class="line">        &lt;&lt; <span class="string">"Incorrect number of weight blobs."</span>;</div><div class="line">    <span class="keyword">if</span> (weight_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape()) &#123;</div><div class="line">      Blob&lt;Dtype&gt; weight_shaped_blob(weight_shape);</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect weight shape: expected shape "</span></div><div class="line">          &lt;&lt; weight_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></div><div class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape_string();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (bias_term_ &amp;&amp; bias_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape()) &#123;</div><div class="line">      Blob&lt;Dtype&gt; bias_shaped_blob(bias_shape);</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect bias shape: expected shape "</span></div><div class="line">          &lt;&lt; bias_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></div><div class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape_string();</div><div class="line">    &#125;</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">2</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// Initialize and fill the weights:</span></div><div class="line">    <span class="comment">// output channels x input channels per-group x kernel height x kernel width</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(weight_shape));</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; weight_filler(GetFiller&lt;Dtype&gt;(</div><div class="line">        <span class="keyword">this</span>-&gt;layer_param_.convolution_param().weight_filler()));</div><div class="line">    weight_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get());</div><div class="line">    <span class="comment">// If necessary, initialize and fill the biases.</span></div><div class="line">    <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(bias_shape));</div><div class="line">      <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; bias_filler(GetFiller&lt;Dtype&gt;(</div><div class="line">          <span class="keyword">this</span>-&gt;layer_param_.convolution_param().bias_filler()));</div><div class="line">      bias_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].get());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  kernel_dim_ = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;count(<span class="number">1</span>); <span class="comment">//是3维度的乘积，输入图像的维度*卷积核的h*卷积核的w</span></div><div class="line">  weight_offset_ = conv_out_channels_ * kernel_dim_ / group_;</div><div class="line">  <span class="comment">// Propagate gradients to the parameters (as directed by backward pass).</span></div><div class="line">  <span class="keyword">this</span>-&gt;param_propagate_down_.resize(<span class="keyword">this</span>-&gt;blobs_.size(), <span class="literal">true</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape函数"><a href="#Reshape函数" class="headerlink" title="Reshape函数"></a><code>Reshape</code>函数</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> first_spatial_axis = channel_axis_ + <span class="number">1</span>;</div><div class="line">  CHECK_EQ(bottom[<span class="number">0</span>]-&gt;num_axes(), first_spatial_axis + num_spatial_axes_)</div><div class="line">      &lt;&lt; <span class="string">"bottom num_axes may not change."</span>;</div><div class="line">  num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>, channel_axis_);</div><div class="line">  CHECK_EQ(bottom[<span class="number">0</span>]-&gt;shape(channel_axis_), channels_)</div><div class="line">      &lt;&lt; <span class="string">"Input size incompatible with convolution kernel."</span>;</div><div class="line">  <span class="comment">// <span class="doctag">TODO:</span> generalize to handle inputs of different shapes.</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">1</span>; bottom_id &lt; bottom.size(); ++bottom_id) &#123;</div><div class="line">    CHECK(bottom[<span class="number">0</span>]-&gt;shape() == bottom[bottom_id]-&gt;shape())</div><div class="line">        &lt;&lt; <span class="string">"All inputs must have the same shape."</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Shape the tops.</span></div><div class="line">  bottom_shape_ = &amp;bottom[<span class="number">0</span>]-&gt;shape();</div><div class="line">  compute_output_shape();</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; top_shape(bottom[<span class="number">0</span>]-&gt;shape().begin(),</div><div class="line">      bottom[<span class="number">0</span>]-&gt;shape().begin() + channel_axis_);</div><div class="line">  top_shape.push_back(num_output_);</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    top_shape.push_back(output_shape_[i]);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</div><div class="line">    top[top_id]-&gt;Reshape(top_shape);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">    conv_out_spatial_dim_ = bottom[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    conv_out_spatial_dim_ = top[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  &#125;</div><div class="line">  col_offset_ = kernel_dim_ * conv_out_spatial_dim_;</div><div class="line">  output_offset_ = conv_out_channels_ * conv_out_spatial_dim_ / group_;</div><div class="line">  <span class="comment">// Setup input dimensions (conv_input_shape_).</span></div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bottom_dim_blob_shape(<span class="number">1</span>, num_spatial_axes_ + <span class="number">1</span>);</div><div class="line">  conv_input_shape_.Reshape(bottom_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* conv_input_shape_data = conv_input_shape_.mutable_cpu_data();</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_ + <span class="number">1</span>; ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">      conv_input_shape_data[i] = top[<span class="number">0</span>]-&gt;shape(channel_axis_ + i);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      conv_input_shape_data[i] = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_ + i);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The im2col result buffer will only hold one image at a time to avoid</span></div><div class="line">  <span class="comment">// overly large memory usage. In the special case of 1x1 convolution</span></div><div class="line">  <span class="comment">// it goes lazily unused to save memory.</span></div><div class="line">  col_buffer_shape_.clear();</div><div class="line">  col_buffer_shape_.push_back(kernel_dim_ * group_);</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">      col_buffer_shape_.push_back(input_shape(i + <span class="number">1</span>));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      col_buffer_shape_.push_back(output_shape_[i]);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  col_buffer_.Reshape(col_buffer_shape_);</div><div class="line">  bottom_dim_ = bottom[<span class="number">0</span>]-&gt;count(channel_axis_);</div><div class="line">  top_dim_ = top[<span class="number">0</span>]-&gt;count(channel_axis_);</div><div class="line">  num_kernels_im2col_ = conv_in_channels_ * conv_out_spatial_dim_;</div><div class="line">  num_kernels_col2im_ = reverse_dimensions() ? top_dim_ : bottom_dim_;</div><div class="line">  <span class="comment">// Set up the all ones "bias multiplier" for adding biases by BLAS</span></div><div class="line">  out_spatial_dim_ = top[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bias_multiplier_shape(<span class="number">1</span>, out_spatial_dim_);</div><div class="line">    bias_multiplier_.Reshape(bias_multiplier_shape);</div><div class="line">    caffe_set(bias_multiplier_.count(), Dtype(<span class="number">1</span>),</div><div class="line">        bias_multiplier_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="forward-cpu-gemm"><a href="#forward-cpu-gemm" class="headerlink" title="forward_cpu_gemm"></a><code>forward_cpu_gemm</code></h2><p>调用<code>conv_im2col_cpu</code>将图像对应卷积的图像块变成列向量，变成矩阵，方便后面进行矩阵乘法<br>这里需要注意的是<code>kernel_dim=输入的通道数*卷积核的高*卷积核的宽</code>。weights的形状是<code>[输出通道数，kernel_dim]</code>。<br>输入的图像的形状是<code>[图像的高\*图像的宽，输入的通道数\*卷积核的高\*卷积核的宽]</code><br>因此weights可以通过乘以输入图像的转置完成矩阵的相乘。</p>
<p>具体可以看上一篇的博文，<a href="https://satisfie.github.io/2016/11/19/Caffe%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-im2col/" target="_blank" rel="external">Caffe中的卷积分析</a>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_gemm(<span class="keyword">const</span> Dtype* input,</div><div class="line">    <span class="keyword">const</span> Dtype* weights, Dtype* output, <span class="keyword">bool</span> skip_im2col) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* col_buff = input;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    <span class="keyword">if</span> (!skip_im2col) &#123;</div><div class="line">      conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line">    col_buff = col_buffer_.cpu_data();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">  </div><div class="line">     <span class="comment">//kernel_dim_=输入的通道数*卷积核的高*卷积核的宽</span></div><div class="line">  	 <span class="comment">//weights的形状是 [输出通道数,kernel_dim_]</span></div><div class="line">  	 </div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, conv_out_channels_ /</div><div class="line">        group_, conv_out_spatial_dim_, kernel_dim_,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g, col_buff + col_offset_ * g,</div><div class="line">        (Dtype)<span class="number">0.</span>, output + output_offset_ * g);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_bias(Dtype* output,</div><div class="line">    <span class="keyword">const</span> Dtype* bias) &#123;</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num_output_,</div><div class="line">      out_spatial_dim_, <span class="number">1</span>, (Dtype)<span class="number">1.</span>, bias, bias_multiplier_.cpu_data(),</div><div class="line">      (Dtype)<span class="number">1.</span>, output);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="backward-cpu-gemm"><a href="#backward-cpu-gemm" class="headerlink" title="backward_cpu_gemm"></a><code>backward_cpu_gemm</code></h2><p>主要是调用<code>conv_col2im_cpu</code>来讲列向量转回图像的形式，具体是调用了<code>col2im_cpu</code>函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::backward_cpu_gemm(<span class="keyword">const</span> Dtype* output,</div><div class="line">    <span class="keyword">const</span> Dtype* weights, Dtype* input) &#123;</div><div class="line">  Dtype* col_buff = col_buffer_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (is_1x1_) &#123;</div><div class="line">    col_buff = input;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans, kernel_dim_,</div><div class="line">        conv_out_spatial_dim_, conv_out_channels_ / group_,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g, output + output_offset_ * g,</div><div class="line">        (Dtype)<span class="number">0.</span>, col_buff + col_offset_ * g);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    conv_col2im_cpu(col_buff, input);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="weight-cpu-gemm"><a href="#weight-cpu-gemm" class="headerlink" title="weight_cpu_gemm"></a><code>weight_cpu_gemm</code></h2><p>这个用于计算weight的导数，具体还要进一步细看</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::weight_cpu_gemm(<span class="keyword">const</span> Dtype* input,</div><div class="line">    <span class="keyword">const</span> Dtype* output, Dtype* weights) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* col_buff = input;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());</div><div class="line">    col_buff = col_buffer_.cpu_data();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasTrans, conv_out_channels_ / group_,</div><div class="line">        kernel_dim_, conv_out_spatial_dim_,</div><div class="line">        (Dtype)<span class="number">1.</span>, output + output_offset_ * g, col_buff + col_offset_ * g,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/19/Caffe源码解读-im2col/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/19/Caffe源码解读-im2col/" itemprop="url">
                  Caffe源码解读--im2col
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-19T19:45:20+08:00">
              2016-11-19
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-19T19:46:46+08:00">
              2016-12-19
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/19/Caffe源码解读-im2col/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/19/Caffe源码解读-im2col/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/19/Caffe源码解读-im2col/" class="leancloud_visitors" data-flag-title="Caffe源码解读--im2col">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Caffe源码解读之卷积篇–im2col"><a href="#Caffe源码解读之卷积篇–im2col" class="headerlink" title="Caffe源码解读之卷积篇–im2col"></a>Caffe源码解读之卷积篇–im2col</h1><p>卷积中比较重要的几个参数有</p>
<ul>
<li>深度,depth，即决定有多少个filters</li>
<li>步长,stride,即卷积核的滑动间隔，默认为1</li>
<li>补充，padding，即在图像两边补充数据</li>
<li>膨胀，dilated, 在卷积的时候可以skip一定长度的像素</li>
</ul>
<p>通常情况下，卷积层后通常需要跟池化层，池化层统计区局部范围内的统计信息，然后再进一步的跟卷积层，这样可以扩大感受野的范围。但是这样会带来一个问题，池化层使得特征层的尺寸变得越来越小,针对层数很深的网络，最后卷积的特征量维度会很小。为了解决这个问题，有几种方法可以尝试：</p>
<p>（1）反卷积，利用上采样扩大特征层的尺度，比如<a href="https://arxiv.org/pdf/1605.06211.pdf" target="_blank" rel="external">FCN</a><br><img src="http://oi8824myj.bkt.clouddn.com/fcn.png" alt="fcn"></p>
<p>FCN网络是一个全卷积神经网络，直接在像素级别上端到端(end-to-end)地进行语义分割。为了解决下采样导致特征层尺度变小的问题，FCN采用双线性插值将响应张亮的长宽上采样到原图大小。</p>
<p>（2）dilated convolution，膨胀卷积的方法，来自于这篇论文<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="external">Multi-Scale Context Aggregation by Dilated Convolutions</a></p>
<p>由于池化层的存在，会使得特征层的尺寸越来越小，若去掉池化层，则感受野的范围就不能进行有效的扩大。在FCN中采用了上采样的方法来扩大特征层的尺寸，但是上采样不能将丢失的信息全部无损地找回来。</p>
<p>dilated convolution是一个很好的解决方法，去掉了池化层，针对感受野的问题，采用了skip的方法，通过skip一定的像素来进行感受野的扩大。</p>
<p>比如下图中，红色圆点对应的是卷积核中的数，绿色范围是感受野的大小。$3\times3$的卷积，图(a)中是默认的1-dilated卷积，感受野是$3\times3$；图(b)中采用2-dilated卷积，感受野为$7\times7$；图(c)中采用4-dilated，感受野是$15\times15$。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/dilated.png" alt="dilated"></p>
<h1 id="Caffe中的im2col"><a href="#Caffe中的im2col" class="headerlink" title="Caffe中的im2col"></a>Caffe中的im2col</h1><p>Reference:</p>
<p><a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">Convolution in Caffe: a memo</a></p>
<p><a href="https://www.zhihu.com/question/28385679" target="_blank" rel="external">在Caffe中如何计算卷积知乎</a></p>
<p>假设宽度为W，高度为H的图像，共有D个通道，则针对$K\times K$的卷积核，并考虑输出的filters个数为M。在Caffe中卷积操作的优化采用了一定的trick,将局部的patch转化成列向量来操作，对于$C\times H\times W$的图像，则对于1个filter,可以转化为$(H\times  W,K\times K\times D)$的矩阵来进行，然后用BLAS库中的<code>Gemm</code>函数来进行举证的乘法操作。</p>
<p>针对1个$K\times K$的卷积核，C个通道的feature map局部patch的转化为列向量如下图所示：<br><img src="http://oi8824myj.bkt.clouddn.com/conv1.png" alt="im2col1"></p>
<p>feature map平移后转化为列向量，如下：<br><img src="http://oi8824myj.bkt.clouddn.com/conv2.png" alt="im2col2"></p>
<p>对于$C\times H \times W$的feature map,得到的矩阵为$(H\times W,C\times K \times K)$</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/conv3.png" alt="im2col3"></p>
<p>针对$C_{out}$个filters，可以得到$(C_{out},C\times K\times K)$的矩阵，然后与转置后的feature map相乘得到$(C_{out},H\times W)$的矩阵。<br><img src="http://oi8824myj.bkt.clouddn.com/conv4.jpg" alt="im2col4"></p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>计算输出的feature map的尺寸，针对的而是2D的卷积，n维的卷积与之类似。</p>
<ul>
<li>输出的高度: output_h = (height + 2 <em> pad_h -(dilation_h </em> (kernel_h - 1) + 1)) / stride_h + 1</li>
<li>输出的宽度: output_w = (width  + 2 <em> pad_w -(dilation_w </em> (kernel_w - 1)+ 1)) / stride_w + 1</li>
<li>一个通道的size: channel_size=height*width</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">im2col_cpu</span><span class="params">(<span class="keyword">const</span> Dtype* data_im, <span class="keyword">const</span> <span class="keyword">int</span> channels,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</div><div class="line">    Dtype* data_col) &#123;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> output_h = (height + <span class="number">2</span> * pad_h -</div><div class="line">    (dilation_h * (kernel_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> output_w = (width + <span class="number">2</span> * pad_w -</div><div class="line">    (dilation_w * (kernel_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> channel_size = height * width;</div><div class="line">  </div><div class="line">  <span class="comment">//安装CHW的顺序进行</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> channel = channels; channel--; data_im += channel_size) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> kernel_row = <span class="number">0</span>; kernel_row &lt; kernel_h; kernel_row++) &#123;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> kernel_col = <span class="number">0</span>; kernel_col &lt; kernel_w; kernel_col++) &#123;</div><div class="line">        <span class="keyword">int</span> input_row = -pad_h + kernel_row * dilation_h;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> output_rows = output_h; output_rows; output_rows--) &#123;</div><div class="line">          <span class="keyword">if</span> (!is_a_ge_zero_and_a_lt_b(input_row, height)) &#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> output_cols = output_w; output_cols; output_cols--) &#123;</div><div class="line">              *(data_col++) = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">int</span> input_col = -pad_w + kernel_col * dilation_w;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> output_col = output_w; output_col; output_col--) &#123;</div><div class="line">              <span class="keyword">if</span> (is_a_ge_zero_and_a_lt_b(input_col, width)) &#123;</div><div class="line">                <span class="comment">//相应的数据copy到data_col指针</span></div><div class="line">                *(data_col++) = data_im[input_row * width + input_col];</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                *(data_col++) = <span class="number">0</span>;</div><div class="line">              &#125;</div><div class="line">              <span class="comment">//宽度上的stride</span></div><div class="line">              input_col += stride_w;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">          <span class="comment">//高度上stride</span></div><div class="line">          input_row += stride_h;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/20/Caffe源码解读2-Blob/" itemprop="url">
                  Caffe源码解读2--Blob
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-20T16:50:01+08:00">
              2016-10-20
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-15T21:42:50+08:00">
              2016-12-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/20/Caffe源码解读2-Blob/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/10/20/Caffe源码解读2-Blob/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/10/20/Caffe源码解读2-Blob/" class="leancloud_visitors" data-flag-title="Caffe源码解读2--Blob">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。在Blob中主要定义了关于数据<code>data_</code>和梯度<code>diff_</code>以及相关的一系列方法,使用的变量都是<code>SyncedMemory</code>的智能指针，所以在解读Blob之前，需要先看上一篇的<a href="">SyncedMemory解读</a></p>
<h1 id="私有保护变量"><a href="#私有保护变量" class="headerlink" title="私有保护变量"></a>私有保护变量</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_;</div><div class="line"> <span class="keyword">int</span> count_;</div><div class="line"> <span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure>
<p>shared_ptr是智能指针，使用引用计数，当计数为0时，自动释放内存。其中</p>
<ul>
<li>data_ : 用来存放正向传播时的权重系数</li>
<li>diff_ : 用来存放反向传播时的梯度差</li>
<li>shape_data_: 用来存储Blob的形状数据的</li>
<li>shape_ : 用来存储Blob的形状数据</li>
<li>count_ : 表示Blob中的元素个数，等于 $num \times channels \times height \times width$</li>
<li>capacity_ : 表示Blob的容量</li>
</ul>
<h1 id="Reshape函数"><a href="#Reshape函数" class="headerlink" title="Reshape函数"></a>Reshape函数</h1><p><code>Reshape</code>能够被调用用来实现 （1）内存的初始化分配；（2）在<code>Layer::Reshape</code>或者<code>Layer::Forward</code>时 用来调节top blob的维度。 当改变blob的size时，只有当原来的内存已经不够了才会重新分配，而创建后多余的内存是不会释放的。需要注意的是，当对输入blob进行<code>reshape</code>时，不能马上调用<code>Net::Backward</code>,因为需要在进行<br><code>reshape</code>后需要调用<code>Net::Forward</code>或者<code>Net::Reshape</code>将新的输入shape传递到更高的层。</p>
<p>Reshape成员函数有4种:</p>
<ul>
<li><code>void Reshape(const int num, const int channels, const int height,const int width);</code></li>
<li><code>void Reshape(const vector&lt;int&gt;&amp; shape);</code></li>
<li><code>void Reshape(const BlobShape&amp; shape);</code></li>
<li><code>void ReshapeLike(const Blob&amp; other);</code></li>
</ul>
<h2 id="Reshape成员函数1"><a href="#Reshape成员函数1" class="headerlink" title="Reshape成员函数1"></a>Reshape成员函数1</h2><p> <code>Reshape(const int num, const int channels, const int height,const int width)</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接用num,channels,height,width来完成reshape,调用了Reshape(const vector&lt;int&gt; &amp;shape)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width) &#123;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape(<span class="number">4</span>);</div><div class="line">  shape[<span class="number">0</span>] = num;</div><div class="line">  shape[<span class="number">1</span>] = channels;</div><div class="line">  shape[<span class="number">2</span>] = height;</div><div class="line">  shape[<span class="number">3</span>] = width;</div><div class="line">  Reshape(shape);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数2"><a href="#Reshape成员函数2" class="headerlink" title="Reshape成员函数2"></a>Reshape成员函数2</h2><p><code>Reshape(const vector&lt;int&gt;&amp; shape)</code>，重点Reshape函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape) &#123;</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * CHECK_LE 位于logging.h文件 767行</div><div class="line">   * 定义为：</div><div class="line">   * #define CHECK_LE(val1, val2) CHECK_OP(_LE, &lt;=, val1, val2)</div><div class="line">   * 用来检查val1&lt;=val2, 用到了GLOG日志库</div><div class="line">   */</div><div class="line">  CHECK_LE(shape.size(), kMaxBlobAxes); <span class="comment">//kMaxBlobAxes定义为shape参数最大的个数，设定为32</span></div><div class="line">  count_ = <span class="number">1</span>; <span class="comment">//开始初始化时赋值为1，因为后面要乘以shape_中的每个元素值</span></div><div class="line">  shape_.resize(shape.size());<span class="comment">//shape_ 开始初始化</span></div><div class="line"> </div><div class="line">  <span class="comment">//shape_data_的初始化和赋值，它是一个SyncedMemory类指针</span></div><div class="line">  <span class="keyword">if</span> (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)) &#123;</div><div class="line">    shape_data_.reset(<span class="keyword">new</span> SyncedMemory(shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">int</span>* shape_data = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>*&gt;(shape_data_-&gt;mutable_cpu_data());<span class="comment">//获得shape_data_的cpu内存地址</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.size(); ++i) &#123;</div><div class="line">    CHECK_GE(shape[i], <span class="number">0</span>); <span class="comment">//检查shape中该参数是否为0</span></div><div class="line">    <span class="keyword">if</span> (count_ != <span class="number">0</span>) &#123;    </div><div class="line">      <span class="comment">//检查乘以shape[i]后，count_是否会超过INT_MAX</span></div><div class="line">      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; <span class="string">"blob size exceeds INT_MAX"</span>; </div><div class="line">    &#125;</div><div class="line">    count_ *= shape[i]; <span class="comment">//统计Blob元素个数= num*channels*height*width</span></div><div class="line">    shape_[i] = shape[i]; <span class="comment">//给成员变量shape_ 赋值</span></div><div class="line">    shape_data[i] = shape[i]; <span class="comment">//给shape_data_赋值</span></div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">//超过容量，设定容量为count_</span></div><div class="line">  <span class="keyword">if</span> (count_ &gt; capacity_) &#123;</div><div class="line">    capacity_ = count_;</div><div class="line">    data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//data_ 进行内存分配</span></div><div class="line">    diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//diff_ 进行内存分配</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数3"><a href="#Reshape成员函数3" class="headerlink" title="Reshape成员函数3"></a>Reshape成员函数3</h2><p><code>Reshape(const BlobShape&amp; shape)</code> 用<code>BlobShape</code>来进行Reshape, <code>BlobShape</code>是在caffe.proto中定义的，用来定义Blob的shape维度的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> BlobShape&amp; shape) &#123;</div><div class="line">  CHECK_LE(shape.dim_size(), kMaxBlobAxes);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_vec(shape.dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.dim_size(); ++i) &#123;</div><div class="line">    shape_vec[i] = shape.dim(i);</div><div class="line">  &#125;</div><div class="line">  Reshape(shape_vec);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape成员函数4"><a href="#Reshape成员函数4" class="headerlink" title="Reshape成员函数4"></a>Reshape成员函数4</h2><p><code>ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)</code>，用其他的Blob参数进行Reshape</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ReshapeLike(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; other) &#123;</div><div class="line">  Reshape(other.shape());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="shape数据输出函数"><a href="#shape数据输出函数" class="headerlink" title="shape数据输出函数"></a>shape数据输出函数</h2><p>两个内敛函数用来输出shape的形状数据</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="built_in">string</span> <span class="title">shape_string</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="built_in">ostringstream</span> stream;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</div><div class="line">    stream &lt;&lt; shape_[i] &lt;&lt; <span class="string">" "</span>;</div><div class="line">  &#125;</div><div class="line">  stream &lt;&lt; <span class="string">"("</span> &lt;&lt; count_ &lt;&lt; <span class="string">")"</span>;</div><div class="line">  <span class="keyword">return</span> stream.str();</div><div class="line">&#125;</div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape() <span class="keyword">const</span> &#123; <span class="keyword">return</span> shape_; &#125;</div></pre></td></tr></table></figure>
<h1 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h1><p>总共声明了3种构造函数：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Blob(): data_(), diff_(), count_(<span class="number">0</span>), capacity_(<span class="number">0</span>) &#123;&#125;</div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width);   </div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</div></pre></td></tr></table></figure>
<p>explict关键字可以防止构造函数的隐式转换,构造函数的实现主要是调用了<code>Reshape</code>函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(num, channels, height, width);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(shape);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="一些变量访问函数"><a href="#一些变量访问函数" class="headerlink" title="一些变量访问函数"></a>一些变量访问函数</h1><ul>
<li><code>num_axes()</code>: 返回shape_ 的size</li>
<li><code>count()</code>:返回count_</li>
<li><code>CanonicalAxisIndex(int axis_index)</code>：返回规范化的坐标，支持负坐标的访问</li>
<li><code>shape(int index)</code>:返回shape_索引处的值</li>
<li><code>LegacyShape(int index)</code>:内部调用shape(intdex)，多了一些合法性检查</li>
<li><code>num()</code>,<code>channels()</code>,<code>height()</code>,<code>width()</code>:分别返回对应的值</li>
</ul>
<h2 id="num-axes"><a href="#num-axes" class="headerlink" title="num_axes()"></a><code>num_axes()</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_axes</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> shape_.size(); &#125;</div></pre></td></tr></table></figure>
<h2 id="count"><a href="#count" class="headerlink" title="count()"></a><code>count()</code></h2><p>这个函数有多个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接返回count_</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> count_; &#125; </div><div class="line"><span class="comment">//计算一个片内的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis, <span class="keyword">int</span> end_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">CHECK_LE(start_axis, end_axis);</div><div class="line">CHECK_GE(start_axis, <span class="number">0</span>);</div><div class="line">CHECK_GE(end_axis, <span class="number">0</span>);</div><div class="line">CHECK_LE(start_axis, num_axes());</div><div class="line">CHECK_LE(end_axis, num_axes());</div><div class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = start_axis; i &lt; end_axis; ++i) &#123;</div><div class="line">  count *= shape(i);</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> count;</div><div class="line">&#125;</div><div class="line"><span class="comment">//给定一个起始，计算到最后片的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">   <span class="keyword">return</span> count(start_axis, num_axes());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="CanonicalAxisIndex"><a href="#CanonicalAxisIndex" class="headerlink" title="CanonicalAxisIndex()"></a><code>CanonicalAxisIndex()</code></h2><p>用来进行坐标的规范化，和Python一样，支持负数的访问。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">CanonicalAxisIndex</span><span class="params">(<span class="keyword">int</span> axis_index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_GE(axis_index, -num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  CHECK_LT(axis_index, num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  <span class="keyword">if</span> (axis_index &lt; <span class="number">0</span>) &#123;</div><div class="line">    <span class="keyword">return</span> axis_index + num_axes();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> axis_index;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="int-shape-int-intdex"><a href="#int-shape-int-intdex" class="headerlink" title="int shape(int intdex)"></a><code>int shape(int intdex)</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">shape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> shape_[CanonicalAxisIndex(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="LegacyShape-int-index"><a href="#LegacyShape-int-index" class="headerlink" title="LegacyShape(int index)"></a><code>LegacyShape(int index)</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">LegacyShape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(num_axes(), <span class="number">4</span>)</div><div class="line">      &lt;&lt; <span class="string">"Cannot use legacy accessors on Blobs with &gt; 4 axes."</span>;</div><div class="line">  CHECK_LT(index, <span class="number">4</span>);</div><div class="line">  CHECK_GE(index, <span class="number">-4</span>);</div><div class="line">  <span class="keyword">if</span> (index &gt;= num_axes() || index &lt; -num_axes()) &#123;</div><div class="line">    <span class="comment">// Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse</span></div><div class="line">    <span class="comment">// indexing) -- this special case simulates the one-padding used to fill</span></div><div class="line">    <span class="comment">// extraneous axes of legacy blobs.</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape(index);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="num-channels-height-width"><a href="#num-channels-height-width" class="headerlink" title="num(),channels(),height(),width()"></a><code>num()</code>,<code>channels()</code>,<code>height()</code>,<code>width()</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor num: use shape(0) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">0</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor channels: use shape(1) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">channels</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">1</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor height: use shape(2) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">2</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor width: use shape(3) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">width</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">3</span>); &#125;</div></pre></td></tr></table></figure>
<h1 id="CPU和GPU中数据的获得"><a href="#CPU和GPU中数据的获得" class="headerlink" title="CPU和GPU中数据的获得"></a>CPU和GPU中数据的获得</h1><p>以下是一些get和set函数</p>
<ul>
<li><code>const Dtype* cpu_data() const;</code></li>
<li><code>void set_cpu_data(Dtype* data);</code></li>
<li><code>const int* gpu_shape() const;</code></li>
<li><code>const Dtype* gpu_data() const;</code></li>
<li><code>const Dtype* cpu_diff() const;</code></li>
<li><code>const Dtype* gpu_diff() const;</code></li>
<li><code>Dtype* mutable_cpu_data();</code></li>
<li><code>Dtype* mutable_gpu_data();</code></li>
<li><code>Dtype* mutable_cpu_diff();</code></li>
<li><code>Dtype* mutable_gpu_diff();</code></li>
</ul>
<h2 id="cpu-data"><a href="#cpu-data" class="headerlink" title="cpu_data()"></a><code>cpu_data()</code></h2><p>返回数据<code>data_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="set-cpu-data"><a href="#set-cpu-data" class="headerlink" title="set_cpu_data()"></a><code>set_cpu_data()</code></h2><p>设定<code>data_</code>在cpu中的数据，直接用指针替换的方式</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) &#123;</div><div class="line">  CHECK(data);</div><div class="line">  data_-&gt;set_cpu_data(data); <span class="comment">//在Syscedmem.cpp中实现，用cpu_ptr_ = data实现替换</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-shape"><a href="#gpu-shape" class="headerlink" title="gpu_shape"></a><code>gpu_shape</code></h2><p>返回的是shape_data_中存储的gpu数据</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span>* Blob&lt;Dtype&gt;::gpu_shape() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(shape_data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">int</span>*)shape_data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-data"><a href="#gpu-data" class="headerlink" title="gpu_data()"></a><code>gpu_data()</code></h2><p>返回数据<code>data_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="cpu-diff"><a href="#cpu-diff" class="headerlink" title="cpu_diff()"></a><code>cpu_diff()</code></h2><p>返回梯度<code>diff_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="gpu-diff"><a href="#gpu-diff" class="headerlink" title="gpu_diff()"></a><code>gpu_diff()</code></h2><p>返回梯度<code>diff_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="mutable版本的"><a href="#mutable版本的" class="headerlink" title="mutable版本的"></a>mutable版本的</h2><p>下面这4个函数与上面类似，不同之处在于mutable，可以对其进行修改，而当面的返回形式是const，不可修改的</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="具体offset位置处的访问"><a href="#具体offset位置处的访问" class="headerlink" title="具体offset位置处的访问"></a>具体offset位置处的访问</h1><p>先需要用<code>offset()</code>函数计算具体的位置index，然后对<code>data_</code>和<code>diff_</code>具体index处进行访问,主要的函数有</p>
<ul>
<li><code>offset(const int n, const int c = 0, const int h = 0, const int w = 0)</code></li>
<li><code>offset(const vector&lt;int&gt;&amp; indices)</code></li>
<li><code>data_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>diff_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>data_at(const vector&lt;int&gt;&amp; index)</code></li>
<li><code>diff_at(const vector&lt;int&gt;&amp; index)</code></li>
</ul>
<h2 id="offset"><a href="#offset" class="headerlink" title="offset()"></a><code>offset()</code></h2><p><code>offset()</code>函数有两个实现的版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c = <span class="number">0</span>, <span class="keyword">const</span> <span class="keyword">int</span> h = <span class="number">0</span>,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w = <span class="number">0</span>) <span class="keyword">const</span> &#123;</div><div class="line">  CHECK_GE(n, <span class="number">0</span>);</div><div class="line">  CHECK_LE(n, num());</div><div class="line">  CHECK_GE(channels(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(c, channels());</div><div class="line">  CHECK_GE(height(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(h, height());</div><div class="line">  CHECK_GE(width(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(w, width());</div><div class="line">  <span class="keyword">return</span> ((n * channels() + c) * height() + h) * width() + w;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//用[n,c,h,w]的vector向量实现</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; indices)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(indices.size(), num_axes());</div><div class="line">  <span class="keyword">int</span> offset = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_axes(); ++i) &#123;</div><div class="line">    offset *= shape(i);</div><div class="line">    <span class="keyword">if</span> (indices.size() &gt; i) &#123;</div><div class="line">      CHECK_GE(indices[i], <span class="number">0</span>);</div><div class="line">      CHECK_LT(indices[i], shape(i));</div><div class="line">      offset += indices[i]; </div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> offset;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="data-at-和diff-data-函数的访问"><a href="#data-at-和diff-data-函数的访问" class="headerlink" title="data_at()和diff_data()函数的访问"></a><code>data_at()</code>和<code>diff_data()</code>函数的访问</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(index)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="返回data-和diff-指针"><a href="#返回data-和diff-指针" class="headerlink" title="返回data_和diff_指针"></a>返回<code>data_</code>和<code>diff_</code>指针</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> data_;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> diff_;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="data-的更新"><a href="#data-的更新" class="headerlink" title="data_的更新"></a><code>data_</code>的更新</h1><p>对于<code>data_</code>的更新，一般是对其减去反向传播的<code>diff_</code>乘以相应的系数，这里主要用到的函数有</p>
<ul>
<li><code>void Update()</code>: 用来对<code>data_</code>进行更新</li>
<li><code>Dtype asum_data() const</code>: 对<code>data_</code>求绝对值之和，即L1范数</li>
<li><code>Dtype asum_diff() const</code>: 对<code>diff_</code>求L1范数</li>
<li><code>Dtype sumsq_data() const</code>:对<code>data_</code>求平方和之和，即L2范数</li>
<li><code>Dtype sumsq_diff() const</code>:对<code>diff_</code>求L2范数</li>
<li><code>void scale_data(Dtype scale_factor)</code>:对<code>data_</code>乘以相应的标量</li>
<li><code>void scale_diff(Dtype scale_factor)</code>:对<code>diff_</code>乘以相应的标量</li>
</ul>
<h2 id="Update-方法"><a href="#Update-方法" class="headerlink" title="Update()方法"></a><code>Update()</code>方法</h2><p><code>Updata()</code>方法组要是用来对<code>data_</code>进行<code>diff_</code>的更新，主要是封装了cblas和cublas中的版本，其中里面分别有针对<code>float</code>和<code>double</code>版本的。同样，<code>Updata()</code>方法是针对Blob<float>和Blob<double>版本的，因此没有实现<code>int</code>版本和<code>unsigned int</code>版本</double></float></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Update() &#123;</div><div class="line">  <span class="comment">// We will perform update based on where the data is located.</span></div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:  <span class="comment">//对于cpu中的数据,调用caffe_axpy()</span></div><div class="line">    <span class="comment">// perform computation on CPU</span></div><div class="line">    caffe_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;cpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY                     </span></div><div class="line">    <span class="comment">// perform computation on GPU  //对于gpu中的数据调用caffe_gpu_axpy()</span></div><div class="line">    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;gpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;     <span class="comment">//log报错                   </span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Syncedmem not initialized."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在<code>Update</code>函数中调用了<code>caffe_axpy</code>,分别封装了cpu实现版本<code>cblas_saxpy</code>,主要是调用了cblas中的函数；<br>gpu实现版本<code>caffe_gpu_axpy</code>，主要是调用了cublas中的函数。</p>
<p>两者的声明在/caffe/util/math_function.hpp中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div></pre></td></tr></table></figure>
<p>cpu版本的实现只有一种，位于/caffe/util/math_function.cpp中</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123; cblas_saxpy(N, alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>); &#125;</div></pre></td></tr></table></figure>
<p>在<code>cblas_saxpy</code>函数中，N是这个向量的元素个数，在Blob中就是<code>count_</code>。alpha是X前面的系数，X,Y 是输入的矢量。其中的1和1分别是X,Y的步进，这里每个元素都要更新，所以是1。函数实现的功能是</p>
<p>\begin{equation}<br>Y=alpha * X+Y<br>\end{equation}</p>
<p>gpu版本的实现有两种,位于/caffe/util/math_function.cu中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">const</span> <span class="keyword">double</span>* X,</div><div class="line">    <span class="keyword">double</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="求L1范数"><a href="#求L1范数" class="headerlink" title="求L1范数"></a>求L1范数</h2><p><code>asum_data()</code>函数用来计算data数据的绝对值之和(L1范数),<code>asum_diff()</code>用来计算梯度数据diff的L1范数。<br>主要是调用了cpu版本的<code>caffe_cpu_asum</code>和gpu版本的<code>caffe_gpu_asum</code>，这两个函数同样分别对cblas和cublas中的函数进行了封装。</p>
<p><code>asum_data()</code>的实现如下，<code>asum_diff()</code>的实现类似，无非是将<code>data_</code>指针换成了<code>diff_</code>指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    <span class="keyword">return</span> caffe_cpu_asum(count_, cpu_data());</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  &#123;</div><div class="line">    Dtype asum;</div><div class="line">    caffe_gpu_asum(count_, gpu_data(), &amp;asum);</div><div class="line">    <span class="keyword">return</span> asum;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中,<code>caffe_cpu_asum</code>和<code>caffe_gpu_asum</code>的声明如下，位于/caffe/util/math_function.hpp</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, Dtype* y)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_asum</code>的实现在/caffe/util/math_function.cpp中,针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sasum(n, x, <span class="number">1</span>);<span class="comment">//用来计算向量x的和，共有n个元素，1是stride,每个元素都用所以取1</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_dasum(n, x, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_asum</code>的实现在/caffe/util/math_function.cu中，针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//gpu的计算使用cublas</span></div><div class="line"><span class="keyword">template</span> &lt;&gt; </div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">float</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">double</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h2><p><code>sumq_data()</code>函数用来计算data数据的平方和(L2范数),<code>sumsq_diff()</code> 用来计算梯度数据diff的L2范数。<br>主要是调用了cpu版本的<code>caffe_cpu_dout</code>和gpu版本的<code>caffe_gpu_dot</code>函数。</p>
<p><code>sumq_data()</code>的实现如下，<code>sumsq_diff()</code>与之类似：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  Dtype sumsq;</div><div class="line">  <span class="keyword">const</span> Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = cpu_data();</div><div class="line">    sumsq = caffe_cpu_dot(count_, data, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = gpu_data();</div><div class="line">    caffe_gpu_dot(count_, data, data, &amp;sumsq); <span class="comment">//gpu版本</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> sumsq;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中<code>caffe_cpu_dout</code>和<code>caffe_gpu_dot</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_dot</code>的实现调用了</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> caffe_cpu_strided_dot(n, x, <span class="number">1</span>, y, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_strided_dot</code>是一个模板函数，对<code>cblas_sdot</code>和<code>cblas_ddot</code>进行了封装，其声明和实现分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_strided_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</span></span></div><div class="line">    <span class="keyword">const</span> Dtype* y, <span class="keyword">const</span> <span class="keyword">int</span> incy);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_strided_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sdot(n, x, incx, y, incy);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_strided_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> incx, <span class="keyword">const</span> <span class="keyword">double</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_ddot(n, x, incx, y, incy);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_dot</code>是一个模板函数，它的声明如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p>该函数有<code>float</code>和<code>double</code>两个实现版本,位于math_functions.cu中，实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">float</span>* y,</div><div class="line">    <span class="keyword">float</span>* out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">const</span> <span class="keyword">double</span>* y,</div><div class="line">    <span class="keyword">double</span> * out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="scale-data和scale-diff"><a href="#scale-data和scale-diff" class="headerlink" title="scale_data和scale_diff"></a><code>scale_data</code>和<code>scale_diff</code></h2><p><code>scale_data</code>函数和<code>scale_diff</code>函数主要是对Blob内的<code>data_</code>向量或者<code>diff_</code>向量乘以一个标量。<br>主要调用的两个函数cpu版本的<code>caffe_scal()</code>和gpu版本的<code>caffe_gpu_scal()</code></p>
<p><code>scale_data</code>的实现如下,<code>scale_diff</code>与之类似。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::scale_data(<span class="keyword">unsigned</span> <span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::scale_data(<span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) &#123;</div><div class="line">  Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = mutable_cpu_data();</div><div class="line">    caffe_scal(count_, scale_factor, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = mutable_gpu_data();</div><div class="line">    caffe_gpu_scal(count_, scale_factor, data); <span class="comment">//gpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_scal</code>的模板函数声明为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p>对应的<code>float</code>和<code>double</code>实现版本如下，分别是对<code>cblas_sscal</code>和<code>cblas_dscal</code>函数的调用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  cblas_sscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  cblas_dscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的实现同样有两个版本：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="其他的一些函数"><a href="#其他的一些函数" class="headerlink" title="其他的一些函数"></a>其他的一些函数</h1><ul>
<li><code>ShareData</code></li>
<li><code>ShareDiff</code></li>
<li><code>ShapeEquals</code></li>
<li><code>CopyFrom</code></li>
<li><code>FromProto</code></li>
<li><code>ToProto</code></li>
</ul>
<h2 id="ShareData和ShareDiff"><a href="#ShareData和ShareDiff" class="headerlink" title="ShareData和ShareDiff"></a><code>ShareData</code>和<code>ShareDiff</code></h2><p><code>ShareData</code>和<code>ShareDiff</code>实现方式是直接将<code>data_</code>和<code>diff_</code>的指针替换成其他类中的指针。这可以简化Layer中前向传递时只是简单的copy的情况。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareData(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  data_ = other.data();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareDiff(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  diff_ = other.diff();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="ShapeEquals"><a href="#ShapeEquals" class="headerlink" title="ShapeEquals"></a><code>ShapeEquals</code></h2><p>Blob的shape是否相同的检查</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">bool</span> Blob&lt;Dtype&gt;::ShapeEquals(<span class="keyword">const</span> BlobProto&amp; other) &#123;</div><div class="line">  <span class="keyword">if</span> (other.has_num() || other.has_channels() ||</div><div class="line">      other.has_height() || other.has_width()) &#123;</div><div class="line">    <span class="comment">// Using deprecated 4D Blob dimensions --</span></div><div class="line">    <span class="comment">// shape is (num, channels, height, width).</span></div><div class="line">    <span class="comment">// Note: we do not use the normal Blob::num(), Blob::channels(), etc.</span></div><div class="line">    <span class="comment">// methods as these index from the beginning of the blob shape, where legacy</span></div><div class="line">    <span class="comment">// parameter blobs were indexed from the end of the blob shape (e.g., bias</span></div><div class="line">    <span class="comment">// Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).</span></div><div class="line">    <span class="keyword">return</span> shape_.size() &lt;= <span class="number">4</span> &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-4</span>) == other.num() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-3</span>) == other.channels() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-2</span>) == other.height() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-1</span>) == other.width();</div><div class="line">  &#125;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; other_shape(other.shape().dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; other.shape().dim_size(); ++i) &#123;</div><div class="line">    other_shape[i] = other.shape().dim(i);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape_ == other_shape;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="CopyFrom"><a href="#CopyFrom" class="headerlink" title="CopyFrom"></a><code>CopyFrom</code></h2><p><code>CopyFrom</code>的声明如下，其中<code>copy_diff</code>为<code>false</code>,则拷贝的是data,否则拷贝diff</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></div><div class="line">    <span class="keyword">bool</span> reshape = <span class="literal">false</span>);</div></pre></td></tr></table></figure>
<p>实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, <span class="keyword">bool</span> reshape) &#123;</div><div class="line">  <span class="comment">//先做size检查</span></div><div class="line">  <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</div><div class="line">    <span class="keyword">if</span> (reshape) &#123;</div><div class="line">      ReshapeLike(source);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Trying to copy blobs of different sizes."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.gpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.gpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.cpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.cpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中，<code>copy_copy</code>封装了cpu内存之间的内存copy和gpu内存的copy</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_copy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (X != Y) &#123;</div><div class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">      <span class="comment">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></div><div class="line">      CUDA_CHECK(cudaMemcpy(Y, X, <span class="keyword">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">      NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">memcpy</span>(Y, X, <span class="keyword">sizeof</span>(Dtype) * N);  <span class="comment">// NOLINT(caffe/alt_fn)</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="FromProto和ToProto"><a href="#FromProto和ToProto" class="headerlink" title="FromProto和ToProto"></a><code>FromProto</code>和<code>ToProto</code></h2><p><code>FromProto</code>用proto文件来实现Blob的初始化,<code>ToProto</code>是将Blob的内容写入到proto文件</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总而言之，Blob相对而言是一个比较简单的类，在看懂<code>SyncedMemory</code>以后，就比较容易看懂这个类了，里面的各种调用数学函数的声明都是在/caffe/util/math_functions.hpp中，实现分别是在math_functions.cpp和math_functions.cu中。</p>
<p>在类的最后可以看到 <code>DISABLE_COPY_AND_ASSIGN(Blob)</code>,这是一个宏函数，可以看到它的实现,主要是禁止这个类的拷贝和赋值操作。是为了防止两个大型数据结构内容的复制和赋值，如果要使用，应该是使用指针和引用来指向。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Disable the copy and assignment operator for a class.</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> DISABLE_COPY_AND_ASSIGN(classname) \</span></div><div class="line">private:\</div><div class="line">  classname(const classname&amp;);\</div><div class="line">  classname&amp; operator=(const classname&amp;)</div></pre></td></tr></table></figure>
<h4 id="接下去将对Layer进行分析…"><a href="#接下去将对Layer进行分析…" class="headerlink" title="接下去将对Layer进行分析….."></a>接下去将对Layer进行分析…..</h4>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/10/15/Caffe源码解读1——SyncedMemory/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/15/Caffe源码解读1——SyncedMemory/" itemprop="url">
                  Caffe源码解读1——SyncedMemory
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-15T17:30:43+08:00">
              2016-10-15
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-15T21:42:43+08:00">
              2016-12-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/15/Caffe源码解读1——SyncedMemory/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/10/15/Caffe源码解读1——SyncedMemory/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/10/15/Caffe源码解读1——SyncedMemory/" class="leancloud_visitors" data-flag-title="Caffe源码解读1——SyncedMemory">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Caffe源码解读1–SyncedMemory"><a href="#Caffe源码解读1–SyncedMemory" class="headerlink" title="Caffe源码解读1–SyncedMemory"></a>Caffe源码解读1–SyncedMemory</h1><p>这个类主要用来做内存的分配和同步，代码量较少，包含<code>syncedmem.hpp</code> 和 <code>syncedmem.cpp</code>，这个类相对比较简单易懂~</p>
<p>首先简单介绍一下Pinned Memory和Non-Pinned Memory,详细的介绍可以参见我上一篇博客<a href=""></a></p>
<p>为了在CPU和GPU之间传输内存，关于CPU的内存分配和释放方式有以下两种：</p>
<ul>
<li>通过C标准库中的<code>malloc</code>函数完成内存分配，<code>free</code>进行内存释放</li>
<li>调用CUDA中的<code>cudaMallocHost</code>函数完成内存分配，<code>cudaFreeHost</code>进行内存释放</li>
</ul>
<p><code>malloc</code>和<code>free</code>的优点是分配和释放的耗时少，缺点是CPU和GPU之间的传输相比而言比较慢。<code>cudaMallocHost</code>和<code>cudaFreeHost</code>正好相反，优点是CPU和GPU之间的传输快，缺点是分配和释放内存比较耗时。</p>
<p>在Caffe中，让在GPU模式和CUDA可用的情况下，采用Pinned Memory方式，即使用<code>cudaMallocHost</code>和<code>cudaFreeHost</code>来进行CPU内存的分配和释放。这在单个GPU的时候可能效果并不明显，但是更利于并行训练，更重要的是，在多个GPU上，这种方式更加稳定。</p>
<p>在Caffe中，对CPU内存分配方式的封装如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeMallocHost</span><span class="params">(<span class="keyword">void</span>** ptr, <span class="keyword">size_t</span> size, <span class="keyword">bool</span>* use_cuda)</span> </span>&#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123; <span class="comment">//GPU模式下使用cudaMallocHost</span></div><div class="line">    CUDA_CHECK(cudaMallocHost(ptr, size));</div><div class="line">    *use_cuda = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  *ptr = <span class="built_in">malloc</span>(size);    <span class="comment">//CPU模式下使用malloc</span></div><div class="line">  *use_cuda = <span class="literal">false</span>;</div><div class="line">  CHECK(*ptr) &lt;&lt; <span class="string">"host allocation of size "</span> &lt;&lt; size &lt;&lt; <span class="string">" failed"</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeFreeHost</span><span class="params">(<span class="keyword">void</span>* ptr, <span class="keyword">bool</span> use_cuda)</span> </span>&#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY         <span class="comment">//GPU模式下使用cudaFreeHost</span></span></div><div class="line">  <span class="keyword">if</span> (use_cuda) &#123;</div><div class="line">    CUDA_CHECK(cudaFreeHost(ptr));</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="built_in">free</span>(ptr);            <span class="comment">//CPU模式下使用free</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>SyncedMemory</code>类的私有成员变量有以下几个：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span>* cpu_ptr_;   <span class="comment">//数据在cpu的指针</span></div><div class="line"><span class="keyword">void</span>* gpu_ptr_;   <span class="comment">//数据在gpu的指针</span></div><div class="line"><span class="keyword">size_t</span> size_;     <span class="comment">//数据的大小</span></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"> *用来表示数据的同步状态，有4种状态，分别是未初始化的，数据在cpu中，数据在gpu中，cpu和gpu都有。</div><div class="line"> *enum SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;; </div><div class="line"> */</div><div class="line">SyncedHead head_;   </div><div class="line"><span class="keyword">bool</span> own_cpu_data_;   <span class="comment">//是否是自己的cpu数据</span></div><div class="line"><span class="keyword">bool</span> cpu_malloc_use_cuda_; <span class="comment">//</span></div><div class="line"><span class="keyword">bool</span> own_gpu_data_;   <span class="comment">//是否有gpu数据</span></div><div class="line"><span class="keyword">int</span> gpu_device_;</div></pre></td></tr></table></figure>
<p>构造函数和析构函数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">SyncedMemory()</div><div class="line">    : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(0), head_(UNINITIALIZED),</div><div class="line">      own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">      gpu_device_(-1) &#123;&#125;</div><div class="line">explicit SyncedMemory(size_t size)</div><div class="line">    : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(size), head_(UNINITIALIZED),</div><div class="line">      own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">      gpu_device_(-1) &#123;&#125;</div><div class="line">~SyncedMemory();</div></pre></td></tr></table></figure>
<p>两个内敛私有函数<code>to_cpu()</code>和<code>to_gpu()</code>:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_cpu() &#123;</div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED: <span class="comment">//如果是未初始化状态，则进行cpu内存分配</span></div><div class="line">    CaffeMallocHost(&amp;cpu_ptr_, size_, &amp;cpu_malloc_use_cuda_);</div><div class="line">    caffe_memset(size_, <span class="number">0</span>, cpu_ptr_);</div><div class="line">    head_ = HEAD_AT_CPU;   <span class="comment">//设置状态为“数据在cpu”</span></div><div class="line">    own_cpu_data_ = <span class="literal">true</span>;  <span class="comment">//cpu拥有数据置为真</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:  <span class="comment">//如果是gpu拥有数据 </span></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY      <span class="comment">//如果有GPU存在</span></span></div><div class="line">    <span class="keyword">if</span> (cpu_ptr_ == <span class="literal">NULL</span>) &#123; <span class="comment">//如果cpu还未分配内存空间，则进行分配</span></div><div class="line">      CaffeMallocHost(&amp;cpu_ptr_, size_, &amp;cpu_malloc_use_cuda_);</div><div class="line">      own_cpu_data_ = <span class="literal">true</span>; <span class="comment">//cpu拥有数据置为真</span></div><div class="line">    &#125;</div><div class="line">    caffe_gpu_memcpy(size_, gpu_ptr_, cpu_ptr_); <span class="comment">//内存从gpu拷贝到cpu</span></div><div class="line">    head_ = SYNCED;         <span class="comment">//设置状态为"数据在cpu和gpu都拥有"</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;       <span class="comment">//否则log报错，这是宏定义</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:     <span class="comment">//CPU 已经拥有数据，则直接跳过</span></div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>to_gpu()</code>:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_gpu() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED:                          <span class="comment">//如果状态是未初始化</span></div><div class="line">    CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); <span class="comment">//分配GPU显存</span></div><div class="line">    caffe_gpu_memset(size_, <span class="number">0</span>, gpu_ptr_);</div><div class="line">    head_ = HEAD_AT_GPU;                      <span class="comment">//设置状态为在GPU</span></div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:                           <span class="comment">//如果状态是在CPU</span></div><div class="line">    <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); <span class="comment">//分配GPU显存  </span></div><div class="line">      own_gpu_data_ = <span class="literal">true</span>;            </div><div class="line">    &#125;</div><div class="line">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);   <span class="comment">//将数据从CPU传输到GPU</span></div><div class="line">    head_ = SYNCED;   						           <span class="comment">//状态设置为CPU和GPU同步</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:                            <span class="comment">//如果GPU已经拥有数据，则跳过</span></div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;                         <span class="comment">//如果没有GPU，则log报错</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>获取cpu和gpu内存地址的方法</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</div></pre></td></tr></table></figure>
<p>实现分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">void</span>* SyncedMemory::cpu_data() &#123;</div><div class="line">  to_cpu();        <span class="comment">//保证cpu内存存在数据</span></div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">void</span>*)cpu_ptr_; <span class="comment">//返回数据地址指针</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">void</span>* SyncedMemory::gpu_data() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY     <span class="comment">//如果是GPU模式</span></span></div><div class="line">  to_gpu();          <span class="comment">//保证GPU内存中存在数据</span></div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">void</span>*)gpu_ptr_;    <span class="comment">//返回GPU中数据地址指针</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;            <span class="comment">//GPU不存在则log报错</span></div><div class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;      </div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>mutable_cpu_data()</code>和<code>mutable_gpu_data()</code>与上面两个函数类似，只是多了设置同步状态,实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span>* SyncedMemory::mutable_cpu_data() &#123;</div><div class="line">  to_cpu();</div><div class="line">  head_ = HEAD_AT_CPU;      <span class="comment">//设置装填为数据在CPU</span></div><div class="line">  <span class="keyword">return</span> cpu_ptr_;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span>* SyncedMemory::mutable_gpu_data() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  to_gpu();</div><div class="line">  head_ = HEAD_AT_GPU;     <span class="comment">//设置状态为数据在GPU</span></div><div class="line">  <span class="keyword">return</span> gpu_ptr_;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;</div><div class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>还有两个设置cpu内存数据和设置gpu内存数据的函数 <code>set_cpu_data(void* data)</code>和<code>set_gpu_data(void* data)</code>,其实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">//cpu的data指针指向一个新的区域由data指针传入，并且将原来申请的内存释放</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::set_cpu_data(<span class="keyword">void</span>* data) &#123;</div><div class="line">  CHECK(data);</div><div class="line">  <span class="keyword">if</span> (own_cpu_data_) &#123;       <span class="comment">//如果cpu内存有数据，则释放</span></div><div class="line">    CaffeFreeHost(cpu_ptr_, cpu_malloc_use_cuda_);</div><div class="line">  &#125;</div><div class="line">  cpu_ptr_ = data;         <span class="comment">//设置</span></div><div class="line">  head_ = HEAD_AT_CPU;     <span class="comment">//设置状态为数据在CPU</span></div><div class="line">  own_cpu_data_ = <span class="literal">false</span>;  </div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::set_gpu_data(<span class="keyword">void</span>* data) &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  CHECK(data);</div><div class="line">  <span class="keyword">if</span> (own_gpu_data_) &#123;</div><div class="line">    <span class="keyword">int</span> initial_device;</div><div class="line">    cudaGetDevice(&amp;initial_device);</div><div class="line">    <span class="keyword">if</span> (gpu_device_ != <span class="number">-1</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaSetDevice(gpu_device_));</div><div class="line">    &#125;</div><div class="line">    CUDA_CHECK(cudaFree(gpu_ptr_));</div><div class="line">    cudaSetDevice(initial_device);</div><div class="line">  &#125;</div><div class="line">  gpu_ptr_ = data;</div><div class="line">  head_ = HEAD_AT_GPU;</div><div class="line">  own_gpu_data_ = <span class="literal">false</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后还有一个异步传输的函数<code>async_gpu_push(const cudaStream_t&amp; stream)</code>,cuda拷贝的异步传输，从数据从cpu拷贝到gpu,实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::async_gpu_push(<span class="keyword">const</span> cudaStream_t&amp; stream) &#123;</div><div class="line">  CHECK(head_ == HEAD_AT_CPU);</div><div class="line">  <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">    CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> cudaMemcpyKind put = cudaMemcpyHostToDevice;</div><div class="line">  CUDA_CHECK(cudaMemcpyAsync(gpu_ptr_, cpu_ptr_, size_, put, stream));</div><div class="line">  <span class="comment">// Assume caller will synchronize on the stream before use</span></div><div class="line">  head_ = SYNCED;</div><div class="line">&#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/10/03/Protocol-Buffers-详解1/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/03/Protocol-Buffers-详解1/" itemprop="url">
                  Protocol Buffers 详解1
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-03T17:06:15+08:00">
              2016-10-03
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-15T21:42:35+08:00">
              2016-12-15
            </time>
            
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/OpenSources-tools/" itemprop="url" rel="index">
                    <span itemprop="name">OpenSources tools</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/03/Protocol-Buffers-详解1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/10/03/Protocol-Buffers-详解1/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/10/03/Protocol-Buffers-详解1/" class="leancloud_visitors" data-flag-title="Protocol Buffers 详解1">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Protocol-Buffers-详解1"><a href="#Protocol-Buffers-详解1" class="headerlink" title="Protocol Buffers 详解1"></a>Protocol Buffers 详解1</h1><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h3><p><a href="https://github.com/google/protobuf" target="_blank" rel="external">Protobuf Git</a></p>
<p><a href="https://github.com/google/protobuf/blob/master/src/README.md" target="_blank" rel="external">Protobuf 安装</a></p>
<p><a href="https://developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="external">Google Developer Guide</a></p>
<p><a href="https://developers.google.com/protocol-buffers/docs/proto" target="_blank" rel="external">proto文件格式</a></p>
<p>下面这个翻译已经翻译的很详细</p>
<p><a href="http://blog.csdn.net/cchd0001/article/details/50669079" target="_blank" rel="external">Protobuf 语言指南翻译</a></p>
<p>Protocol Buffers 是一种与语言无关，平台无关，可扩展的轻便高效的数据存储格式，用于结构化数据的序列化，适用于通信协议，数据存储等领域，包括C++,Java,Python,C#等多种语言支持。</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul>
<li>提供灵活高效的结构化数据序列化</li>
<li>类似与XML，但是更小，更快，更简单</li>
<li>在proto文件中定义数据结构后，可以用Protobuf编译器编译成各种目标语言</li>
</ul>
<h2 id="Proto-文件的书写"><a href="#Proto-文件的书写" class="headerlink" title="Proto 文件的书写"></a>Proto 文件的书写</h2><h3 id="定义-Message-类型"><a href="#定义-Message-类型" class="headerlink" title="定义 Message 类型"></a>定义 Message 类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">message SearchRequest&#123;</div><div class="line">	required string query=1;</div><div class="line">	optional int32 page_number=2;</div><div class="line">	optional int32 result_per_page=3;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如下的message类型定了3种特定的name/value字段</p>
<h4 id="字段的类型"><a href="#字段的类型" class="headerlink" title="字段的类型"></a>字段的类型</h4><ul>
<li>标量类型: 包括double,float,int32,int64,uint32,uint64,string,bool,bytes等</li>
<li>枚举类型：例如 </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">message SearchRequest &#123;</div><div class="line">  required string query = 1;</div><div class="line">  optional int32 page_number = 2;</div><div class="line">  optional int32 result_per_page = 3 [default = 10];</div><div class="line">  enum Corpus &#123;</div><div class="line">    UNIVERSAL = 0;</div><div class="line">    WEB = 1;</div><div class="line">    IMAGES = 2;</div><div class="line">    LOCAL = 3;</div><div class="line">    NEWS = 4;</div><div class="line">    PRODUCTS = 5;</div><div class="line">    VIDEO = 6;</div><div class="line">  &#125;</div><div class="line">  optional Corpus corpus = 4 [default = UNIVERSAL];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>其他类型</li>
</ul>
<h4 id="字段的tag"><a href="#字段的tag" class="headerlink" title="字段的tag"></a>字段的tag</h4><p>数字tag，用于标识该字段。1-15可以用1个byte编码，从16到2047使用两个byte编码,所以常用的字段应该用1-15打上tag。</p>
<h4 id="字段的规则："><a href="#字段的规则：" class="headerlink" title="字段的规则："></a>字段的规则：</h4><ul>
<li>required： 确定有一个</li>
<li>optional： 可以有0个或1个，不能多于1个</li>
<li>repeated： 字段可以重复任意多次（包括0次），定义该字段时在后面加上[packed=true]用以更有效的编码，如</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">repeated int32 samples = 4 [packed=true];</div></pre></td></tr></table></figure>
<ul>
<li>reserved 保留字段，保证这个字段不会再次被使用</li>
</ul>
<h2 id="从Proto-文件生成"><a href="#从Proto-文件生成" class="headerlink" title="从Proto 文件生成"></a>从Proto 文件生成</h2><p>用protocol buffer编译器编译proto文件，生成对应语言的代码。包括getting和setting对应的字段，以及序列化流和消息的反序列化。</p>
<ul>
<li>对于C++: 每个proto文件生成一个.h和.cc文件，每个消息类型生成对应的类</li>
<li>对于Java：编译器为每个消息生成一个.java文件，外加一个特殊的Builder类来生成消息实例</li>
<li>对于Python: 一点点不同 —– Python编译器生成有一个静态的对每个消息的描述器的模块。然后，用一个元类在运行时创建必要的Python数据访问类。</li>
<li>对于Go：编译器对文件中的每个消息生成一个.pb.go文件。</li>
</ul>
<h2 id="使用其它的Message类型"><a href="#使用其它的Message类型" class="headerlink" title="使用其它的Message类型"></a>使用其它的Message类型</h2><h3 id="写在一个proto文件中可以直接用"><a href="#写在一个proto文件中可以直接用" class="headerlink" title="写在一个proto文件中可以直接用"></a>写在一个proto文件中可以直接用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">message SearchResponse &#123;</div><div class="line">  repeated Result result = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">message Result &#123;</div><div class="line">  required string url = 1;</div><div class="line">  optional string title = 2;</div><div class="line">  repeated string snippets = 3;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Import"><a href="#Import" class="headerlink" title="Import"></a>Import</h3><p>如果需要使用的message定义在其它的proto文件中，可以使用import </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import &quot;myproject/other_protos.proto&quot;;</div></pre></td></tr></table></figure>
<p>默认情况下你只能使用直接导入的文件中的定义。然而有的时候你需要将一个文件从一个路径移动到另一个路径的时候，与其将所有的引用这个文件的地方都更新到新的路径，不如在原来的路径上留下一个假的文件，使用import public来指向新的路径。import public语句可以将它导入的文件简介传递给导入本文减的文件。比如 ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// new.proto</div><div class="line">// All definitions are moved here</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// old.proto</div><div class="line">// This is the proto that all clients are importing.</div><div class="line">import public &quot;new.proto&quot;;</div><div class="line">import &quot;other.proto&quot;;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">// client.proto</div><div class="line">import &quot;old.proto&quot;;</div><div class="line">// You use definitions from old.proto and new.proto, but not other.proto</div></pre></td></tr></table></figure>
<p>在命令行中试用-I/–proto_path来指定一系列的编译器搜索路径，如果这个参数没有被设置，那么默认在命令执行的路径查找。通常情况下使用-I/–proto_path来指定到你项目的根目录，然后使用完整的路径来导入所需的文件。</p>
<h2 id="内嵌形式"><a href="#内嵌形式" class="headerlink" title="内嵌形式"></a>内嵌形式</h2><p>你可以在一个消息中定义并使用其他消息类型，比如下面的例子 —— Result消息是在SearchResponse中定义的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">message SearchResponse &#123;</div><div class="line">  message Result &#123;</div><div class="line">    required string url = 1;</div><div class="line">    optional string title = 2;</div><div class="line">    repeated string snippets = 3;</div><div class="line">  &#125;</div><div class="line">  repeated Result result = 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果你打算在这个消息的父消息之外重用这个消息的话，你可以这样引用它 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">message SomeOtherMessage &#123;</div><div class="line">  optional SearchResponse.Result result = 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>你想嵌套多深就嵌套多深，没有限制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">message Outer &#123;                  // Level 0</div><div class="line">  message MiddleAA &#123;  // Level 1</div><div class="line">    message Inner &#123;   // Level 2</div><div class="line">      required int64 ival = 1;</div><div class="line">      optional bool  booly = 2;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  message MiddleBB &#123;  // Level 1</div><div class="line">    message Inner &#123;   // Level 2</div><div class="line">      required int32 ival = 1;</div><div class="line">      optional bool  booly = 2;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="先写到这，有需要继续。先实践"><a href="#先写到这，有需要继续。先实践" class="headerlink" title="先写到这，有需要继续。先实践"></a>先写到这，有需要继续。先实践</h2><h2 id="创建自己的Protocol格式"><a href="#创建自己的Protocol格式" class="headerlink" title="创建自己的Protocol格式"></a>创建自己的Protocol格式</h2><p>定义个proto文件，添加message以及各个字段。如以下addressbook.proto</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">syntax= &quot;proto2&quot;;</div><div class="line">package tutorial;</div><div class="line"></div><div class="line">message Person &#123;</div><div class="line">  required string name = 1;</div><div class="line">  required int32 id = 2;</div><div class="line">  optional string email = 3;</div><div class="line"></div><div class="line">  enum PhoneType &#123;</div><div class="line">    MOBILE = 0;</div><div class="line">    HOME = 1;</div><div class="line">    WORK = 2;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  message PhoneNumber &#123;</div><div class="line">    required string number = 1;</div><div class="line">    optional PhoneType type = 2 [default = HOME];</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  repeated PhoneNumber phone = 4;</div><div class="line">&#125;</div><div class="line"></div><div class="line">message AddressBook &#123;</div><div class="line">  repeated Person person = 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="编译-proto-文件"><a href="#编译-proto-文件" class="headerlink" title="编译 .proto 文件"></a>编译 .proto 文件</h3><p>写好 proto 文件之后就可以用 Protobuf 编译器将该文件编译成目标语言了。本例中我们将使用 C++。</p>
<p>假设您的 proto 文件存放在 $SRC_DIR 下面，您也想把生成的文件放在同一个目录下，则可以使用如下命令就可以生存对应的addressbook.pb.h和addressbook.pb.cc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">protoc -I=$SRC_DIR --cpp_out=$DST_DIR $SRC_DIR/addressbook.proto</div></pre></td></tr></table></figure>
<p>其中package 后面的字段会生成对应的域名空间 上面的例子对应namespace 是tutorial。<br>则对应的h文件中，会有下列的接口，实际的会更多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">// name</div><div class="line"> inline bool has_name() const;</div><div class="line"> inline void clear_name();</div><div class="line"> inline const ::std::string&amp; name() const;</div><div class="line"> inline void set_name(const ::std::string&amp; value);</div><div class="line"> inline void set_name(const char* value);</div><div class="line"> inline ::std::string* mutable_name();</div><div class="line"></div><div class="line"> // id</div><div class="line"> inline bool has_id() const;</div><div class="line"> inline void clear_id();</div><div class="line"> inline int32_t id() const;</div><div class="line"> inline void set_id(int32_t value);</div><div class="line"></div><div class="line"> // email</div><div class="line"> inline bool has_email() const;</div><div class="line"> inline void clear_email();</div><div class="line"> inline const ::std::string&amp; email() const;</div><div class="line"> inline void set_email(const ::std::string&amp; value);</div><div class="line"> inline void set_email(const char* value);</div><div class="line"> inline ::std::string* mutable_email();</div><div class="line"></div><div class="line"> // phone</div><div class="line"> inline int phone_size() const;</div><div class="line"> inline void clear_phone();</div><div class="line"> inline const ::google::protobuf::RepeatedPtrField&lt; ::tutorial::Person_PhoneNumber &gt;&amp; phone() const;</div><div class="line"> inline ::google::protobuf::RepeatedPtrField&lt; ::tutorial::Person_PhoneNumber &gt;* mutable_phone();</div><div class="line"> inline const ::tutorial::Person_PhoneNumber&amp; phone(int index) const;</div><div class="line"> inline ::tutorial::Person_PhoneNumber* mutable_phone(int index);</div><div class="line"> inline ::tutorial::Person_PhoneNumber* add_phone();</div></pre></td></tr></table></figure>
<h3 id="写以下-Application-writeMain-cpp"><a href="#写以下-Application-writeMain-cpp" class="headerlink" title="写以下 Application  (writeMain.cpp)"></a>写以下 Application  (writeMain.cpp)</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"addressbook.pb.h"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// This function fills in a Person message based on user input.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">PromptForAddress</span><span class="params">(tutorial::Person* person)</span> </span>&#123;</div><div class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">"Enter person ID number: "</span>;</div><div class="line">  <span class="keyword">int</span> id;</div><div class="line">  <span class="built_in">cin</span> &gt;&gt; id;</div><div class="line">  person-&gt;set_id(id);</div><div class="line">  <span class="built_in">cin</span>.ignore(<span class="number">256</span>, <span class="string">'\n'</span>);</div><div class="line"></div><div class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">"Enter name: "</span>;</div><div class="line">  getline(<span class="built_in">cin</span>, *person-&gt;mutable_name());</div><div class="line"></div><div class="line">  <span class="built_in">cout</span> &lt;&lt; <span class="string">"Enter email address (blank for none): "</span>;</div><div class="line">  <span class="built_in">string</span> email;</div><div class="line">  getline(<span class="built_in">cin</span>, email);</div><div class="line">  <span class="keyword">if</span> (!email.empty()) &#123;</div><div class="line">    person-&gt;set_email(email);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Enter a phone number (or leave blank to finish): "</span>;</div><div class="line">    <span class="built_in">string</span> number;</div><div class="line">    getline(<span class="built_in">cin</span>, number);</div><div class="line">    <span class="keyword">if</span> (number.empty()) &#123;</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    tutorial::Person::PhoneNumber* phone_number = person-&gt;add_phone();</div><div class="line">    phone_number-&gt;set_number(number);</div><div class="line"></div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Is this a mobile, home, or work phone? "</span>;</div><div class="line">    <span class="built_in">string</span> type;</div><div class="line">    getline(<span class="built_in">cin</span>, type);</div><div class="line">    <span class="keyword">if</span> (type == <span class="string">"mobile"</span>) &#123;</div><div class="line">      phone_number-&gt;set_type(tutorial::Person::MOBILE);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"home"</span>) &#123;</div><div class="line">      phone_number-&gt;set_type(tutorial::Person::HOME);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"work"</span>) &#123;</div><div class="line">      phone_number-&gt;set_type(tutorial::Person::WORK);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">"Unknown phone type.  Using default."</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Main function:  Reads the entire address book from a file,</span></div><div class="line"><span class="comment">//   adds one person based on user input, then writes it back out to the same</span></div><div class="line"><span class="comment">//   file.</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</div><div class="line">  <span class="comment">// Verify that the version of the library that we linked against is</span></div><div class="line">  <span class="comment">// compatible with the version of the headers we compiled against.</span></div><div class="line">  GOOGLE_PROTOBUF_VERIFY_VERSION;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</div><div class="line">    <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Usage:  "</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">" ADDRESS_BOOK_FILE"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  tutorial::AddressBook address_book;</div><div class="line"></div><div class="line">  &#123;</div><div class="line">    <span class="comment">// Read the existing address book.</span></div><div class="line">    <span class="function">fstream <span class="title">input</span><span class="params">(argv[<span class="number">1</span>], ios::in | ios::binary)</span></span>;</div><div class="line">    <span class="keyword">if</span> (!input) &#123;</div><div class="line">      <span class="built_in">cout</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">": File not found.  Creating a new file."</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!address_book.ParseFromIstream(&amp;input)) &#123;</div><div class="line">      <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Failed to parse address book."</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">      <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Add an address.</span></div><div class="line">  PromptForAddress(address_book.add_person());</div><div class="line"></div><div class="line">  &#123;</div><div class="line">    <span class="comment">// Write the new address book back to disk.</span></div><div class="line">    <span class="function">fstream <span class="title">output</span><span class="params">(argv[<span class="number">1</span>], ios::out | ios::trunc | ios::binary)</span></span>;</div><div class="line">    <span class="keyword">if</span> (!address_book.SerializeToOstream(&amp;output)) &#123;</div><div class="line">      <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Failed to write address book."</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">      <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Optional:  Delete all global objects allocated by libprotobuf.</span></div><div class="line">  google::protobuf::ShutdownProtobufLibrary();</div><div class="line"></div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>g++ -o write addressbook.pb.cc writeMain.cpp -lprotobuf 进行编译</p>
<p>编译时需要-lprotobuf</p>
<h2 id="写Read-Message-的Application（readmain-cpp）"><a href="#写Read-Message-的Application（readmain-cpp）" class="headerlink" title="写Read Message 的Application（readmain.cpp）"></a>写Read Message 的Application（readmain.cpp）</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"addressbook.pb.h"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// Iterates though all people in the AddressBook and prints info about them.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ListPeople</span><span class="params">(<span class="keyword">const</span> tutorial::AddressBook&amp; address_book)</span> </span>&#123;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; address_book.person_size(); i++) &#123;</div><div class="line">    <span class="keyword">const</span> tutorial::Person&amp; person = address_book.person(i);</div><div class="line"></div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Person ID: "</span> &lt;&lt; person.id() &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"  Name: "</span> &lt;&lt; person.name() &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">if</span> (person.has_email()) &#123;</div><div class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">"  E-mail address: "</span> &lt;&lt; person.email() &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; person.phone_size(); j++) &#123;</div><div class="line">      <span class="keyword">const</span> tutorial::Person::PhoneNumber&amp; phone_number = person.phone(j);</div><div class="line"></div><div class="line">      <span class="keyword">switch</span> (phone_number.type()) &#123;</div><div class="line">        <span class="keyword">case</span> tutorial::Person::MOBILE:</div><div class="line">          <span class="built_in">cout</span> &lt;&lt; <span class="string">"  Mobile phone #: "</span>;</div><div class="line">          <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> tutorial::Person::HOME:</div><div class="line">          <span class="built_in">cout</span> &lt;&lt; <span class="string">"  Home phone #: "</span>;</div><div class="line">          <span class="keyword">break</span>;</div><div class="line">        <span class="keyword">case</span> tutorial::Person::WORK:</div><div class="line">          <span class="built_in">cout</span> &lt;&lt; <span class="string">"  Work phone #: "</span>;</div><div class="line">          <span class="keyword">break</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="built_in">cout</span> &lt;&lt; phone_number.number() &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Main function:  Reads the entire address book from a file and prints all</span></div><div class="line"><span class="comment">//   the information inside.</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</div><div class="line">  <span class="comment">// Verify that the version of the library that we linked against is</span></div><div class="line">  <span class="comment">// compatible with the version of the headers we compiled against.</span></div><div class="line">  GOOGLE_PROTOBUF_VERIFY_VERSION;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</div><div class="line">    <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Usage:  "</span> &lt;&lt; argv[<span class="number">0</span>] &lt;&lt; <span class="string">" ADDRESS_BOOK_FILE"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  tutorial::AddressBook address_book;</div><div class="line"></div><div class="line">  &#123;</div><div class="line">    <span class="comment">// Read the existing address book.</span></div><div class="line">    <span class="function">fstream <span class="title">input</span><span class="params">(argv[<span class="number">1</span>], ios::in | ios::binary)</span></span>;</div><div class="line">    <span class="keyword">if</span> (!address_book.ParseFromIstream(&amp;input)) &#123;</div><div class="line">      <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Failed to parse address book."</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">      <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  ListPeople(address_book);</div><div class="line"></div><div class="line">  <span class="comment">// Optional:  Delete all global objects allocated by libprotobuf.</span></div><div class="line">  google::protobuf::ShutdownProtobufLibrary();</div><div class="line"></div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译g++ -o read  addressbook.pb.cc readMain.cpp -lprotobuf</p>
<p>运行 ./write Address写入信息</p>
<p>运行./read Address读出信息</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/15/Pinned-Memory-Vs-Non-Pinned-Memory/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/Pinned-Memory-Vs-Non-Pinned-Memory/" itemprop="url">
                  Pinned Memory Vs. Non-Pinned Memory
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-09-15T21:37:56+08:00">
              2016-09-15
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-15T21:42:31+08:00">
              2016-12-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/09/15/Pinned-Memory-Vs-Non-Pinned-Memory/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/09/15/Pinned-Memory-Vs-Non-Pinned-Memory/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/09/15/Pinned-Memory-Vs-Non-Pinned-Memory/" class="leancloud_visitors" data-flag-title="Pinned Memory Vs. Non-Pinned Memory">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>首先需要介绍一下Pinned Memory 和 Non-Pinned Memory,关于CPU和GPU内存的分配和传输可以具体可以Google这里：</p>
<p><a href="https://www.cs.virginia.edu/~mwb7w/cuda_support/pinned_tradeoff.html" target="_blank" rel="external">Choosing Between Pinned and Non-Pinned Memory</a></p>
<p><a href="https://www.cs.virginia.edu/~mwb7w/cuda_support/memory_management_overhead.html" target="_blank" rel="external">Memory Allocation Overhead</a></p>
<p><a href="https://www.cs.virginia.edu/~mwb7w/cuda_support/memory_transfer_overhead.html" target="_blank" rel="external">Memory Transfer Overhead</a></p>
<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><p>为了协调CPU和GPU之间的内存创建和分配，</p>
<h2 id="CPU分配"><a href="#CPU分配" class="headerlink" title="CPU分配"></a>CPU分配</h2><p>CPU分配内存主要有两种方式：</p>
<ul>
<li>通过C标准库中的<code>malloc</code>函数完成</li>
<li>调用CUDA中的<code>cudaMallocHost</code>函数</li>
</ul>
<p><code>cudaMallocHost</code>函数通过页面锁定，可以提供更高的CPU和GPU传输速率，吞吐量增加是Barracuda10的2.4倍，Barracuda04的2.0倍，Barracuda01的1.5倍。但是缺点是使用<code>cudaMallocHost</code>分配内存比<code>malloc</code>更加慢，<br>每次调用<code>cudaMallocHost</code>分配1M的内存需要2300微妙左右，当需要分配512M，时间上升到61毫秒，比<code>malloc</code>在分配时慢了3-5个量级</p>
<h2 id="GPU分配"><a href="#GPU分配" class="headerlink" title="GPU分配"></a>GPU分配</h2><p>GPU的分配方式有：</p>
<ul>
<li>通过CUDA的<code>cudaMalloc</code>函数</li>
<li><code>cudaMallocPitch</code>函数</li>
<li><code>cudaMallocArray</code>函数</li>
</ul>
<p><code>cudaMallocPitch</code> 和 <code>cudaMallocArray</code>都没有<code>cudaMalloc</code>来的快。<code>cudaMalloc</code>的分配时间如图1所示，256bytes的调用分配耗时1微妙，2KB到4KB分配耗时有显著提升，接下来在512KB分配大约需要50微妙，大于512K时，耗时显著提升，当分配512M时，需要12.5毫秒左右。总而言之，在小于4M时，<code>cudaMalloc</code>分配速度比<code>malloc</code>慢1.5个量级，大于4MB时，<code>cudaMalloc</code>比<code>malloc</code>慢2-4个量级。</p>
<p>图1为不同函数分配不同内存的调用时间<br> <img src="http://oi8824myj.bkt.clouddn.com/malloc_time_per_call.png" alt="不同函数分配内存的调用时间"></p>
<p>图2为不同函数每个Byte分配的平均时间<br><img src="http://oi8824myj.bkt.clouddn.com/malloc_time_per_byte.png" alt="不同函数每个Byte分配的平均时间"></p>
<h2 id="内存释放"><a href="#内存释放" class="headerlink" title="内存释放"></a>内存释放</h2><p>每个内存分配函数都有对应的内存释放函数，如<code>free</code>对应于<code>malloc</code>,<code>cudaFree</code>对应于<code>cudaMalloc</code>,<code>cudaFreeHost</code>对应于<code>cudaMallocHost</code></p>
<p>图3显示了不同的释放函数随着释放内存大小的调用耗时的比较,与分配时相似，<code>cudaFreeHost</code>比<code>free</code>慢3-5个量级，<code>cudaFree</code>比<code>free</code>慢2-3个量级。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/free_time_per_call.png" alt="不同函数释放内存的调用时间"></p>
<p>图4显示了不同函数每个Byte数据的释放平均时间<br><img src="http://oi8824myj.bkt.clouddn.com/free_time_per_byte.png" alt="不同函数每个Byte数据的释放平均时间"></p>
<h1 id="CPU和GPU之间的内存传输"><a href="#CPU和GPU之间的内存传输" class="headerlink" title="CPU和GPU之间的内存传输"></a>CPU和GPU之间的内存传输</h1><p>在GPU核执行任务前，需要把数据都从CPU内存传输到GPU内存，任务完成后，再将运算结果或处理后的数据传回CPU，常用的函数是<code>cudaMemcpy</code></p>
<p>图5显示了使用<code>cudaMemcpy</code>传输不同大小的内存的平均传输时间，硬件设置如下:  3.20 GHz Core 2 Extreme processor, GTX 280 GPU, and PCIe version 2.0。测试了4种不同的配置: Non-Pinned to Device, Non-Pinned from Device, Pinned to Device 和 Pinned from Device。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/transfer_time.png" alt="four measures"><br>可以看到，对于传输小规模的内存，基本都是10微妙级别的常量时间。从8K大小开始，传输速度呈现线性增长。</p>
<p>图6显示了传输相同大小的字节的平均吞吐率，对于小块数据，差异不大，对于大块数据，Pinned memory提供了2.4倍于Non-Pinned memory的吞吐率。<br><img src="http://oi8824myj.bkt.clouddn.com/transfer_throughput.png" alt="吞吐率"></p>
<h1 id="选择-Pinned-或者-Non-Pinned-Memory"><a href="#选择-Pinned-或者-Non-Pinned-Memory" class="headerlink" title="选择 Pinned 或者 Non-Pinned Memory"></a>选择 Pinned 或者 Non-Pinned Memory</h1><p>当需要分配CPU内存且会传输到GPU时，有两种方式可以选择：<code>pinned</code> 和 <code>non-pinned</code>。</p>
<p>Pinned memory使用<code>cudaMallocHost</code>来分配CPU内存,可以防止内存页被交换出去，因此可以提供更高的传输速度。Non-pinned memory使用<code>malloc</code>函数分配CPU内存。但是Pinned memory 比Non-pinned memory有更昂贵的内存分配和释放，因为<code>cudaMallocHost</code>分配和释放CPU内存相比<code>malloc</code>更加耗时。</p>
<p>因此，一个很明显的问题是：<code>当分配多少内存时，选择pinned-memory方式的性能更加优越？</code></p>
<p>主要有以下两类情况，在这两类情况下，我们都假设内存的分配和释放都执行了1次。<br>（1）假设分配的内存被用来从CPU传到GPU，然后从GPU传回CPU，两次传输的size相同<br>（2）假设分配的内存全部从CPU传到GPU，然后从GPU只是传递了运算结果回CPU，这种情况下GPU核通常执行的是reduction操作，比如常见的计算平均数。</p>
<p>在第一种情况下，我们可以看到图7的对比结果，结果显示，只有当内存的大小超过16MB时，使用Pinned memory才有更优越的性能。<br><img src="http://oi8824myj.bkt.clouddn.com/pinned_two-way.png" alt="two way"></p>
<p>在第二中情况下，我们可以看到图8的对比结果，结果显示，Pinned 方式没有优越性，应该选择 Non-Pinned方式。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/pinned_one-way.png" alt="one way"></p>
<p>我们接着比较实用Pinned Memory来分配内存的情况下，（1）和（2）两种情况的性能提升对比，如图9所示：<br><img src="http://oi8824myj.bkt.clouddn.com/pinned_speedup.png" alt="pinned memmory two way"><br>我们可以看到，对于第1种情况，当传输的内存大于10M时才有意义，对于第2种情况，传输的数据大于128M时才有意义。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><ul>
<li>使用<code>Pinned Memory</code>方式的CPU和GPU之间传输速度更大，但是分配和释放的耗时更大</li>
<li>使用<code>Non-Pinned Memory</code>的方式CPU和GPU之间传输耗时更大，但是分配和释放更快</li>
<li>具体使用哪种方式，要结合数据的大小以及CPU和GPU之间传输的类型，当传输的数据比较大，且均需从CPU传至GPU和从GPU传至CPU时，使用<code>Pinned Memory</code>有比较大的性能，具体可以看图9的对比。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/15/ResNet解读/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/ResNet解读/" itemprop="url">
                  ResNet解读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-09-15T18:10:50+08:00">
              2016-09-15
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-02-15T18:11:39+08:00">
              2017-02-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/09/15/ResNet解读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/09/15/ResNet解读/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/09/15/ResNet解读/" class="leancloud_visitors" data-flag-title="ResNet解读">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ResNet-论文解读笔记"><a href="#ResNet-论文解读笔记" class="headerlink" title="ResNet 论文解读笔记"></a>ResNet 论文解读笔记</h1><p><code>Reference</code></p>
<p><a href="https://zhuanlan.zhihu.com/p/22447440" target="_blank" rel="external">机器之眼-Deep Residual Network 深度残差网络</a></p>
<p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">[1]Deep Residual Learning for Image Recognition</a></p>
<p><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="external">[2]Identity Mappings in Deep Residual Networks</a></p>
<h1 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h1><p>深度卷积神经网络的表达能力通常随着深度的增加而增强，然而网络并非越深越好，一来是因为深度越深，对计算资源的要求越高，二是当网络的深度较深时，继续增加层数并不能提高性能，如下图所示，随着网络层数的增加，性能不仅没有提升，反而出现了退化。这不是因为过拟合而造成的，因为在训练数据上也出现了相同的结果。此外，在网络训练时也使用了ReLU激活，BN等手段，一定程度上也缓解了梯度消失。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet1.png" alt="ResNet1"></p>
<h1 id="深度残差学习"><a href="#深度残差学习" class="headerlink" title="深度残差学习"></a>深度残差学习</h1><h2 id="Deep-Residual-Learning"><a href="#Deep-Residual-Learning" class="headerlink" title="Deep Residual Learning"></a>Deep Residual Learning</h2><p>我们假设深度网络中某隐藏层为$H(X)-x \rightarrow{F(x)}$,如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也可以同样假设隐藏层的残差近似于某个复杂函数。即我们可以将隐藏层表示为$H(x)=F(x)+x$。</p>
<p>这样一来，我们可以得到一种全新的残差结构单元，如下图所示：<br><img src="http://oi8824myj.bkt.clouddn.com/ResNet2.png" alt="ResNet2"></p>
<p>可以看到，残差单元的输出由多个卷积层级联的输出和输入元素相加(保证卷积层的输出和输入的元素维度相同)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。论文中给出的18，34，50，101，152层网络如下表所示:</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet3.png" alt="ResNet3"></p>
<p>可以注意到残差网络有这样几个特点：</p>
<p>1.网络较瘦，控制了参数数量；</p>
<p>2.存在明显层级，特征图数逐层递进，保证输出特征表达能力；</p>
<p>3.使用了较少的池化层，大量使用下采样，提高传播效率；</p>
<p>4.没有使用Dropout,利用BN和全局平均池化进行正则化，加快了训练速度；</p>
<p>5.层数较高时减少了$3\times3$卷积个数，并用$1\times1$卷积控制了$3\times3$卷积的输入输出特征图数量，称这种结构为“瓶颈”(bottleneck)。</p>
<h2 id="收敛性能"><a href="#收敛性能" class="headerlink" title="收敛性能"></a>收敛性能</h2><p>在ImageNet上的性能，左边为“朴素”网络结构，右边为残差结构。</p>
<p>从图中可以看到，残差网络增加了一定层数后，并未出现性能退化，反而性能有了一定程度的提升：残差网络有着更低的收敛损失，同时也没有产生过高的过拟合。同时注意到，残差网络在浅层时并未表现出更多的优势，说明残差网络必须要配合较深的深度才能发挥其结构优势，与“平整”网络拉开性能差距。<br><img src="http://oi8824myj.bkt.clouddn.com/ResNet4.png" alt="ResNet4"></p>
<h2 id="分类性能"><a href="#分类性能" class="headerlink" title="分类性能"></a>分类性能</h2><p>下图是几种网络结构在ImageNet数据集上单模型和集成模型分类错误率的对比。可以发现，残差网络系由于层数普遍高于以上模型，且又有残差结构作为极深度的支持前提，使得其性能普遍高于此前的各类优秀模型。此外，残差网络的性能也确实如期望随着网络层数的提升而提升。在100层以上时已经远远甩开了IRSVRC 2014的冠亚军网络模型。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet5.png" alt="ResNet5"></p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet6.png" alt="ResNet6"></p>
<h2 id="网络响应"><a href="#网络响应" class="headerlink" title="网络响应"></a>网络响应</h2><p><img src="http://oi8824myj.bkt.clouddn.com/ResNet7.png" alt="ResNet7"><br>从图中可以看出，残差网络中大部分层的响应方差都处在较低水平，响应都是在BN层后面，ReLU函数前。这一定程度上印证了我们的假设：这些响应方差较低的层响应较为固定，很有可能权重近似于零，这也就是说其所对应的残差结构可能近似于单位映射,网络的实际有效层数是要比全部层数要少一些的，产生了跳过连接(Skip-connection)的作用。这样也就是网络为什么在较深的深度下仍可以保持并提升性能，且没有过多增加训练难度的原因。</p>
<h2 id="更深的网络结构"><a href="#更深的网络结构" class="headerlink" title="更深的网络结构"></a>更深的网络结构</h2><p>在CIFAR-10数据集上尝试更深的网络结构，层数为6n+2层，第一层为$3 \times 3$的卷积层，最后一层为全连接层和SoftMax.中间卷积层特征图的尺寸分别为{32,16,8},特征图的个数分别为{16,32,64},每种特征图的都有2n层的卷积。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet9.png" alt="ResNet9"></p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet8.png" alt="ResNet8"><br>何恺明等人经过一段时间的研究，认为极其深的深度网络可能深受梯度消失问题的困扰，BN、ReLU等手段对于这种程度的梯度消失缓解能力有限，并提出了单位映射的残差结构。这种结构从本源上杜绝了梯度消失的问题。</p>
<h1 id="Identity-Mappings-in-Deep-Residual-Networks"><a href="#Identity-Mappings-in-Deep-Residual-Networks" class="headerlink" title="Identity Mappings in Deep Residual Networks"></a>Identity Mappings in Deep Residual Networks</h1><p>上面主要介绍了残差单元，它的数学形式为：<br>$$<br>y_l=h(x_l)+F(x_l,W_l)<br>$$</p>
<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>其中，$x_l$和$x_{l+1}$是第$l$个残差单元的输入和输出，$F$是一个残差函数。在论文[1]中，$h(x_l)=x_l$是一个单位映射, $f$是ReLU函数。</p>
<p>我们令$h(x_l)=x_l$, $x_{l+1}=y_l$，从而可以递推得到</p>
<p>$$<br>x_L=x_l+\sum_{i=l}^{L-1}F(x_i,w_i)<br>$$<br>也就是，第L个残差单元的输入可以表示为某一浅层残差单元的输入和其中间所有复杂映射之和。记损失函数为 $\epsilon $，从而我们计算反向传播得到:</p>
<p>$$<br>\frac{\partial{\epsilon}}{\partial{x_l}}=\frac{\partial{\epsilon}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}=\frac{\partial{\epsilon}}{\partial{x_L}}(1+\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}F(x_i,w_i))<br>$$<br>显然这里不存在网络结构级联所产生的连乘了(朴素的网络中$x_L$是由各级的矩阵和向量连乘得到的，忽略BN和ReLU函数，$\prod_{i=0}^{L-1}W_ix_0$)。$\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}F(x_i,w_i)$不可能每个batch的采样值都是-1，这也就是说梯度消失的本源已不存在。但需要注意的是，我们这里的前提条件$h(x_l)=x_l, x_{l+1}=y_l$是非常强的，一旦打破，如上的关系即不成立。其中$h(x_l)=x_l$是我们之前一直在遵守的条件，而且实验也证明单位映射的shortcut是最优的；$x_{l+1}=y_l$则要求我们去除隐藏层之间的激活，但必要的激活不可少，这需要对残差结构重新设计。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet10.png" alt="ResNet10"><br>实验证明，将激活层融合到残差支路中，并使用ReLU预激活的残差单元，不仅可以满足之前的假设，并且实验证明在各种已知的结构里也是最优的，预激活单元和原始残差单元的示意图如上图所示，可以看到，预激活的残差单元在残差支路中进行每次卷积之前即完成激活，然后再进行矩阵元素间加法进行合并，既满足了激活要求，也使得之路不再需要额外激活。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet11.png" alt="ResNet11"><br>可以看到，使用预激活残差单元，相较于使用原始单元更易收敛，且有一定正则化的效果，测试集上性能也普遍好于原始残差单元。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div><div class="line">1501</div><div class="line">1502</div><div class="line">1503</div><div class="line">1504</div><div class="line">1505</div><div class="line">1506</div><div class="line">1507</div><div class="line">1508</div><div class="line">1509</div><div class="line">1510</div><div class="line">1511</div><div class="line">1512</div><div class="line">1513</div><div class="line">1514</div><div class="line">1515</div><div class="line">1516</div><div class="line">1517</div><div class="line">1518</div><div class="line">1519</div><div class="line">1520</div><div class="line">1521</div><div class="line">1522</div><div class="line">1523</div><div class="line">1524</div><div class="line">1525</div><div class="line">1526</div><div class="line">1527</div><div class="line">1528</div><div class="line">1529</div><div class="line">1530</div><div class="line">1531</div><div class="line">1532</div><div class="line">1533</div><div class="line">1534</div><div class="line">1535</div><div class="line">1536</div><div class="line">1537</div><div class="line">1538</div><div class="line">1539</div><div class="line">1540</div><div class="line">1541</div><div class="line">1542</div><div class="line">1543</div><div class="line">1544</div><div class="line">1545</div><div class="line">1546</div><div class="line">1547</div><div class="line">1548</div><div class="line">1549</div><div class="line">1550</div><div class="line">1551</div><div class="line">1552</div><div class="line">1553</div><div class="line">1554</div><div class="line">1555</div><div class="line">1556</div><div class="line">1557</div><div class="line">1558</div><div class="line">1559</div><div class="line">1560</div><div class="line">1561</div><div class="line">1562</div><div class="line">1563</div><div class="line">1564</div><div class="line">1565</div><div class="line">1566</div><div class="line">1567</div><div class="line">1568</div><div class="line">1569</div><div class="line">1570</div><div class="line">1571</div><div class="line">1572</div><div class="line">1573</div><div class="line">1574</div><div class="line">1575</div><div class="line">1576</div><div class="line">1577</div><div class="line">1578</div><div class="line">1579</div><div class="line">1580</div><div class="line">1581</div><div class="line">1582</div><div class="line">1583</div><div class="line">1584</div><div class="line">1585</div><div class="line">1586</div><div class="line">1587</div><div class="line">1588</div><div class="line">1589</div><div class="line">1590</div><div class="line">1591</div><div class="line">1592</div><div class="line">1593</div><div class="line">1594</div><div class="line">1595</div><div class="line">1596</div><div class="line">1597</div><div class="line">1598</div><div class="line">1599</div><div class="line">1600</div><div class="line">1601</div><div class="line">1602</div><div class="line">1603</div><div class="line">1604</div><div class="line">1605</div><div class="line">1606</div><div class="line">1607</div><div class="line">1608</div><div class="line">1609</div><div class="line">1610</div><div class="line">1611</div><div class="line">1612</div><div class="line">1613</div><div class="line">1614</div><div class="line">1615</div><div class="line">1616</div><div class="line">1617</div><div class="line">1618</div><div class="line">1619</div><div class="line">1620</div><div class="line">1621</div><div class="line">1622</div><div class="line">1623</div><div class="line">1624</div><div class="line">1625</div><div class="line">1626</div><div class="line">1627</div><div class="line">1628</div><div class="line">1629</div><div class="line">1630</div><div class="line">1631</div><div class="line">1632</div><div class="line">1633</div><div class="line">1634</div><div class="line">1635</div><div class="line">1636</div><div class="line">1637</div><div class="line">1638</div><div class="line">1639</div><div class="line">1640</div><div class="line">1641</div><div class="line">1642</div><div class="line">1643</div><div class="line">1644</div><div class="line">1645</div><div class="line">1646</div><div class="line">1647</div><div class="line">1648</div><div class="line">1649</div><div class="line">1650</div><div class="line">1651</div><div class="line">1652</div><div class="line">1653</div><div class="line">1654</div><div class="line">1655</div><div class="line">1656</div><div class="line">1657</div><div class="line">1658</div><div class="line">1659</div><div class="line">1660</div><div class="line">1661</div><div class="line">1662</div><div class="line">1663</div><div class="line">1664</div><div class="line">1665</div><div class="line">1666</div><div class="line">1667</div><div class="line">1668</div><div class="line">1669</div><div class="line">1670</div><div class="line">1671</div><div class="line">1672</div><div class="line">1673</div><div class="line">1674</div><div class="line">1675</div><div class="line">1676</div><div class="line">1677</div><div class="line">1678</div><div class="line">1679</div><div class="line">1680</div><div class="line">1681</div><div class="line">1682</div><div class="line">1683</div><div class="line">1684</div><div class="line">1685</div><div class="line">1686</div><div class="line">1687</div><div class="line">1688</div><div class="line">1689</div><div class="line">1690</div><div class="line">1691</div><div class="line">1692</div><div class="line">1693</div><div class="line">1694</div><div class="line">1695</div><div class="line">1696</div><div class="line">1697</div><div class="line">1698</div><div class="line">1699</div><div class="line">1700</div><div class="line">1701</div><div class="line">1702</div><div class="line">1703</div><div class="line">1704</div><div class="line">1705</div><div class="line">1706</div><div class="line">1707</div><div class="line">1708</div><div class="line">1709</div><div class="line">1710</div><div class="line">1711</div><div class="line">1712</div><div class="line">1713</div><div class="line">1714</div><div class="line">1715</div><div class="line">1716</div><div class="line">1717</div><div class="line">1718</div><div class="line">1719</div><div class="line">1720</div><div class="line">1721</div><div class="line">1722</div><div class="line">1723</div><div class="line">1724</div><div class="line">1725</div><div class="line">1726</div><div class="line">1727</div><div class="line">1728</div><div class="line">1729</div><div class="line">1730</div><div class="line">1731</div><div class="line">1732</div><div class="line">1733</div><div class="line">1734</div><div class="line">1735</div><div class="line">1736</div><div class="line">1737</div><div class="line">1738</div><div class="line">1739</div><div class="line">1740</div><div class="line">1741</div><div class="line">1742</div><div class="line">1743</div><div class="line">1744</div><div class="line">1745</div><div class="line">1746</div><div class="line">1747</div><div class="line">1748</div><div class="line">1749</div><div class="line">1750</div><div class="line">1751</div><div class="line">1752</div><div class="line">1753</div><div class="line">1754</div><div class="line">1755</div><div class="line">1756</div><div class="line">1757</div><div class="line">1758</div><div class="line">1759</div><div class="line">1760</div><div class="line">1761</div><div class="line">1762</div><div class="line">1763</div><div class="line">1764</div><div class="line">1765</div><div class="line">1766</div><div class="line">1767</div><div class="line">1768</div><div class="line">1769</div><div class="line">1770</div><div class="line">1771</div><div class="line">1772</div><div class="line">1773</div><div class="line">1774</div><div class="line">1775</div><div class="line">1776</div><div class="line">1777</div><div class="line">1778</div><div class="line">1779</div><div class="line">1780</div><div class="line">1781</div><div class="line">1782</div><div class="line">1783</div><div class="line">1784</div><div class="line">1785</div><div class="line">1786</div><div class="line">1787</div><div class="line">1788</div><div class="line">1789</div><div class="line">1790</div><div class="line">1791</div><div class="line">1792</div><div class="line">1793</div><div class="line">1794</div><div class="line">1795</div><div class="line">1796</div><div class="line">1797</div><div class="line">1798</div><div class="line">1799</div><div class="line">1800</div><div class="line">1801</div><div class="line">1802</div><div class="line">1803</div><div class="line">1804</div><div class="line">1805</div><div class="line">1806</div><div class="line">1807</div><div class="line">1808</div><div class="line">1809</div><div class="line">1810</div><div class="line">1811</div><div class="line">1812</div><div class="line">1813</div><div class="line">1814</div><div class="line">1815</div><div class="line">1816</div><div class="line">1817</div><div class="line">1818</div><div class="line">1819</div><div class="line">1820</div><div class="line">1821</div><div class="line">1822</div><div class="line">1823</div><div class="line">1824</div><div class="line">1825</div><div class="line">1826</div><div class="line">1827</div><div class="line">1828</div><div class="line">1829</div><div class="line">1830</div><div class="line">1831</div><div class="line">1832</div><div class="line">1833</div><div class="line">1834</div><div class="line">1835</div><div class="line">1836</div><div class="line">1837</div><div class="line">1838</div><div class="line">1839</div><div class="line">1840</div><div class="line">1841</div><div class="line">1842</div><div class="line">1843</div><div class="line">1844</div><div class="line">1845</div><div class="line">1846</div><div class="line">1847</div><div class="line">1848</div><div class="line">1849</div><div class="line">1850</div><div class="line">1851</div><div class="line">1852</div><div class="line">1853</div><div class="line">1854</div><div class="line">1855</div><div class="line">1856</div><div class="line">1857</div><div class="line">1858</div><div class="line">1859</div><div class="line">1860</div><div class="line">1861</div><div class="line">1862</div><div class="line">1863</div><div class="line">1864</div><div class="line">1865</div><div class="line">1866</div><div class="line">1867</div><div class="line">1868</div><div class="line">1869</div><div class="line">1870</div><div class="line">1871</div><div class="line">1872</div><div class="line">1873</div><div class="line">1874</div><div class="line">1875</div><div class="line">1876</div><div class="line">1877</div><div class="line">1878</div><div class="line">1879</div><div class="line">1880</div><div class="line">1881</div><div class="line">1882</div><div class="line">1883</div><div class="line">1884</div><div class="line">1885</div><div class="line">1886</div><div class="line">1887</div><div class="line">1888</div><div class="line">1889</div><div class="line">1890</div><div class="line">1891</div><div class="line">1892</div><div class="line">1893</div><div class="line">1894</div><div class="line">1895</div><div class="line">1896</div><div class="line">1897</div><div class="line">1898</div><div class="line">1899</div><div class="line">1900</div><div class="line">1901</div><div class="line">1902</div><div class="line">1903</div><div class="line">1904</div><div class="line">1905</div><div class="line">1906</div><div class="line">1907</div><div class="line">1908</div><div class="line">1909</div><div class="line">1910</div><div class="line">1911</div><div class="line">1912</div><div class="line">1913</div><div class="line">1914</div><div class="line">1915</div><div class="line">1916</div><div class="line">1917</div><div class="line">1918</div><div class="line">1919</div><div class="line">1920</div><div class="line">1921</div><div class="line">1922</div><div class="line">1923</div><div class="line">1924</div><div class="line">1925</div><div class="line">1926</div><div class="line">1927</div><div class="line">1928</div><div class="line">1929</div><div class="line">1930</div><div class="line">1931</div><div class="line">1932</div><div class="line">1933</div><div class="line">1934</div><div class="line">1935</div><div class="line">1936</div><div class="line">1937</div><div class="line">1938</div><div class="line">1939</div><div class="line">1940</div><div class="line">1941</div><div class="line">1942</div><div class="line">1943</div><div class="line">1944</div><div class="line">1945</div><div class="line">1946</div><div class="line">1947</div><div class="line">1948</div><div class="line">1949</div><div class="line">1950</div><div class="line">1951</div><div class="line">1952</div><div class="line">1953</div><div class="line">1954</div><div class="line">1955</div><div class="line">1956</div><div class="line">1957</div><div class="line">1958</div><div class="line">1959</div><div class="line">1960</div><div class="line">1961</div><div class="line">1962</div><div class="line">1963</div><div class="line">1964</div><div class="line">1965</div><div class="line">1966</div><div class="line">1967</div><div class="line">1968</div><div class="line">1969</div><div class="line">1970</div><div class="line">1971</div><div class="line">1972</div><div class="line">1973</div><div class="line">1974</div><div class="line">1975</div><div class="line">1976</div><div class="line">1977</div><div class="line">1978</div><div class="line">1979</div><div class="line">1980</div><div class="line">1981</div><div class="line">1982</div><div class="line">1983</div><div class="line">1984</div><div class="line">1985</div><div class="line">1986</div><div class="line">1987</div><div class="line">1988</div><div class="line">1989</div><div class="line">1990</div><div class="line">1991</div><div class="line">1992</div><div class="line">1993</div><div class="line">1994</div><div class="line">1995</div><div class="line">1996</div><div class="line">1997</div><div class="line">1998</div><div class="line">1999</div><div class="line">2000</div><div class="line">2001</div><div class="line">2002</div><div class="line">2003</div><div class="line">2004</div><div class="line">2005</div><div class="line">2006</div><div class="line">2007</div><div class="line">2008</div><div class="line">2009</div><div class="line">2010</div><div class="line">2011</div><div class="line">2012</div><div class="line">2013</div><div class="line">2014</div><div class="line">2015</div><div class="line">2016</div><div class="line">2017</div><div class="line">2018</div><div class="line">2019</div><div class="line">2020</div><div class="line">2021</div><div class="line">2022</div><div class="line">2023</div><div class="line">2024</div><div class="line">2025</div><div class="line">2026</div><div class="line">2027</div><div class="line">2028</div><div class="line">2029</div><div class="line">2030</div><div class="line">2031</div><div class="line">2032</div><div class="line">2033</div><div class="line">2034</div><div class="line">2035</div><div class="line">2036</div><div class="line">2037</div><div class="line">2038</div><div class="line">2039</div><div class="line">2040</div><div class="line">2041</div><div class="line">2042</div><div class="line">2043</div><div class="line">2044</div><div class="line">2045</div><div class="line">2046</div><div class="line">2047</div><div class="line">2048</div><div class="line">2049</div><div class="line">2050</div><div class="line">2051</div><div class="line">2052</div><div class="line">2053</div><div class="line">2054</div><div class="line">2055</div><div class="line">2056</div><div class="line">2057</div><div class="line">2058</div><div class="line">2059</div><div class="line">2060</div><div class="line">2061</div><div class="line">2062</div><div class="line">2063</div><div class="line">2064</div><div class="line">2065</div><div class="line">2066</div><div class="line">2067</div><div class="line">2068</div><div class="line">2069</div><div class="line">2070</div><div class="line">2071</div><div class="line">2072</div><div class="line">2073</div><div class="line">2074</div><div class="line">2075</div><div class="line">2076</div><div class="line">2077</div><div class="line">2078</div><div class="line">2079</div><div class="line">2080</div><div class="line">2081</div><div class="line">2082</div><div class="line">2083</div><div class="line">2084</div><div class="line">2085</div><div class="line">2086</div><div class="line">2087</div><div class="line">2088</div><div class="line">2089</div><div class="line">2090</div><div class="line">2091</div><div class="line">2092</div><div class="line">2093</div><div class="line">2094</div><div class="line">2095</div><div class="line">2096</div><div class="line">2097</div><div class="line">2098</div><div class="line">2099</div><div class="line">2100</div><div class="line">2101</div><div class="line">2102</div><div class="line">2103</div><div class="line">2104</div><div class="line">2105</div><div class="line">2106</div><div class="line">2107</div><div class="line">2108</div><div class="line">2109</div><div class="line">2110</div><div class="line">2111</div><div class="line">2112</div><div class="line">2113</div><div class="line">2114</div><div class="line">2115</div><div class="line">2116</div><div class="line">2117</div><div class="line">2118</div><div class="line">2119</div><div class="line">2120</div><div class="line">2121</div><div class="line">2122</div><div class="line">2123</div><div class="line">2124</div><div class="line">2125</div><div class="line">2126</div><div class="line">2127</div><div class="line">2128</div><div class="line">2129</div><div class="line">2130</div><div class="line">2131</div><div class="line">2132</div><div class="line">2133</div><div class="line">2134</div><div class="line">2135</div><div class="line">2136</div><div class="line">2137</div><div class="line">2138</div><div class="line">2139</div><div class="line">2140</div><div class="line">2141</div><div class="line">2142</div><div class="line">2143</div><div class="line">2144</div><div class="line">2145</div><div class="line">2146</div><div class="line">2147</div><div class="line">2148</div><div class="line">2149</div><div class="line">2150</div><div class="line">2151</div><div class="line">2152</div><div class="line">2153</div><div class="line">2154</div><div class="line">2155</div><div class="line">2156</div><div class="line">2157</div><div class="line">2158</div><div class="line">2159</div><div class="line">2160</div><div class="line">2161</div><div class="line">2162</div><div class="line">2163</div><div class="line">2164</div><div class="line">2165</div><div class="line">2166</div><div class="line">2167</div><div class="line">2168</div><div class="line">2169</div><div class="line">2170</div><div class="line">2171</div><div class="line">2172</div><div class="line">2173</div><div class="line">2174</div><div class="line">2175</div><div class="line">2176</div><div class="line">2177</div><div class="line">2178</div><div class="line">2179</div><div class="line">2180</div><div class="line">2181</div><div class="line">2182</div><div class="line">2183</div><div class="line">2184</div><div class="line">2185</div><div class="line">2186</div><div class="line">2187</div><div class="line">2188</div><div class="line">2189</div><div class="line">2190</div><div class="line">2191</div><div class="line">2192</div><div class="line">2193</div><div class="line">2194</div><div class="line">2195</div><div class="line">2196</div><div class="line">2197</div><div class="line">2198</div><div class="line">2199</div><div class="line">2200</div><div class="line">2201</div><div class="line">2202</div><div class="line">2203</div><div class="line">2204</div><div class="line">2205</div><div class="line">2206</div><div class="line">2207</div><div class="line">2208</div><div class="line">2209</div><div class="line">2210</div><div class="line">2211</div><div class="line">2212</div><div class="line">2213</div><div class="line">2214</div><div class="line">2215</div><div class="line">2216</div><div class="line">2217</div><div class="line">2218</div><div class="line">2219</div><div class="line">2220</div><div class="line">2221</div><div class="line">2222</div><div class="line">2223</div><div class="line">2224</div><div class="line">2225</div><div class="line">2226</div><div class="line">2227</div><div class="line">2228</div><div class="line">2229</div><div class="line">2230</div><div class="line">2231</div><div class="line">2232</div><div class="line">2233</div><div class="line">2234</div><div class="line">2235</div><div class="line">2236</div><div class="line">2237</div><div class="line">2238</div><div class="line">2239</div><div class="line">2240</div><div class="line">2241</div><div class="line">2242</div><div class="line">2243</div><div class="line">2244</div><div class="line">2245</div><div class="line">2246</div><div class="line">2247</div><div class="line">2248</div><div class="line">2249</div><div class="line">2250</div><div class="line">2251</div><div class="line">2252</div><div class="line">2253</div><div class="line">2254</div><div class="line">2255</div><div class="line">2256</div><div class="line">2257</div><div class="line">2258</div><div class="line">2259</div><div class="line">2260</div><div class="line">2261</div><div class="line">2262</div><div class="line">2263</div><div class="line">2264</div><div class="line">2265</div><div class="line">2266</div><div class="line">2267</div><div class="line">2268</div><div class="line">2269</div><div class="line">2270</div><div class="line">2271</div><div class="line">2272</div><div class="line">2273</div><div class="line">2274</div><div class="line">2275</div><div class="line">2276</div><div class="line">2277</div><div class="line">2278</div><div class="line">2279</div><div class="line">2280</div><div class="line">2281</div><div class="line">2282</div><div class="line">2283</div><div class="line">2284</div><div class="line">2285</div><div class="line">2286</div><div class="line">2287</div><div class="line">2288</div><div class="line">2289</div><div class="line">2290</div><div class="line">2291</div><div class="line">2292</div><div class="line">2293</div><div class="line">2294</div><div class="line">2295</div><div class="line">2296</div><div class="line">2297</div><div class="line">2298</div><div class="line">2299</div><div class="line">2300</div><div class="line">2301</div><div class="line">2302</div><div class="line">2303</div><div class="line">2304</div><div class="line">2305</div><div class="line">2306</div><div class="line">2307</div><div class="line">2308</div><div class="line">2309</div><div class="line">2310</div><div class="line">2311</div><div class="line">2312</div><div class="line">2313</div><div class="line">2314</div><div class="line">2315</div><div class="line">2316</div><div class="line">2317</div><div class="line">2318</div><div class="line">2319</div><div class="line">2320</div><div class="line">2321</div><div class="line">2322</div><div class="line">2323</div><div class="line">2324</div><div class="line">2325</div><div class="line">2326</div><div class="line">2327</div><div class="line">2328</div><div class="line">2329</div><div class="line">2330</div><div class="line">2331</div><div class="line">2332</div><div class="line">2333</div><div class="line">2334</div><div class="line">2335</div><div class="line">2336</div><div class="line">2337</div><div class="line">2338</div><div class="line">2339</div><div class="line">2340</div><div class="line">2341</div><div class="line">2342</div><div class="line">2343</div><div class="line">2344</div><div class="line">2345</div><div class="line">2346</div><div class="line">2347</div><div class="line">2348</div><div class="line">2349</div><div class="line">2350</div><div class="line">2351</div><div class="line">2352</div><div class="line">2353</div><div class="line">2354</div><div class="line">2355</div><div class="line">2356</div><div class="line">2357</div><div class="line">2358</div><div class="line">2359</div><div class="line">2360</div><div class="line">2361</div><div class="line">2362</div><div class="line">2363</div><div class="line">2364</div><div class="line">2365</div><div class="line">2366</div><div class="line">2367</div><div class="line">2368</div><div class="line">2369</div><div class="line">2370</div><div class="line">2371</div><div class="line">2372</div><div class="line">2373</div><div class="line">2374</div><div class="line">2375</div><div class="line">2376</div><div class="line">2377</div><div class="line">2378</div><div class="line">2379</div><div class="line">2380</div><div class="line">2381</div><div class="line">2382</div><div class="line">2383</div><div class="line">2384</div><div class="line">2385</div><div class="line">2386</div><div class="line">2387</div><div class="line">2388</div><div class="line">2389</div><div class="line">2390</div><div class="line">2391</div><div class="line">2392</div><div class="line">2393</div><div class="line">2394</div><div class="line">2395</div><div class="line">2396</div><div class="line">2397</div><div class="line">2398</div><div class="line">2399</div><div class="line">2400</div><div class="line">2401</div><div class="line">2402</div><div class="line">2403</div><div class="line">2404</div><div class="line">2405</div><div class="line">2406</div><div class="line">2407</div><div class="line">2408</div><div class="line">2409</div><div class="line">2410</div><div class="line">2411</div><div class="line">2412</div><div class="line">2413</div><div class="line">2414</div><div class="line">2415</div><div class="line">2416</div><div class="line">2417</div><div class="line">2418</div><div class="line">2419</div><div class="line">2420</div><div class="line">2421</div><div class="line">2422</div><div class="line">2423</div><div class="line">2424</div><div class="line">2425</div><div class="line">2426</div><div class="line">2427</div><div class="line">2428</div><div class="line">2429</div><div class="line">2430</div><div class="line">2431</div><div class="line">2432</div><div class="line">2433</div><div class="line">2434</div><div class="line">2435</div><div class="line">2436</div><div class="line">2437</div><div class="line">2438</div><div class="line">2439</div><div class="line">2440</div><div class="line">2441</div><div class="line">2442</div><div class="line">2443</div><div class="line">2444</div><div class="line">2445</div><div class="line">2446</div><div class="line">2447</div><div class="line">2448</div><div class="line">2449</div><div class="line">2450</div><div class="line">2451</div><div class="line">2452</div><div class="line">2453</div><div class="line">2454</div><div class="line">2455</div><div class="line">2456</div><div class="line">2457</div><div class="line">2458</div><div class="line">2459</div><div class="line">2460</div><div class="line">2461</div><div class="line">2462</div><div class="line">2463</div><div class="line">2464</div><div class="line">2465</div><div class="line">2466</div><div class="line">2467</div><div class="line">2468</div><div class="line">2469</div><div class="line">2470</div><div class="line">2471</div><div class="line">2472</div><div class="line">2473</div><div class="line">2474</div><div class="line">2475</div><div class="line">2476</div><div class="line">2477</div><div class="line">2478</div><div class="line">2479</div><div class="line">2480</div><div class="line">2481</div><div class="line">2482</div><div class="line">2483</div><div class="line">2484</div><div class="line">2485</div><div class="line">2486</div><div class="line">2487</div><div class="line">2488</div><div class="line">2489</div><div class="line">2490</div><div class="line">2491</div><div class="line">2492</div><div class="line">2493</div><div class="line">2494</div><div class="line">2495</div><div class="line">2496</div><div class="line">2497</div><div class="line">2498</div><div class="line">2499</div><div class="line">2500</div><div class="line">2501</div><div class="line">2502</div><div class="line">2503</div><div class="line">2504</div><div class="line">2505</div><div class="line">2506</div><div class="line">2507</div><div class="line">2508</div><div class="line">2509</div><div class="line">2510</div><div class="line">2511</div><div class="line">2512</div><div class="line">2513</div><div class="line">2514</div><div class="line">2515</div><div class="line">2516</div><div class="line">2517</div><div class="line">2518</div><div class="line">2519</div><div class="line">2520</div><div class="line">2521</div><div class="line">2522</div><div class="line">2523</div><div class="line">2524</div><div class="line">2525</div><div class="line">2526</div><div class="line">2527</div><div class="line">2528</div><div class="line">2529</div><div class="line">2530</div><div class="line">2531</div><div class="line">2532</div><div class="line">2533</div><div class="line">2534</div><div class="line">2535</div><div class="line">2536</div><div class="line">2537</div><div class="line">2538</div><div class="line">2539</div><div class="line">2540</div><div class="line">2541</div><div class="line">2542</div><div class="line">2543</div><div class="line">2544</div><div class="line">2545</div><div class="line">2546</div><div class="line">2547</div><div class="line">2548</div><div class="line">2549</div><div class="line">2550</div><div class="line">2551</div><div class="line">2552</div><div class="line">2553</div><div class="line">2554</div><div class="line">2555</div><div class="line">2556</div><div class="line">2557</div><div class="line">2558</div><div class="line">2559</div><div class="line">2560</div><div class="line">2561</div><div class="line">2562</div><div class="line">2563</div><div class="line">2564</div><div class="line">2565</div><div class="line">2566</div><div class="line">2567</div><div class="line">2568</div><div class="line">2569</div><div class="line">2570</div><div class="line">2571</div><div class="line">2572</div><div class="line">2573</div><div class="line">2574</div><div class="line">2575</div><div class="line">2576</div><div class="line">2577</div><div class="line">2578</div><div class="line">2579</div><div class="line">2580</div><div class="line">2581</div><div class="line">2582</div><div class="line">2583</div><div class="line">2584</div><div class="line">2585</div><div class="line">2586</div><div class="line">2587</div><div class="line">2588</div><div class="line">2589</div><div class="line">2590</div><div class="line">2591</div><div class="line">2592</div><div class="line">2593</div><div class="line">2594</div><div class="line">2595</div><div class="line">2596</div><div class="line">2597</div><div class="line">2598</div><div class="line">2599</div><div class="line">2600</div><div class="line">2601</div><div class="line">2602</div><div class="line">2603</div><div class="line">2604</div><div class="line">2605</div><div class="line">2606</div><div class="line">2607</div><div class="line">2608</div><div class="line">2609</div><div class="line">2610</div><div class="line">2611</div><div class="line">2612</div><div class="line">2613</div><div class="line">2614</div><div class="line">2615</div><div class="line">2616</div><div class="line">2617</div><div class="line">2618</div><div class="line">2619</div><div class="line">2620</div><div class="line">2621</div><div class="line">2622</div><div class="line">2623</div><div class="line">2624</div><div class="line">2625</div><div class="line">2626</div><div class="line">2627</div><div class="line">2628</div><div class="line">2629</div><div class="line">2630</div><div class="line">2631</div><div class="line">2632</div><div class="line">2633</div><div class="line">2634</div><div class="line">2635</div><div class="line">2636</div><div class="line">2637</div><div class="line">2638</div><div class="line">2639</div><div class="line">2640</div><div class="line">2641</div><div class="line">2642</div><div class="line">2643</div><div class="line">2644</div><div class="line">2645</div><div class="line">2646</div><div class="line">2647</div><div class="line">2648</div><div class="line">2649</div><div class="line">2650</div><div class="line">2651</div><div class="line">2652</div><div class="line">2653</div><div class="line">2654</div><div class="line">2655</div><div class="line">2656</div><div class="line">2657</div><div class="line">2658</div><div class="line">2659</div><div class="line">2660</div></pre></td><td class="code"><pre><div class="line">name: <span class="string">"ResNet-50"</span></div><div class="line">layer &#123;</div><div class="line">    name: <span class="string">"data"</span></div><div class="line">    type: <span class="string">"ImageData"</span></div><div class="line">    top: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"label"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TRAIN</div><div class="line">    &#125;</div><div class="line">    transform_param &#123;</div><div class="line">        mirror: <span class="literal">true</span></div><div class="line">        #crop_size: <span class="number">224</span></div><div class="line">        mean_value: <span class="number">104</span></div><div class="line">        mean_value: <span class="number">117</span></div><div class="line">        mean_value: <span class="number">123</span></div><div class="line">    &#125;</div><div class="line">    image_data_param &#123;</div><div class="line">        source: <span class="string">"/home/satisfie/imagenet/train_new.txt"</span></div><div class="line">        batch_size: <span class="number">4</span></div><div class="line">        new_height: <span class="number">224</span></div><div class="line">        new_width: <span class="number">224</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">    name: <span class="string">"data"</span></div><div class="line">    type: <span class="string">"Data"</span></div><div class="line">    top: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"label"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">    transform_param &#123;</div><div class="line">        mirror: <span class="literal">false</span></div><div class="line">        #crop_size: <span class="number">224</span></div><div class="line">        mean_value: <span class="number">104</span></div><div class="line">        mean_value: <span class="number">117</span></div><div class="line">        mean_value: <span class="number">123</span></div><div class="line">    &#125;</div><div class="line">    image_data_param &#123;</div><div class="line">        source: <span class="string">"/home/satisfie/imagenet/val_new.txt"</span></div><div class="line">        batch_size: <span class="number">1</span></div><div class="line">        new_height: <span class="number">224</span></div><div class="line">        new_width: <span class="number">224</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"conv1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">7</span></div><div class="line">        pad: <span class="number">3</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"bn_conv1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"scale_conv1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"conv1_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"pool1"</span></div><div class="line">    name: <span class="string">"pool1"</span></div><div class="line">    type: <span class="string">"Pooling"</span></div><div class="line">    pooling_param &#123;</div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        pool: MAX</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"res2a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"bn2a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"scale2a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool1"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"res2a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"bn2a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"scale2a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"res2a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"res2a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"bn2a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"scale2a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"res2a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"res2a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"bn2a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"scale2a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a"</span></div><div class="line">    name: <span class="string">"res2a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    top: <span class="string">"res2a"</span></div><div class="line">    name: <span class="string">"res2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"res2b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"bn2b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"scale2b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"res2b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"res2b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"bn2b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"scale2b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"res2b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"res2b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"bn2b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"scale2b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b"</span></div><div class="line">    name: <span class="string">"res2b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    top: <span class="string">"res2b"</span></div><div class="line">    name: <span class="string">"res2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"res2c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"bn2c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"scale2c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"res2c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"res2c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"bn2c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"scale2c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"res2c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"res2c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"bn2c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"scale2c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c"</span></div><div class="line">    name: <span class="string">"res2c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res2c"</span></div><div class="line">    name: <span class="string">"res2c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"res3a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"bn3a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"scale3a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"res3a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"bn3a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"scale3a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"res3a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"res3a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"bn3a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"scale3a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"res3a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"res3a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"bn3a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"scale3a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a"</span></div><div class="line">    name: <span class="string">"res3a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    top: <span class="string">"res3a"</span></div><div class="line">    name: <span class="string">"res3a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"res3b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"bn3b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"scale3b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"res3b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"res3b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"bn3b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"scale3b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"res3b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"res3b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"bn3b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"scale3b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b"</span></div><div class="line">    name: <span class="string">"res3b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    top: <span class="string">"res3b"</span></div><div class="line">    name: <span class="string">"res3b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"res3c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"bn3c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"scale3c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"res3c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"res3c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"bn3c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"scale3c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"res3c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"res3c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"bn3c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"scale3c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c"</span></div><div class="line">    name: <span class="string">"res3c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    top: <span class="string">"res3c"</span></div><div class="line">    name: <span class="string">"res3c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"res3d_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"bn3d_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"scale3d_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"res3d_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"res3d_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"bn3d_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"scale3d_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"res3d_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"res3d_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"bn3d_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"scale3d_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d"</span></div><div class="line">    name: <span class="string">"res3d"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res3d"</span></div><div class="line">    name: <span class="string">"res3d_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"res4a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"bn4a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"scale4a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"res4a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"bn4a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"scale4a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"res4a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"res4a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"bn4a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"scale4a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"res4a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"res4a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"bn4a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"scale4a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a"</span></div><div class="line">    name: <span class="string">"res4a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    top: <span class="string">"res4a"</span></div><div class="line">    name: <span class="string">"res4a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"res4b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"bn4b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"scale4b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"res4b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"res4b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"bn4b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"scale4b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"res4b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"res4b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"bn4b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"scale4b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b"</span></div><div class="line">    name: <span class="string">"res4b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    top: <span class="string">"res4b"</span></div><div class="line">    name: <span class="string">"res4b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"res4c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"bn4c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"scale4c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"res4c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"res4c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"bn4c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"scale4c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"res4c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"res4c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"bn4c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"scale4c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c"</span></div><div class="line">    name: <span class="string">"res4c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    top: <span class="string">"res4c"</span></div><div class="line">    name: <span class="string">"res4c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"res4d_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"bn4d_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"scale4d_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"res4d_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"res4d_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"bn4d_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"scale4d_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"res4d_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"res4d_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"bn4d_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"scale4d_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d"</span></div><div class="line">    name: <span class="string">"res4d"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    top: <span class="string">"res4d"</span></div><div class="line">    name: <span class="string">"res4d_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"res4e_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"bn4e_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"scale4e_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"res4e_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"res4e_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"bn4e_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"scale4e_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"res4e_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"res4e_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"bn4e_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"scale4e_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e"</span></div><div class="line">    name: <span class="string">"res4e"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    top: <span class="string">"res4e"</span></div><div class="line">    name: <span class="string">"res4e_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"res4f_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"bn4f_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"scale4f_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"res4f_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"res4f_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"bn4f_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"scale4f_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"res4f_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"res4f_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"bn4f_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"scale4f_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f"</span></div><div class="line">    name: <span class="string">"res4f"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res4f"</span></div><div class="line">    name: <span class="string">"res4f_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"res5a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"bn5a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"scale5a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"res5a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"bn5a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"scale5a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"res5a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"res5a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"bn5a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"scale5a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"res5a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"res5a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"bn5a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"scale5a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a"</span></div><div class="line">    name: <span class="string">"res5a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    top: <span class="string">"res5a"</span></div><div class="line">    name: <span class="string">"res5a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"res5b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"bn5b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"scale5b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"res5b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"res5b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"bn5b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"scale5b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"res5b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"res5b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"bn5b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"scale5b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b"</span></div><div class="line">    name: <span class="string">"res5b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    top: <span class="string">"res5b"</span></div><div class="line">    name: <span class="string">"res5b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"res5c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"bn5c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"scale5c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"res5c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"res5c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"bn5c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"scale5c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"res5c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"res5c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"bn5c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"scale5c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c"</span></div><div class="line">    name: <span class="string">"res5c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c"</span></div><div class="line">    top: <span class="string">"res5c"</span></div><div class="line">    name: <span class="string">"res5c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c"</span></div><div class="line">    top: <span class="string">"pool5"</span></div><div class="line">    name: <span class="string">"pool5"</span></div><div class="line">    type: <span class="string">"Pooling"</span></div><div class="line">    pooling_param &#123;</div><div class="line">        kernel_size: <span class="number">7</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        pool: AVE</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool5"</span></div><div class="line">    top: <span class="string">"fc1000"</span></div><div class="line">    name: <span class="string">"fc1000"</span></div><div class="line">    type: <span class="string">"InnerProduct"</span></div><div class="line">    param &#123;</div><div class="line">        lr_mult: <span class="number">1</span></div><div class="line">        decay_mult: <span class="number">1</span></div><div class="line">    &#125;</div><div class="line">    param &#123;</div><div class="line">        lr_mult: <span class="number">2</span></div><div class="line">        decay_mult: <span class="number">1</span></div><div class="line">    &#125;</div><div class="line">    inner_product_param &#123;</div><div class="line">        num_output: <span class="number">1000</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"xavier"</span></div><div class="line">        &#125;</div><div class="line">        bias_filler &#123;</div><div class="line">            type: <span class="string">"constant"</span></div><div class="line">            value: <span class="number">0</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    name: <span class="string">"loss"</span></div><div class="line">    type: <span class="string">"SoftmaxWithLoss"</span></div><div class="line">    top: <span class="string">"loss"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    top: <span class="string">"acc/top-1"</span></div><div class="line">    name: <span class="string">"acc/top-1"</span></div><div class="line">    type: <span class="string">"Accuracy"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    top: <span class="string">"acc/top-5"</span></div><div class="line">    name: <span class="string">"acc/top-5"</span></div><div class="line">    type: <span class="string">"Accuracy"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">    accuracy_param &#123;</div><div class="line">        top_k: <span class="number">5</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="John Doe" />
          <p class="site-author-name" itemprop="name">John Doe</p>
          <p class="site-description motion-element" itemprop="description">Stay hungry</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>







        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
</html>
