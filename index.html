<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="learn, think" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Stay hungry">
<meta property="og:type" content="website">
<meta property="og:title" content="Keson's blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Keson's blog">
<meta property="og:description" content="Stay hungry">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keson's blog">
<meta name="twitter:description" content="Stay hungry">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Keson's blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>











  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Keson's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/27/Caffe-Layer-学习/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/27/Caffe-Layer-学习/" itemprop="url">
                  Caffe Layer 学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-27T16:13:13+08:00">
              2016-11-27
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-02-27T16:13:43+08:00">
              2017-02-27
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/27/Caffe-Layer-学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/27/Caffe-Layer-学习/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/27/Caffe-Layer-学习/" class="leancloud_visitors" data-flag-title="Caffe Layer 学习">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/26/Jetson-TX1-训练darknet/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/26/Jetson-TX1-训练darknet/" itemprop="url">
                  Jetson TX1 训练darknet
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-26T10:35:13+08:00">
              2016-11-26
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-06T10:36:00+08:00">
              2017-03-06
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/26/Jetson-TX1-训练darknet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/26/Jetson-TX1-训练darknet/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/26/Jetson-TX1-训练darknet/" class="leancloud_visitors" data-flag-title="Jetson TX1 训练darknet">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="在Jetson-TX1-上训练darknet"><a href="#在Jetson-TX1-上训练darknet" class="headerlink" title="在Jetson TX1 上训练darknet"></a>在Jetson TX1 上训练darknet</h1><p>本文介绍在Jetson TX1上用VOC2007数据集训练YOLO。</p>
<p>Reference:</p>
<p><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">YOLO主页</a></p>
<h2 id="1-下载数据集"><a href="#1-下载数据集" class="headerlink" title="1 下载数据集"></a>1 下载数据集</h2><p>Pascal VOC 数据集是一个用来进行目标检测的数据集，官网链接<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="external">pascal voc</a></p>
<p>从上面下载VOC2007 的数据集，解压后放到 /darknet/scripts  </p>
<h2 id="2-修改"><a href="#2-修改" class="headerlink" title="2 修改"></a>2 修改</h2><p>scripts下的voc_label.py可以用来将voc数据集转换成yolo格式的数据集，对voc_label.py进行修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sets=[(<span class="string">'2007'</span>,<span class="string">'train'</span>),(<span class="string">'2007'</span>,<span class="string">'val'</span>)] //这里只用了VOC2007里的train和validation数据集</div></pre></td></tr></table></figure>
<p>如果是自己的数据集，需要相应的修改成VOC格式，在进行修改，<br>可以参考这里<a href="http://blog.csdn.net/sinat_30071459/article/details/53100791" target="_blank" rel="external">YOLO(v1)用自己的数据集训练模型</a><br><a href="http://blog.csdn.net/sinat_30071459/article/details/50723212" target="_blank" rel="external">做自己的VOC2007数据集</a></p>
<p>运行pyhon voc_label.py后可以发现/VOCdevkit/VOC2007 下多了一个labels文件夹。</p>
<h2 id="修改YOLO-源码"><a href="#修改YOLO-源码" class="headerlink" title="修改YOLO 源码"></a>修改YOLO 源码</h2><p>然后修改/darknet/src/yolo.c中的源码</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">char</span> *train_images=<span class="string">"/home/ubuntu/darknet/scripts/2007_train.txt"</span> </div><div class="line"><span class="keyword">char</span> *backup_directory= <span class="string">"/home/ubuntu/darknet/backup"</span> <span class="comment">//用来保存中间的权重</span></div></pre></td></tr></table></figure>
<h3 id="重新训练"><a href="#重新训练" class="headerlink" title="重新训练"></a>重新训练</h3><p>make编译后，可以重新训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">./darknet yolo train cfg/tiny-yolo.cfg darknet.conv.weights</div><div class="line">``` </div><div class="line">darknet.conv.weights 是卷积网络在分类网络上预训练的权重，在此基础上训练，</div><div class="line">可以从这里下载[darknet.conv.weights](http://download.csdn.net/detail/kesonyk/9707462)</div><div class="line"></div><div class="line">下载后放到/darknet目录即可</div><div class="line"></div><div class="line">### 在Jetson TX1 上训练时的技巧</div><div class="line">由于TX1 只有4G的共用内存和显存，因此batch需要重新设置，更改tiny-yolo.cfg的配置</div></pre></td></tr></table></figure>
<p>batch=64<br>subdivisions=8<br>```<br>其中batch是64，指的是64张图片后更新权重，subdivions是为了适应GPU有限的资源，在这里GPU每次只处理 8(64/8)张图片.所以是进行8个新的子batch后更新权重。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" itemprop="url">
                  Caffe源码解读6--BaseConvolutionLayer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-20T14:27:12+08:00">
              2016-11-20
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-20T14:28:35+08:00">
              2016-12-20
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/20/Caffe源码解读6-BaseConvolutionLayer/" class="leancloud_visitors" data-flag-title="Caffe源码解读6--BaseConvolutionLayer">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Message-ConvolutionParameter"><a href="#Message-ConvolutionParameter" class="headerlink" title="Message ConvolutionParameter"></a>Message ConvolutionParameter</h1><p>分析caffe中的每个层，应该先看caffe.proto中关于该层的参数定义，<code>message ConvolutionParameter</code>的定义如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">message ConvolutionParameter &#123;</div><div class="line">  optional uint32 num_output = <span class="number">1</span>; <span class="comment">// The number of outputs for the layer</span></div><div class="line">  optional <span class="keyword">bool</span> bias_term = <span class="number">2</span> [<span class="keyword">default</span> = <span class="literal">true</span>]; <span class="comment">// whether to have bias terms</span></div><div class="line"></div><div class="line">  <span class="comment">// Pad, kernel size, and stride are all given as a single value for equal</span></div><div class="line">  <span class="comment">// dimensions in all spatial dimensions, or once per spatial dimension.</span></div><div class="line">  repeated uint32 pad = <span class="number">3</span>; <span class="comment">// The padding size; defaults to 0</span></div><div class="line">  repeated uint32 kernel_size = <span class="number">4</span>; <span class="comment">// The kernel size</span></div><div class="line">  repeated uint32 stride = <span class="number">6</span>; <span class="comment">// The stride; defaults to 1</span></div><div class="line">  <span class="comment">// Factor used to dilate the kernel, (implicitly) zero-filling the resulting</span></div><div class="line">  <span class="comment">// holes. (Kernel dilation is sometimes referred to by its use in the</span></div><div class="line">  <span class="comment">// algorithme à trous from Holschneider et al. 1987.)</span></div><div class="line">  repeated uint32 dilation = <span class="number">18</span>; <span class="comment">// The dilation; defaults to 1</span></div><div class="line"></div><div class="line">  <span class="comment">// For 2D convolution only, the *_h and *_w versions may also be used to</span></div><div class="line">  <span class="comment">// specify both spatial dimensions.</span></div><div class="line">  optional uint32 pad_h = <span class="number">9</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding height (2D only)</span></div><div class="line">  optional uint32 pad_w = <span class="number">10</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding width (2D only)</span></div><div class="line">  optional uint32 kernel_h = <span class="number">11</span>; <span class="comment">// The kernel height (2D only)</span></div><div class="line">  optional uint32 kernel_w = <span class="number">12</span>; <span class="comment">// The kernel width (2D only)</span></div><div class="line">  optional uint32 stride_h = <span class="number">13</span>; <span class="comment">// The stride height (2D only)</span></div><div class="line">  optional uint32 stride_w = <span class="number">14</span>; <span class="comment">// The stride width (2D only)</span></div><div class="line"></div><div class="line">  optional uint32 group = <span class="number">5</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// The group size for group conv</span></div><div class="line"></div><div class="line">  optional FillerParameter weight_filler = <span class="number">7</span>; <span class="comment">// The filler for the weight</span></div><div class="line">  optional FillerParameter bias_filler = <span class="number">8</span>; <span class="comment">// The filler for the bias</span></div><div class="line">  <span class="keyword">enum</span> Engine &#123;</div><div class="line">    DEFAULT = <span class="number">0</span>;</div><div class="line">    CAFFE = <span class="number">1</span>;</div><div class="line">    CUDNN = <span class="number">2</span>;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = <span class="number">15</span> [<span class="keyword">default</span> = DEFAULT];</div><div class="line"></div><div class="line">  <span class="comment">// The axis to interpret as "channels" when performing convolution.</span></div><div class="line">  <span class="comment">// Preceding dimensions are treated as independent inputs;</span></div><div class="line">  <span class="comment">// succeeding dimensions are treated as "spatial".</span></div><div class="line">  <span class="comment">// With (N, C, H, W) inputs, and axis == 1 (the default), we perform</span></div><div class="line">  <span class="comment">// N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</span></div><div class="line">  <span class="comment">// groups g&gt;1) filters across the spatial axes (H, W) of the input.</span></div><div class="line">  <span class="comment">// With (N, C, D, H, W) inputs, and axis == 1, we perform</span></div><div class="line">  <span class="comment">// N independent 3D convolutions, sliding (C/g)-channels</span></div><div class="line">  <span class="comment">// filters across the spatial axes (D, H, W) of the input.</span></div><div class="line">  optional int32 axis = <span class="number">16</span> [<span class="keyword">default</span> = <span class="number">1</span>];</div><div class="line"></div><div class="line">  <span class="comment">// Whether to force use of the general ND convolution, even if a specific</span></div><div class="line">  <span class="comment">// implementation for blobs of the appropriate number of spatial dimensions</span></div><div class="line">  <span class="comment">// is available. (Currently, there is only a 2D-specific convolution</span></div><div class="line">  <span class="comment">// implementation; for input blobs with num_axes != 2, this option is</span></div><div class="line">  <span class="comment">// ignored and the ND implementation will be used.)</span></div><div class="line">  optional <span class="keyword">bool</span> force_nd_im2col = <span class="number">17</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Message生成的ConvolutionParameter类"><a href="#Message生成的ConvolutionParameter类" class="headerlink" title="Message生成的ConvolutionParameter类"></a>Message生成的ConvolutionParameter类</h1><h2 id="对于Message中的optional字段"><a href="#对于Message中的optional字段" class="headerlink" title="对于Message中的optional字段"></a>对于Message中的<code>optional</code>字段</h2><p>例如对于<code>optional uint32 num_output = 1</code>会生成以下对应的accessors</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// optional uint32 num_output = 1;</span></div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">has_num_output</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_num_output</span><span class="params">()</span></span>;</div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> kNumOutputFieldNumber = <span class="number">1</span>;</div><div class="line">::google::protobuf::<span class="function">uint32 <span class="title">num_output</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_num_output</span><span class="params">(::google::protobuf::uint32 value)</span></span>;</div></pre></td></tr></table></figure>
<p>同时会产生如下私有函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_has_num_output</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_has_num_output</span><span class="params">()</span></span>;</div></pre></td></tr></table></figure>
<h2 id="对于Message中的repeated字段"><a href="#对于Message中的repeated字段" class="headerlink" title="对于Message中的repeated字段"></a>对于Message中的<code>repeated</code>字段</h2><p>会产生如下的accessors</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// repeated uint32 pad = 3;</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">pad_size</span><span class="params">()</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear_pad</span><span class="params">()</span></span>;</div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> kPadFieldNumber = <span class="number">3</span>;</div><div class="line">::google::protobuf::<span class="function">uint32 <span class="title">pad</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_pad</span><span class="params">(<span class="keyword">int</span> index, ::google::protobuf::uint32 value)</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_pad</span><span class="params">(::google::protobuf::uint32 value)</span></span>;</div><div class="line"><span class="keyword">const</span> ::google::protobuf::RepeatedField&lt; ::google::protobuf::uint32 &gt;&amp;</div><div class="line">    pad() <span class="keyword">const</span>;</div><div class="line">::google::protobuf::RepeatedField&lt; ::google::protobuf::uint32 &gt;*</div><div class="line">    mutable_pad();</div></pre></td></tr></table></figure>
<h1 id="BaseConvolutionLayer类"><a href="#BaseConvolutionLayer类" class="headerlink" title="BaseConvolutionLayer类"></a><code>BaseConvolutionLayer</code>类</h1><p>该类继承<code>Layer</code>,也是<code>ConvolutionLayer</code>和<code>DeconvolutionLayer</code>的抽象类。</p>
<h2 id="protected成员变量和private变量"><a href="#protected成员变量和private变量" class="headerlink" title="protected成员变量和private变量"></a><code>protected</code>成员变量和<code>private</code>变量</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="comment">/// @brief The spatial dimensions of a filter kernel.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; kernel_shape_;  <span class="comment">//卷积核形状</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the stride.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; stride_;        <span class="comment">//步进</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the padding.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; pad_;           <span class="comment">//补充</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the dilation.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; dilation_;      <span class="comment">//膨胀系数</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the convolution input.</span></div><div class="line"> Blob&lt;<span class="keyword">int</span>&gt; conv_input_shape_; <span class="comment">//卷积的输入形状</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the col_buffer.</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; col_buffer_shape_; <span class="comment">//</span></div><div class="line"> <span class="comment">/// @brief The spatial dimensions of the output.</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; output_shape_;     <span class="comment">//输出的形状</span></div><div class="line"> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;* bottom_shape_; <span class="comment">//</span></div><div class="line"></div><div class="line"> <span class="keyword">int</span> num_spatial_axes_;  <span class="comment">//空间的轴个数</span></div><div class="line"> <span class="keyword">int</span> bottom_dim_;        <span class="comment">//输入维度</span></div><div class="line"> <span class="keyword">int</span> top_dim_;           <span class="comment">//输出维度</span></div><div class="line"></div><div class="line"> <span class="keyword">int</span> channel_axis_;      <span class="comment">//通道轴的索引</span></div><div class="line"> <span class="keyword">int</span> num_;               <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> channels_;          <span class="comment">//输入的通道数</span></div><div class="line"> <span class="keyword">int</span> group_;             <span class="comment">//卷积组的大小</span></div><div class="line"> <span class="keyword">int</span> out_spatial_dim_;   <span class="comment">//输出的空间维度</span></div><div class="line"> <span class="keyword">int</span> weight_offset_;     <span class="comment">//使用卷积组时用到的</span></div><div class="line"> <span class="keyword">int</span> num_output_;        <span class="comment">//卷积后的通道数</span></div><div class="line"> <span class="keyword">bool</span> bias_term_;        <span class="comment">//是否使用偏置</span></div><div class="line"> <span class="keyword">bool</span> is_1x1_;           <span class="comment">//是不是1*1的卷积</span></div><div class="line"> <span class="keyword">bool</span> force_nd_im2col_;  <span class="comment">//强制使用n维通用卷积</span></div><div class="line"> </div><div class="line"> <span class="keyword">private</span>:</div><div class="line"> </div><div class="line"> <span class="keyword">int</span> num_kernels_im2col_; <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> num_kernels_col2im_;</div><div class="line"> <span class="keyword">int</span> conv_out_channels_;    <span class="comment">//卷积的输出通道数</span></div><div class="line"> <span class="keyword">int</span> conv_in_channels_;     <span class="comment">//卷积的输入通道数</span></div><div class="line"> <span class="keyword">int</span> conv_out_spatial_dim_; <span class="comment">//卷积的输出空间维度=卷积后的h*w</span></div><div class="line"> <span class="keyword">int</span> kernel_dim_;           <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> col_offset_;           <span class="comment">//</span></div><div class="line"> <span class="keyword">int</span> output_offset_;</div><div class="line"></div><div class="line"> Blob&lt;Dtype&gt; col_buffer_;   <span class="comment">//使用im2col时使用的存储空间</span></div><div class="line"> Blob&lt;Dtype&gt; bias_multiplier_;</div></pre></td></tr></table></figure>
<h2 id="LayerSetUp"><a href="#LayerSetUp" class="headerlink" title="LayerSetUp"></a><code>LayerSetUp</code></h2><p>在<code>LayerSetUp</code>中，首先是对kernel size，padding,stride和输入的配置</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">// Configure the kernel size, padding, stride, and inputs.</span></div><div class="line">  ConvolutionParameter conv_param = <span class="keyword">this</span>-&gt;layer_param_.convolution_param(); <span class="comment">//配置卷积参数</span></div><div class="line">  force_nd_im2col_ = conv_param.force_nd_im2col(); <span class="comment">//是否强制使用n维im2col</span></div><div class="line">  channel_axis_ = bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(conv_param.axis());<span class="comment">//获取channel的axis</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> first_spatial_axis = channel_axis_ + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_axes = bottom[<span class="number">0</span>]-&gt;num_axes();     <span class="comment">//数据的axis数量</span></div><div class="line">  num_spatial_axes_ = num_axes - first_spatial_axis;</div><div class="line">  CHECK_GE(num_spatial_axes_, <span class="number">0</span>);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bottom_dim_blob_shape(<span class="number">1</span>, num_spatial_axes_ + <span class="number">1</span>);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; spatial_dim_blob_shape(<span class="number">1</span>, <span class="built_in">std</span>::max(num_spatial_axes_, <span class="number">1</span>));</div><div class="line">  </div><div class="line">  <span class="comment">// Setup filter kernel dimensions (kernel_shape_). 开始卷积核的设置</span></div><div class="line">  kernel_shape_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* kernel_shape_data = kernel_shape_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_kernel_h() || conv_param.has_kernel_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"kernel_h &amp; kernel_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.kernel_size_size())</div><div class="line">        &lt;&lt; <span class="string">"Either kernel_size or kernel_h/w should be specified; not both."</span>;</div><div class="line">    kernel_shape_data[<span class="number">0</span>] = conv_param.kernel_h();  <span class="comment">//给kernel_shape_data 赋值高度</span></div><div class="line">    kernel_shape_data[<span class="number">1</span>] = conv_param.kernel_w();  <span class="comment">//给kernel_shape_data 赋值宽度</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_kernel_dims = conv_param.kernel_size_size(); <span class="comment">//卷积核的数目</span></div><div class="line">    CHECK(num_kernel_dims == <span class="number">1</span> || num_kernel_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"kernel_size must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(kernel_size specified "</span> &lt;&lt; num_kernel_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">        kernel_shape_data[i] =</div><div class="line">            conv_param.kernel_size((num_kernel_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    CHECK_GT(kernel_shape_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Filter dimensions must be nonzero."</span>;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup stride dimensions (stride_). 开始步进的设置</span></div><div class="line">  stride_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* stride_data = stride_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_stride_h() || conv_param.has_stride_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"stride_h &amp; stride_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.stride_size())</div><div class="line">        &lt;&lt; <span class="string">"Either stride or stride_h/w should be specified; not both."</span>;</div><div class="line">    stride_data[<span class="number">0</span>] = conv_param.stride_h(); <span class="comment">//给步进赋值</span></div><div class="line">    stride_data[<span class="number">1</span>] = conv_param.stride_w();</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_stride_dims = conv_param.stride_size();</div><div class="line">    CHECK(num_stride_dims == <span class="number">0</span> || num_stride_dims == <span class="number">1</span> ||</div><div class="line">          num_stride_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"stride must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(stride specified "</span> &lt;&lt; num_stride_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultStride = <span class="number">1</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">      stride_data[i] = (num_stride_dims == <span class="number">0</span>) ? kDefaultStride :</div><div class="line">          conv_param.stride((num_stride_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">      CHECK_GT(stride_data[i], <span class="number">0</span>) &lt;&lt; <span class="string">"Stride dimensions must be nonzero."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup pad dimensions (pad_).  开始padding的配置</span></div><div class="line">  pad_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* pad_data = pad_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (conv_param.has_pad_h() || conv_param.has_pad_w()) &#123;</div><div class="line">    CHECK_EQ(num_spatial_axes_, <span class="number">2</span>)</div><div class="line">        &lt;&lt; <span class="string">"pad_h &amp; pad_w can only be used for 2D convolution."</span>;</div><div class="line">    CHECK_EQ(<span class="number">0</span>, conv_param.pad_size())</div><div class="line">        &lt;&lt; <span class="string">"Either pad or pad_h/w should be specified; not both."</span>;</div><div class="line">    pad_data[<span class="number">0</span>] = conv_param.pad_h();</div><div class="line">    pad_data[<span class="number">1</span>] = conv_param.pad_w();</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_pad_dims = conv_param.pad_size();</div><div class="line">    CHECK(num_pad_dims == <span class="number">0</span> || num_pad_dims == <span class="number">1</span> ||</div><div class="line">          num_pad_dims == num_spatial_axes_)</div><div class="line">        &lt;&lt; <span class="string">"pad must be specified once, or once per spatial dimension "</span></div><div class="line">        &lt;&lt; <span class="string">"(pad specified "</span> &lt;&lt; num_pad_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">        &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> kDefaultPad = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">      pad_data[i] = (num_pad_dims == <span class="number">0</span>) ? kDefaultPad :</div><div class="line">          conv_param.pad((num_pad_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Setup dilation dimensions (dilation_). 开始膨胀系数的配置</span></div><div class="line">  dilation_.Reshape(spatial_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* dilation_data = dilation_.mutable_cpu_data();</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_dilation_dims = conv_param.dilation_size();</div><div class="line">  CHECK(num_dilation_dims == <span class="number">0</span> || num_dilation_dims == <span class="number">1</span> ||</div><div class="line">        num_dilation_dims == num_spatial_axes_)</div><div class="line">      &lt;&lt; <span class="string">"dilation must be specified once, or once per spatial dimension "</span></div><div class="line">      &lt;&lt; <span class="string">"(dilation specified "</span> &lt;&lt; num_dilation_dims &lt;&lt; <span class="string">" times; "</span></div><div class="line">      &lt;&lt; num_spatial_axes_ &lt;&lt; <span class="string">" spatial dims)."</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kDefaultDilation = <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    dilation_data[i] = (num_dilation_dims == <span class="number">0</span>) ? kDefaultDilation :</div><div class="line">                       conv_param.dilation((num_dilation_dims == <span class="number">1</span>) ? <span class="number">0</span> : i);</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Special case: im2col is the identity for 1x1 convolution with stride 1</span></div><div class="line">  <span class="comment">// and no padding, so flag for skipping the buffer and transformation.</span></div><div class="line">  is_1x1_ = <span class="literal">true</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    is_1x1_ &amp;=</div><div class="line">        kernel_shape_data[i] == <span class="number">1</span> &amp;&amp; stride_data[i] == <span class="number">1</span> &amp;&amp; pad_data[i] == <span class="number">0</span>;</div><div class="line">    <span class="keyword">if</span> (!is_1x1_) &#123; <span class="keyword">break</span>; &#125;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Configure output channels and groups. 开始配置输出的通道数和卷积组</span></div><div class="line">  channels_ = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_);</div><div class="line">  num_output_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().num_output();</div><div class="line">  CHECK_GT(num_output_, <span class="number">0</span>);</div><div class="line">  group_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().group();</div><div class="line">  CHECK_EQ(channels_ % group_, <span class="number">0</span>);</div><div class="line">  CHECK_EQ(num_output_ % group_, <span class="number">0</span>)</div><div class="line">      &lt;&lt; <span class="string">"Number of output should be multiples of group."</span>;</div><div class="line">  <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">    conv_out_channels_ = channels_;</div><div class="line">    conv_in_channels_ = num_output_;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    conv_out_channels_ = num_output_;</div><div class="line">    conv_in_channels_ = channels_;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">// Handle the parameters: weights and biases. 开始处理权重参数</span></div><div class="line">  <span class="comment">// - blobs_[0] holds the filter weights       权重参数</span></div><div class="line">  <span class="comment">// - blobs_[1] holds the biases (optional)    偏置参数</span></div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weight_shape(<span class="number">2</span>);</div><div class="line">  weight_shape[<span class="number">0</span>] = conv_out_channels_;          <span class="comment">//输出的通道数</span></div><div class="line">  weight_shape[<span class="number">1</span>] = conv_in_channels_ / group_;  <span class="comment">//输入的通道数，分组卷积，针对多GPU的问题</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    weight_shape.push_back(kernel_shape_data[i]);</div><div class="line">  &#125;</div><div class="line">  bias_term_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().bias_term();</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bias_shape(bias_term_, num_output_);</div><div class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</div><div class="line">    CHECK_EQ(<span class="number">1</span> + bias_term_, <span class="keyword">this</span>-&gt;blobs_.size())</div><div class="line">        &lt;&lt; <span class="string">"Incorrect number of weight blobs."</span>;</div><div class="line">    <span class="keyword">if</span> (weight_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape()) &#123;</div><div class="line">      Blob&lt;Dtype&gt; weight_shaped_blob(weight_shape);</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect weight shape: expected shape "</span></div><div class="line">          &lt;&lt; weight_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></div><div class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;shape_string();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (bias_term_ &amp;&amp; bias_shape != <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape()) &#123;</div><div class="line">      Blob&lt;Dtype&gt; bias_shaped_blob(bias_shape);</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Incorrect bias shape: expected shape "</span></div><div class="line">          &lt;&lt; bias_shaped_blob.shape_string() &lt;&lt; <span class="string">"; instead, shape was "</span></div><div class="line">          &lt;&lt; <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;shape_string();</div><div class="line">    &#125;</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">2</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// Initialize and fill the weights:</span></div><div class="line">    <span class="comment">// output channels x input channels per-group x kernel height x kernel width</span></div><div class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(weight_shape));</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; weight_filler(GetFiller&lt;Dtype&gt;(</div><div class="line">        <span class="keyword">this</span>-&gt;layer_param_.convolution_param().weight_filler()));</div><div class="line">    weight_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].get());</div><div class="line">    <span class="comment">// If necessary, initialize and fill the biases.</span></div><div class="line">    <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">      <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(bias_shape));</div><div class="line">      <span class="built_in">shared_ptr</span>&lt;Filler&lt;Dtype&gt; &gt; bias_filler(GetFiller&lt;Dtype&gt;(</div><div class="line">          <span class="keyword">this</span>-&gt;layer_param_.convolution_param().bias_filler()));</div><div class="line">      bias_filler-&gt;Fill(<span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].get());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  kernel_dim_ = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;count(<span class="number">1</span>); <span class="comment">//是3维度的乘积，输入图像的维度*卷积核的h*卷积核的w</span></div><div class="line">  weight_offset_ = conv_out_channels_ * kernel_dim_ / group_;</div><div class="line">  <span class="comment">// Propagate gradients to the parameters (as directed by backward pass).</span></div><div class="line">  <span class="keyword">this</span>-&gt;param_propagate_down_.resize(<span class="keyword">this</span>-&gt;blobs_.size(), <span class="literal">true</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape函数"><a href="#Reshape函数" class="headerlink" title="Reshape函数"></a><code>Reshape</code>函数</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> first_spatial_axis = channel_axis_ + <span class="number">1</span>;</div><div class="line">  CHECK_EQ(bottom[<span class="number">0</span>]-&gt;num_axes(), first_spatial_axis + num_spatial_axes_)</div><div class="line">      &lt;&lt; <span class="string">"bottom num_axes may not change."</span>;</div><div class="line">  num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>, channel_axis_);</div><div class="line">  CHECK_EQ(bottom[<span class="number">0</span>]-&gt;shape(channel_axis_), channels_)</div><div class="line">      &lt;&lt; <span class="string">"Input size incompatible with convolution kernel."</span>;</div><div class="line">  <span class="comment">// <span class="doctag">TODO:</span> generalize to handle inputs of different shapes.</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">1</span>; bottom_id &lt; bottom.size(); ++bottom_id) &#123;</div><div class="line">    CHECK(bottom[<span class="number">0</span>]-&gt;shape() == bottom[bottom_id]-&gt;shape())</div><div class="line">        &lt;&lt; <span class="string">"All inputs must have the same shape."</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Shape the tops.</span></div><div class="line">  bottom_shape_ = &amp;bottom[<span class="number">0</span>]-&gt;shape();</div><div class="line">  compute_output_shape();</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; top_shape(bottom[<span class="number">0</span>]-&gt;shape().begin(),</div><div class="line">      bottom[<span class="number">0</span>]-&gt;shape().begin() + channel_axis_);</div><div class="line">  top_shape.push_back(num_output_);</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    top_shape.push_back(output_shape_[i]);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</div><div class="line">    top[top_id]-&gt;Reshape(top_shape);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">    conv_out_spatial_dim_ = bottom[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    conv_out_spatial_dim_ = top[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  &#125;</div><div class="line">  col_offset_ = kernel_dim_ * conv_out_spatial_dim_;</div><div class="line">  output_offset_ = conv_out_channels_ * conv_out_spatial_dim_ / group_;</div><div class="line">  <span class="comment">// Setup input dimensions (conv_input_shape_).</span></div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bottom_dim_blob_shape(<span class="number">1</span>, num_spatial_axes_ + <span class="number">1</span>);</div><div class="line">  conv_input_shape_.Reshape(bottom_dim_blob_shape);</div><div class="line">  <span class="keyword">int</span>* conv_input_shape_data = conv_input_shape_.mutable_cpu_data();</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_ + <span class="number">1</span>; ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">      conv_input_shape_data[i] = top[<span class="number">0</span>]-&gt;shape(channel_axis_ + i);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      conv_input_shape_data[i] = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_ + i);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// The im2col result buffer will only hold one image at a time to avoid</span></div><div class="line">  <span class="comment">// overly large memory usage. In the special case of 1x1 convolution</span></div><div class="line">  <span class="comment">// it goes lazily unused to save memory.</span></div><div class="line">  col_buffer_shape_.clear();</div><div class="line">  col_buffer_shape_.push_back(kernel_dim_ * group_);</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_spatial_axes_; ++i) &#123;</div><div class="line">    <span class="keyword">if</span> (reverse_dimensions()) &#123;</div><div class="line">      col_buffer_shape_.push_back(input_shape(i + <span class="number">1</span>));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      col_buffer_shape_.push_back(output_shape_[i]);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  col_buffer_.Reshape(col_buffer_shape_);</div><div class="line">  bottom_dim_ = bottom[<span class="number">0</span>]-&gt;count(channel_axis_);</div><div class="line">  top_dim_ = top[<span class="number">0</span>]-&gt;count(channel_axis_);</div><div class="line">  num_kernels_im2col_ = conv_in_channels_ * conv_out_spatial_dim_;</div><div class="line">  num_kernels_col2im_ = reverse_dimensions() ? top_dim_ : bottom_dim_;</div><div class="line">  <span class="comment">// Set up the all ones "bias multiplier" for adding biases by BLAS</span></div><div class="line">  out_spatial_dim_ = top[<span class="number">0</span>]-&gt;count(first_spatial_axis);</div><div class="line">  <span class="keyword">if</span> (bias_term_) &#123;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bias_multiplier_shape(<span class="number">1</span>, out_spatial_dim_);</div><div class="line">    bias_multiplier_.Reshape(bias_multiplier_shape);</div><div class="line">    caffe_set(bias_multiplier_.count(), Dtype(<span class="number">1</span>),</div><div class="line">        bias_multiplier_.mutable_cpu_data());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="forward-cpu-gemm"><a href="#forward-cpu-gemm" class="headerlink" title="forward_cpu_gemm"></a><code>forward_cpu_gemm</code></h2><p>调用<code>conv_im2col_cpu</code>将图像对应卷积的图像块变成列向量，变成矩阵，方便后面进行矩阵乘法<br>这里需要注意的是<code>kernel_dim=输入的通道数*卷积核的高*卷积核的宽</code>。weights的形状是<code>[输出通道数，kernel_dim]</code>。<br>输入的图像的形状是<code>[图像的高\*图像的宽，输入的通道数\*卷积核的高\*卷积核的宽]</code><br>因此weights可以通过乘以输入图像的转置完成矩阵的相乘。</p>
<p>具体可以看上一篇的博文，<a href="https://satisfie.github.io/2016/11/19/Caffe%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB-im2col/" target="_blank" rel="external">Caffe中的卷积分析</a>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_gemm(<span class="keyword">const</span> Dtype* input,</div><div class="line">    <span class="keyword">const</span> Dtype* weights, Dtype* output, <span class="keyword">bool</span> skip_im2col) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* col_buff = input;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    <span class="keyword">if</span> (!skip_im2col) &#123;</div><div class="line">      conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());</div><div class="line">    &#125;</div><div class="line">    col_buff = col_buffer_.cpu_data();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">  </div><div class="line">     <span class="comment">//kernel_dim_=输入的通道数*卷积核的高*卷积核的宽</span></div><div class="line">  	 <span class="comment">//weights的形状是 [输出通道数,kernel_dim_]</span></div><div class="line">  	 </div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, conv_out_channels_ /</div><div class="line">        group_, conv_out_spatial_dim_, kernel_dim_,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g, col_buff + col_offset_ * g,</div><div class="line">        (Dtype)<span class="number">0.</span>, output + output_offset_ * g);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_bias(Dtype* output,</div><div class="line">    <span class="keyword">const</span> Dtype* bias) &#123;</div><div class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num_output_,</div><div class="line">      out_spatial_dim_, <span class="number">1</span>, (Dtype)<span class="number">1.</span>, bias, bias_multiplier_.cpu_data(),</div><div class="line">      (Dtype)<span class="number">1.</span>, output);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="backward-cpu-gemm"><a href="#backward-cpu-gemm" class="headerlink" title="backward_cpu_gemm"></a><code>backward_cpu_gemm</code></h2><p>主要是调用<code>conv_col2im_cpu</code>来讲列向量转回图像的形式，具体是调用了<code>col2im_cpu</code>函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::backward_cpu_gemm(<span class="keyword">const</span> Dtype* output,</div><div class="line">    <span class="keyword">const</span> Dtype* weights, Dtype* input) &#123;</div><div class="line">  Dtype* col_buff = col_buffer_.mutable_cpu_data();</div><div class="line">  <span class="keyword">if</span> (is_1x1_) &#123;</div><div class="line">    col_buff = input;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans, kernel_dim_,</div><div class="line">        conv_out_spatial_dim_, conv_out_channels_ / group_,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g, output + output_offset_ * g,</div><div class="line">        (Dtype)<span class="number">0.</span>, col_buff + col_offset_ * g);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    conv_col2im_cpu(col_buff, input);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="weight-cpu-gemm"><a href="#weight-cpu-gemm" class="headerlink" title="weight_cpu_gemm"></a><code>weight_cpu_gemm</code></h2><p>这个用于计算weight的导数，具体还要进一步细看</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> BaseConvolutionLayer&lt;Dtype&gt;::weight_cpu_gemm(<span class="keyword">const</span> Dtype* input,</div><div class="line">    <span class="keyword">const</span> Dtype* output, Dtype* weights) &#123;</div><div class="line">  <span class="keyword">const</span> Dtype* col_buff = input;</div><div class="line">  <span class="keyword">if</span> (!is_1x1_) &#123;</div><div class="line">    conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());</div><div class="line">    col_buff = col_buffer_.cpu_data();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group_; ++g) &#123;</div><div class="line">    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasTrans, conv_out_channels_ / group_,</div><div class="line">        kernel_dim_, conv_out_spatial_dim_,</div><div class="line">        (Dtype)<span class="number">1.</span>, output + output_offset_ * g, col_buff + col_offset_ * g,</div><div class="line">        (Dtype)<span class="number">1.</span>, weights + weight_offset_ * g);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/19/Caffe源码解读-im2col/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/19/Caffe源码解读-im2col/" itemprop="url">
                  Caffe源码解读--im2col
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-19T19:45:20+08:00">
              2016-11-19
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2016-12-19T19:46:46+08:00">
              2016-12-19
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/19/Caffe源码解读-im2col/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/11/19/Caffe源码解读-im2col/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/11/19/Caffe源码解读-im2col/" class="leancloud_visitors" data-flag-title="Caffe源码解读--im2col">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Caffe源码解读之卷积篇–im2col"><a href="#Caffe源码解读之卷积篇–im2col" class="headerlink" title="Caffe源码解读之卷积篇–im2col"></a>Caffe源码解读之卷积篇–im2col</h1><p>卷积中比较重要的几个参数有</p>
<ul>
<li>深度,depth，即决定有多少个filters</li>
<li>步长,stride,即卷积核的滑动间隔，默认为1</li>
<li>补充，padding，即在图像两边补充数据</li>
<li>膨胀，dilated, 在卷积的时候可以skip一定长度的像素</li>
</ul>
<p>通常情况下，卷积层后通常需要跟池化层，池化层统计区局部范围内的统计信息，然后再进一步的跟卷积层，这样可以扩大感受野的范围。但是这样会带来一个问题，池化层使得特征层的尺寸变得越来越小,针对层数很深的网络，最后卷积的特征量维度会很小。为了解决这个问题，有几种方法可以尝试：</p>
<p>（1）反卷积，利用上采样扩大特征层的尺度，比如<a href="https://arxiv.org/pdf/1605.06211.pdf" target="_blank" rel="external">FCN</a><br><img src="http://oi8824myj.bkt.clouddn.com/fcn.png" alt="fcn"></p>
<p>FCN网络是一个全卷积神经网络，直接在像素级别上端到端(end-to-end)地进行语义分割。为了解决下采样导致特征层尺度变小的问题，FCN采用双线性插值将响应张亮的长宽上采样到原图大小。</p>
<p>（2）dilated convolution，膨胀卷积的方法，来自于这篇论文<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="external">Multi-Scale Context Aggregation by Dilated Convolutions</a></p>
<p>由于池化层的存在，会使得特征层的尺寸越来越小，若去掉池化层，则感受野的范围就不能进行有效的扩大。在FCN中采用了上采样的方法来扩大特征层的尺寸，但是上采样不能将丢失的信息全部无损地找回来。</p>
<p>dilated convolution是一个很好的解决方法，去掉了池化层，针对感受野的问题，采用了skip的方法，通过skip一定的像素来进行感受野的扩大。</p>
<p>比如下图中，红色圆点对应的是卷积核中的数，绿色范围是感受野的大小。$3\times3$的卷积，图(a)中是默认的1-dilated卷积，感受野是$3\times3$；图(b)中采用2-dilated卷积，感受野为$7\times7$；图(c)中采用4-dilated，感受野是$15\times15$。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/dilated.png" alt="dilated"></p>
<h1 id="Caffe中的im2col"><a href="#Caffe中的im2col" class="headerlink" title="Caffe中的im2col"></a>Caffe中的im2col</h1><p>Reference:</p>
<p><a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">Convolution in Caffe: a memo</a></p>
<p><a href="https://www.zhihu.com/question/28385679" target="_blank" rel="external">在Caffe中如何计算卷积知乎</a></p>
<p>假设宽度为W，高度为H的图像，共有D个通道，则针对$K\times K$的卷积核，并考虑输出的filters个数为M。在Caffe中卷积操作的优化采用了一定的trick,将局部的patch转化成列向量来操作，对于$C\times H\times W$的图像，则对于1个filter,可以转化为$(H\times  W,K\times K\times D)$的矩阵来进行，然后用BLAS库中的<code>Gemm</code>函数来进行举证的乘法操作。</p>
<p>针对1个$K\times K$的卷积核，C个通道的feature map局部patch的转化为列向量如下图所示：<br><img src="http://oi8824myj.bkt.clouddn.com/conv1.png" alt="im2col1"></p>
<p>feature map平移后转化为列向量，如下：<br><img src="http://oi8824myj.bkt.clouddn.com/conv2.png" alt="im2col2"></p>
<p>对于$C\times H \times W$的feature map,得到的矩阵为$(H\times W,C\times K \times K)$</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/conv3.png" alt="im2col3"></p>
<p>针对$C_{out}$个filters，可以得到$(C_{out},C\times K\times K)$的矩阵，然后与转置后的feature map相乘得到$(C_{out},H\times W)$的矩阵。<br><img src="http://oi8824myj.bkt.clouddn.com/conv4.jpg" alt="im2col4"></p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>计算输出的feature map的尺寸，针对的而是2D的卷积，n维的卷积与之类似。</p>
<ul>
<li>输出的高度: output_h = (height + 2 <em> pad_h -(dilation_h </em> (kernel_h - 1) + 1)) / stride_h + 1</li>
<li>输出的宽度: output_w = (width  + 2 <em> pad_w -(dilation_w </em> (kernel_w - 1)+ 1)) / stride_w + 1</li>
<li>一个通道的size: channel_size=height*width</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">im2col_cpu</span><span class="params">(<span class="keyword">const</span> Dtype* data_im, <span class="keyword">const</span> <span class="keyword">int</span> channels,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</div><div class="line">    Dtype* data_col) &#123;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> output_h = (height + <span class="number">2</span> * pad_h -</div><div class="line">    (dilation_h * (kernel_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> output_w = (width + <span class="number">2</span> * pad_w -</div><div class="line">    (dilation_w * (kernel_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> channel_size = height * width;</div><div class="line">  </div><div class="line">  <span class="comment">//安装CHW的顺序进行</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> channel = channels; channel--; data_im += channel_size) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> kernel_row = <span class="number">0</span>; kernel_row &lt; kernel_h; kernel_row++) &#123;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> kernel_col = <span class="number">0</span>; kernel_col &lt; kernel_w; kernel_col++) &#123;</div><div class="line">        <span class="keyword">int</span> input_row = -pad_h + kernel_row * dilation_h;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> output_rows = output_h; output_rows; output_rows--) &#123;</div><div class="line">          <span class="keyword">if</span> (!is_a_ge_zero_and_a_lt_b(input_row, height)) &#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> output_cols = output_w; output_cols; output_cols--) &#123;</div><div class="line">              *(data_col++) = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">int</span> input_col = -pad_w + kernel_col * dilation_w;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> output_col = output_w; output_col; output_col--) &#123;</div><div class="line">              <span class="keyword">if</span> (is_a_ge_zero_and_a_lt_b(input_col, width)) &#123;</div><div class="line">                <span class="comment">//相应的数据copy到data_col指针</span></div><div class="line">                *(data_col++) = data_im[input_row * width + input_col];</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                *(data_col++) = <span class="number">0</span>;</div><div class="line">              &#125;</div><div class="line">              <span class="comment">//宽度上的stride</span></div><div class="line">              input_col += stride_w;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">          <span class="comment">//高度上stride</span></div><div class="line">          input_row += stride_h;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/15/ResNet解读/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/ResNet解读/" itemprop="url">
                  ResNet解读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-09-15T18:10:50+08:00">
              2016-09-15
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-02-15T18:11:39+08:00">
              2017-02-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/09/15/ResNet解读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/09/15/ResNet解读/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/09/15/ResNet解读/" class="leancloud_visitors" data-flag-title="ResNet解读">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ResNet-论文解读笔记"><a href="#ResNet-论文解读笔记" class="headerlink" title="ResNet 论文解读笔记"></a>ResNet 论文解读笔记</h1><p><code>Reference</code></p>
<p><a href="https://zhuanlan.zhihu.com/p/22447440" target="_blank" rel="external">机器之眼-Deep Residual Network 深度残差网络</a></p>
<p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">[1]Deep Residual Learning for Image Recognition</a></p>
<p><a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="external">[2]Identity Mappings in Deep Residual Networks</a></p>
<h1 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h1><p>深度卷积神经网络的表达能力通常随着深度的增加而增强，然而网络并非越深越好，一来是因为深度越深，对计算资源的要求越高，二是当网络的深度较深时，继续增加层数并不能提高性能，如下图所示，随着网络层数的增加，性能不仅没有提升，反而出现了退化。这不是因为过拟合而造成的，因为在训练数据上也出现了相同的结果。此外，在网络训练时也使用了ReLU激活，BN等手段，一定程度上也缓解了梯度消失。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet1.png" alt="ResNet1"></p>
<h1 id="深度残差学习"><a href="#深度残差学习" class="headerlink" title="深度残差学习"></a>深度残差学习</h1><h2 id="Deep-Residual-Learning"><a href="#Deep-Residual-Learning" class="headerlink" title="Deep Residual Learning"></a>Deep Residual Learning</h2><p>我们假设深度网络中某隐藏层为$H(X)-x \rightarrow{F(x)}$,如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也可以同样假设隐藏层的残差近似于某个复杂函数。即我们可以将隐藏层表示为$H(x)=F(x)+x$。</p>
<p>这样一来，我们可以得到一种全新的残差结构单元，如下图所示：<br><img src="http://oi8824myj.bkt.clouddn.com/ResNet2.png" alt="ResNet2"></p>
<p>可以看到，残差单元的输出由多个卷积层级联的输出和输入元素相加(保证卷积层的输出和输入的元素维度相同)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。论文中给出的18，34，50，101，152层网络如下表所示:</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet3.png" alt="ResNet3"></p>
<p>可以注意到残差网络有这样几个特点：</p>
<p>1.网络较瘦，控制了参数数量；</p>
<p>2.存在明显层级，特征图数逐层递进，保证输出特征表达能力；</p>
<p>3.使用了较少的池化层，大量使用下采样，提高传播效率；</p>
<p>4.没有使用Dropout,利用BN和全局平均池化进行正则化，加快了训练速度；</p>
<p>5.层数较高时减少了$3\times3$卷积个数，并用$1\times1$卷积控制了$3\times3$卷积的输入输出特征图数量，称这种结构为“瓶颈”(bottleneck)。</p>
<h2 id="收敛性能"><a href="#收敛性能" class="headerlink" title="收敛性能"></a>收敛性能</h2><p>在ImageNet上的性能，左边为“朴素”网络结构，右边为残差结构。</p>
<p>从图中可以看到，残差网络增加了一定层数后，并未出现性能退化，反而性能有了一定程度的提升：残差网络有着更低的收敛损失，同时也没有产生过高的过拟合。同时注意到，残差网络在浅层时并未表现出更多的优势，说明残差网络必须要配合较深的深度才能发挥其结构优势，与“平整”网络拉开性能差距。<br><img src="http://oi8824myj.bkt.clouddn.com/ResNet4.png" alt="ResNet4"></p>
<h2 id="分类性能"><a href="#分类性能" class="headerlink" title="分类性能"></a>分类性能</h2><p>下图是几种网络结构在ImageNet数据集上单模型和集成模型分类错误率的对比。可以发现，残差网络系由于层数普遍高于以上模型，且又有残差结构作为极深度的支持前提，使得其性能普遍高于此前的各类优秀模型。此外，残差网络的性能也确实如期望随着网络层数的提升而提升。在100层以上时已经远远甩开了IRSVRC 2014的冠亚军网络模型。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet5.png" alt="ResNet5"></p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet6.png" alt="ResNet6"></p>
<h2 id="网络响应"><a href="#网络响应" class="headerlink" title="网络响应"></a>网络响应</h2><p><img src="http://oi8824myj.bkt.clouddn.com/ResNet7.png" alt="ResNet7"><br>从图中可以看出，残差网络中大部分层的响应方差都处在较低水平，响应都是在BN层后面，ReLU函数前。这一定程度上印证了我们的假设：这些响应方差较低的层响应较为固定，很有可能权重近似于零，这也就是说其所对应的残差结构可能近似于单位映射,网络的实际有效层数是要比全部层数要少一些的，产生了跳过连接(Skip-connection)的作用。这样也就是网络为什么在较深的深度下仍可以保持并提升性能，且没有过多增加训练难度的原因。</p>
<h2 id="更深的网络结构"><a href="#更深的网络结构" class="headerlink" title="更深的网络结构"></a>更深的网络结构</h2><p>在CIFAR-10数据集上尝试更深的网络结构，层数为6n+2层，第一层为$3 \times 3$的卷积层，最后一层为全连接层和SoftMax.中间卷积层特征图的尺寸分别为{32,16,8},特征图的个数分别为{16,32,64},每种特征图的都有2n层的卷积。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet9.png" alt="ResNet9"></p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet8.png" alt="ResNet8"><br>何恺明等人经过一段时间的研究，认为极其深的深度网络可能深受梯度消失问题的困扰，BN、ReLU等手段对于这种程度的梯度消失缓解能力有限，并提出了单位映射的残差结构。这种结构从本源上杜绝了梯度消失的问题。</p>
<h1 id="Identity-Mappings-in-Deep-Residual-Networks"><a href="#Identity-Mappings-in-Deep-Residual-Networks" class="headerlink" title="Identity Mappings in Deep Residual Networks"></a>Identity Mappings in Deep Residual Networks</h1><p>上面主要介绍了残差单元，它的数学形式为：<br>$$<br>y_l=h(x_l)+F(x_l,W_l)<br>$$</p>
<p>$$<br>x_{l+1}=f(y_l)<br>$$<br>其中，$x_l$和$x_{l+1}$是第$l$个残差单元的输入和输出，$F$是一个残差函数。在论文[1]中，$h(x_l)=x_l$是一个单位映射, $f$是ReLU函数。</p>
<p>我们令$h(x_l)=x_l$, $x_{l+1}=y_l$，从而可以递推得到</p>
<p>$$<br>x_L=x_l+\sum_{i=l}^{L-1}F(x_i,w_i)<br>$$<br>也就是，第L个残差单元的输入可以表示为某一浅层残差单元的输入和其中间所有复杂映射之和。记损失函数为 $\epsilon $，从而我们计算反向传播得到:</p>
<p>$$<br>\frac{\partial{\epsilon}}{\partial{x_l}}=\frac{\partial{\epsilon}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}=\frac{\partial{\epsilon}}{\partial{x_L}}(1+\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}F(x_i,w_i))<br>$$<br>显然这里不存在网络结构级联所产生的连乘了(朴素的网络中$x_L$是由各级的矩阵和向量连乘得到的，忽略BN和ReLU函数，$\prod_{i=0}^{L-1}W_ix_0$)。$\frac{\partial}{\partial{x_l}}\sum_{i=l}^{L-1}F(x_i,w_i)$不可能每个batch的采样值都是-1，这也就是说梯度消失的本源已不存在。但需要注意的是，我们这里的前提条件$h(x_l)=x_l, x_{l+1}=y_l$是非常强的，一旦打破，如上的关系即不成立。其中$h(x_l)=x_l$是我们之前一直在遵守的条件，而且实验也证明单位映射的shortcut是最优的；$x_{l+1}=y_l$则要求我们去除隐藏层之间的激活，但必要的激活不可少，这需要对残差结构重新设计。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet10.png" alt="ResNet10"><br>实验证明，将激活层融合到残差支路中，并使用ReLU预激活的残差单元，不仅可以满足之前的假设，并且实验证明在各种已知的结构里也是最优的，预激活单元和原始残差单元的示意图如上图所示，可以看到，预激活的残差单元在残差支路中进行每次卷积之前即完成激活，然后再进行矩阵元素间加法进行合并，既满足了激活要求，也使得之路不再需要额外激活。</p>
<p><img src="http://oi8824myj.bkt.clouddn.com/ResNet11.png" alt="ResNet11"><br>可以看到，使用预激活残差单元，相较于使用原始单元更易收敛，且有一定正则化的效果，测试集上性能也普遍好于原始残差单元。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div><div class="line">1501</div><div class="line">1502</div><div class="line">1503</div><div class="line">1504</div><div class="line">1505</div><div class="line">1506</div><div class="line">1507</div><div class="line">1508</div><div class="line">1509</div><div class="line">1510</div><div class="line">1511</div><div class="line">1512</div><div class="line">1513</div><div class="line">1514</div><div class="line">1515</div><div class="line">1516</div><div class="line">1517</div><div class="line">1518</div><div class="line">1519</div><div class="line">1520</div><div class="line">1521</div><div class="line">1522</div><div class="line">1523</div><div class="line">1524</div><div class="line">1525</div><div class="line">1526</div><div class="line">1527</div><div class="line">1528</div><div class="line">1529</div><div class="line">1530</div><div class="line">1531</div><div class="line">1532</div><div class="line">1533</div><div class="line">1534</div><div class="line">1535</div><div class="line">1536</div><div class="line">1537</div><div class="line">1538</div><div class="line">1539</div><div class="line">1540</div><div class="line">1541</div><div class="line">1542</div><div class="line">1543</div><div class="line">1544</div><div class="line">1545</div><div class="line">1546</div><div class="line">1547</div><div class="line">1548</div><div class="line">1549</div><div class="line">1550</div><div class="line">1551</div><div class="line">1552</div><div class="line">1553</div><div class="line">1554</div><div class="line">1555</div><div class="line">1556</div><div class="line">1557</div><div class="line">1558</div><div class="line">1559</div><div class="line">1560</div><div class="line">1561</div><div class="line">1562</div><div class="line">1563</div><div class="line">1564</div><div class="line">1565</div><div class="line">1566</div><div class="line">1567</div><div class="line">1568</div><div class="line">1569</div><div class="line">1570</div><div class="line">1571</div><div class="line">1572</div><div class="line">1573</div><div class="line">1574</div><div class="line">1575</div><div class="line">1576</div><div class="line">1577</div><div class="line">1578</div><div class="line">1579</div><div class="line">1580</div><div class="line">1581</div><div class="line">1582</div><div class="line">1583</div><div class="line">1584</div><div class="line">1585</div><div class="line">1586</div><div class="line">1587</div><div class="line">1588</div><div class="line">1589</div><div class="line">1590</div><div class="line">1591</div><div class="line">1592</div><div class="line">1593</div><div class="line">1594</div><div class="line">1595</div><div class="line">1596</div><div class="line">1597</div><div class="line">1598</div><div class="line">1599</div><div class="line">1600</div><div class="line">1601</div><div class="line">1602</div><div class="line">1603</div><div class="line">1604</div><div class="line">1605</div><div class="line">1606</div><div class="line">1607</div><div class="line">1608</div><div class="line">1609</div><div class="line">1610</div><div class="line">1611</div><div class="line">1612</div><div class="line">1613</div><div class="line">1614</div><div class="line">1615</div><div class="line">1616</div><div class="line">1617</div><div class="line">1618</div><div class="line">1619</div><div class="line">1620</div><div class="line">1621</div><div class="line">1622</div><div class="line">1623</div><div class="line">1624</div><div class="line">1625</div><div class="line">1626</div><div class="line">1627</div><div class="line">1628</div><div class="line">1629</div><div class="line">1630</div><div class="line">1631</div><div class="line">1632</div><div class="line">1633</div><div class="line">1634</div><div class="line">1635</div><div class="line">1636</div><div class="line">1637</div><div class="line">1638</div><div class="line">1639</div><div class="line">1640</div><div class="line">1641</div><div class="line">1642</div><div class="line">1643</div><div class="line">1644</div><div class="line">1645</div><div class="line">1646</div><div class="line">1647</div><div class="line">1648</div><div class="line">1649</div><div class="line">1650</div><div class="line">1651</div><div class="line">1652</div><div class="line">1653</div><div class="line">1654</div><div class="line">1655</div><div class="line">1656</div><div class="line">1657</div><div class="line">1658</div><div class="line">1659</div><div class="line">1660</div><div class="line">1661</div><div class="line">1662</div><div class="line">1663</div><div class="line">1664</div><div class="line">1665</div><div class="line">1666</div><div class="line">1667</div><div class="line">1668</div><div class="line">1669</div><div class="line">1670</div><div class="line">1671</div><div class="line">1672</div><div class="line">1673</div><div class="line">1674</div><div class="line">1675</div><div class="line">1676</div><div class="line">1677</div><div class="line">1678</div><div class="line">1679</div><div class="line">1680</div><div class="line">1681</div><div class="line">1682</div><div class="line">1683</div><div class="line">1684</div><div class="line">1685</div><div class="line">1686</div><div class="line">1687</div><div class="line">1688</div><div class="line">1689</div><div class="line">1690</div><div class="line">1691</div><div class="line">1692</div><div class="line">1693</div><div class="line">1694</div><div class="line">1695</div><div class="line">1696</div><div class="line">1697</div><div class="line">1698</div><div class="line">1699</div><div class="line">1700</div><div class="line">1701</div><div class="line">1702</div><div class="line">1703</div><div class="line">1704</div><div class="line">1705</div><div class="line">1706</div><div class="line">1707</div><div class="line">1708</div><div class="line">1709</div><div class="line">1710</div><div class="line">1711</div><div class="line">1712</div><div class="line">1713</div><div class="line">1714</div><div class="line">1715</div><div class="line">1716</div><div class="line">1717</div><div class="line">1718</div><div class="line">1719</div><div class="line">1720</div><div class="line">1721</div><div class="line">1722</div><div class="line">1723</div><div class="line">1724</div><div class="line">1725</div><div class="line">1726</div><div class="line">1727</div><div class="line">1728</div><div class="line">1729</div><div class="line">1730</div><div class="line">1731</div><div class="line">1732</div><div class="line">1733</div><div class="line">1734</div><div class="line">1735</div><div class="line">1736</div><div class="line">1737</div><div class="line">1738</div><div class="line">1739</div><div class="line">1740</div><div class="line">1741</div><div class="line">1742</div><div class="line">1743</div><div class="line">1744</div><div class="line">1745</div><div class="line">1746</div><div class="line">1747</div><div class="line">1748</div><div class="line">1749</div><div class="line">1750</div><div class="line">1751</div><div class="line">1752</div><div class="line">1753</div><div class="line">1754</div><div class="line">1755</div><div class="line">1756</div><div class="line">1757</div><div class="line">1758</div><div class="line">1759</div><div class="line">1760</div><div class="line">1761</div><div class="line">1762</div><div class="line">1763</div><div class="line">1764</div><div class="line">1765</div><div class="line">1766</div><div class="line">1767</div><div class="line">1768</div><div class="line">1769</div><div class="line">1770</div><div class="line">1771</div><div class="line">1772</div><div class="line">1773</div><div class="line">1774</div><div class="line">1775</div><div class="line">1776</div><div class="line">1777</div><div class="line">1778</div><div class="line">1779</div><div class="line">1780</div><div class="line">1781</div><div class="line">1782</div><div class="line">1783</div><div class="line">1784</div><div class="line">1785</div><div class="line">1786</div><div class="line">1787</div><div class="line">1788</div><div class="line">1789</div><div class="line">1790</div><div class="line">1791</div><div class="line">1792</div><div class="line">1793</div><div class="line">1794</div><div class="line">1795</div><div class="line">1796</div><div class="line">1797</div><div class="line">1798</div><div class="line">1799</div><div class="line">1800</div><div class="line">1801</div><div class="line">1802</div><div class="line">1803</div><div class="line">1804</div><div class="line">1805</div><div class="line">1806</div><div class="line">1807</div><div class="line">1808</div><div class="line">1809</div><div class="line">1810</div><div class="line">1811</div><div class="line">1812</div><div class="line">1813</div><div class="line">1814</div><div class="line">1815</div><div class="line">1816</div><div class="line">1817</div><div class="line">1818</div><div class="line">1819</div><div class="line">1820</div><div class="line">1821</div><div class="line">1822</div><div class="line">1823</div><div class="line">1824</div><div class="line">1825</div><div class="line">1826</div><div class="line">1827</div><div class="line">1828</div><div class="line">1829</div><div class="line">1830</div><div class="line">1831</div><div class="line">1832</div><div class="line">1833</div><div class="line">1834</div><div class="line">1835</div><div class="line">1836</div><div class="line">1837</div><div class="line">1838</div><div class="line">1839</div><div class="line">1840</div><div class="line">1841</div><div class="line">1842</div><div class="line">1843</div><div class="line">1844</div><div class="line">1845</div><div class="line">1846</div><div class="line">1847</div><div class="line">1848</div><div class="line">1849</div><div class="line">1850</div><div class="line">1851</div><div class="line">1852</div><div class="line">1853</div><div class="line">1854</div><div class="line">1855</div><div class="line">1856</div><div class="line">1857</div><div class="line">1858</div><div class="line">1859</div><div class="line">1860</div><div class="line">1861</div><div class="line">1862</div><div class="line">1863</div><div class="line">1864</div><div class="line">1865</div><div class="line">1866</div><div class="line">1867</div><div class="line">1868</div><div class="line">1869</div><div class="line">1870</div><div class="line">1871</div><div class="line">1872</div><div class="line">1873</div><div class="line">1874</div><div class="line">1875</div><div class="line">1876</div><div class="line">1877</div><div class="line">1878</div><div class="line">1879</div><div class="line">1880</div><div class="line">1881</div><div class="line">1882</div><div class="line">1883</div><div class="line">1884</div><div class="line">1885</div><div class="line">1886</div><div class="line">1887</div><div class="line">1888</div><div class="line">1889</div><div class="line">1890</div><div class="line">1891</div><div class="line">1892</div><div class="line">1893</div><div class="line">1894</div><div class="line">1895</div><div class="line">1896</div><div class="line">1897</div><div class="line">1898</div><div class="line">1899</div><div class="line">1900</div><div class="line">1901</div><div class="line">1902</div><div class="line">1903</div><div class="line">1904</div><div class="line">1905</div><div class="line">1906</div><div class="line">1907</div><div class="line">1908</div><div class="line">1909</div><div class="line">1910</div><div class="line">1911</div><div class="line">1912</div><div class="line">1913</div><div class="line">1914</div><div class="line">1915</div><div class="line">1916</div><div class="line">1917</div><div class="line">1918</div><div class="line">1919</div><div class="line">1920</div><div class="line">1921</div><div class="line">1922</div><div class="line">1923</div><div class="line">1924</div><div class="line">1925</div><div class="line">1926</div><div class="line">1927</div><div class="line">1928</div><div class="line">1929</div><div class="line">1930</div><div class="line">1931</div><div class="line">1932</div><div class="line">1933</div><div class="line">1934</div><div class="line">1935</div><div class="line">1936</div><div class="line">1937</div><div class="line">1938</div><div class="line">1939</div><div class="line">1940</div><div class="line">1941</div><div class="line">1942</div><div class="line">1943</div><div class="line">1944</div><div class="line">1945</div><div class="line">1946</div><div class="line">1947</div><div class="line">1948</div><div class="line">1949</div><div class="line">1950</div><div class="line">1951</div><div class="line">1952</div><div class="line">1953</div><div class="line">1954</div><div class="line">1955</div><div class="line">1956</div><div class="line">1957</div><div class="line">1958</div><div class="line">1959</div><div class="line">1960</div><div class="line">1961</div><div class="line">1962</div><div class="line">1963</div><div class="line">1964</div><div class="line">1965</div><div class="line">1966</div><div class="line">1967</div><div class="line">1968</div><div class="line">1969</div><div class="line">1970</div><div class="line">1971</div><div class="line">1972</div><div class="line">1973</div><div class="line">1974</div><div class="line">1975</div><div class="line">1976</div><div class="line">1977</div><div class="line">1978</div><div class="line">1979</div><div class="line">1980</div><div class="line">1981</div><div class="line">1982</div><div class="line">1983</div><div class="line">1984</div><div class="line">1985</div><div class="line">1986</div><div class="line">1987</div><div class="line">1988</div><div class="line">1989</div><div class="line">1990</div><div class="line">1991</div><div class="line">1992</div><div class="line">1993</div><div class="line">1994</div><div class="line">1995</div><div class="line">1996</div><div class="line">1997</div><div class="line">1998</div><div class="line">1999</div><div class="line">2000</div><div class="line">2001</div><div class="line">2002</div><div class="line">2003</div><div class="line">2004</div><div class="line">2005</div><div class="line">2006</div><div class="line">2007</div><div class="line">2008</div><div class="line">2009</div><div class="line">2010</div><div class="line">2011</div><div class="line">2012</div><div class="line">2013</div><div class="line">2014</div><div class="line">2015</div><div class="line">2016</div><div class="line">2017</div><div class="line">2018</div><div class="line">2019</div><div class="line">2020</div><div class="line">2021</div><div class="line">2022</div><div class="line">2023</div><div class="line">2024</div><div class="line">2025</div><div class="line">2026</div><div class="line">2027</div><div class="line">2028</div><div class="line">2029</div><div class="line">2030</div><div class="line">2031</div><div class="line">2032</div><div class="line">2033</div><div class="line">2034</div><div class="line">2035</div><div class="line">2036</div><div class="line">2037</div><div class="line">2038</div><div class="line">2039</div><div class="line">2040</div><div class="line">2041</div><div class="line">2042</div><div class="line">2043</div><div class="line">2044</div><div class="line">2045</div><div class="line">2046</div><div class="line">2047</div><div class="line">2048</div><div class="line">2049</div><div class="line">2050</div><div class="line">2051</div><div class="line">2052</div><div class="line">2053</div><div class="line">2054</div><div class="line">2055</div><div class="line">2056</div><div class="line">2057</div><div class="line">2058</div><div class="line">2059</div><div class="line">2060</div><div class="line">2061</div><div class="line">2062</div><div class="line">2063</div><div class="line">2064</div><div class="line">2065</div><div class="line">2066</div><div class="line">2067</div><div class="line">2068</div><div class="line">2069</div><div class="line">2070</div><div class="line">2071</div><div class="line">2072</div><div class="line">2073</div><div class="line">2074</div><div class="line">2075</div><div class="line">2076</div><div class="line">2077</div><div class="line">2078</div><div class="line">2079</div><div class="line">2080</div><div class="line">2081</div><div class="line">2082</div><div class="line">2083</div><div class="line">2084</div><div class="line">2085</div><div class="line">2086</div><div class="line">2087</div><div class="line">2088</div><div class="line">2089</div><div class="line">2090</div><div class="line">2091</div><div class="line">2092</div><div class="line">2093</div><div class="line">2094</div><div class="line">2095</div><div class="line">2096</div><div class="line">2097</div><div class="line">2098</div><div class="line">2099</div><div class="line">2100</div><div class="line">2101</div><div class="line">2102</div><div class="line">2103</div><div class="line">2104</div><div class="line">2105</div><div class="line">2106</div><div class="line">2107</div><div class="line">2108</div><div class="line">2109</div><div class="line">2110</div><div class="line">2111</div><div class="line">2112</div><div class="line">2113</div><div class="line">2114</div><div class="line">2115</div><div class="line">2116</div><div class="line">2117</div><div class="line">2118</div><div class="line">2119</div><div class="line">2120</div><div class="line">2121</div><div class="line">2122</div><div class="line">2123</div><div class="line">2124</div><div class="line">2125</div><div class="line">2126</div><div class="line">2127</div><div class="line">2128</div><div class="line">2129</div><div class="line">2130</div><div class="line">2131</div><div class="line">2132</div><div class="line">2133</div><div class="line">2134</div><div class="line">2135</div><div class="line">2136</div><div class="line">2137</div><div class="line">2138</div><div class="line">2139</div><div class="line">2140</div><div class="line">2141</div><div class="line">2142</div><div class="line">2143</div><div class="line">2144</div><div class="line">2145</div><div class="line">2146</div><div class="line">2147</div><div class="line">2148</div><div class="line">2149</div><div class="line">2150</div><div class="line">2151</div><div class="line">2152</div><div class="line">2153</div><div class="line">2154</div><div class="line">2155</div><div class="line">2156</div><div class="line">2157</div><div class="line">2158</div><div class="line">2159</div><div class="line">2160</div><div class="line">2161</div><div class="line">2162</div><div class="line">2163</div><div class="line">2164</div><div class="line">2165</div><div class="line">2166</div><div class="line">2167</div><div class="line">2168</div><div class="line">2169</div><div class="line">2170</div><div class="line">2171</div><div class="line">2172</div><div class="line">2173</div><div class="line">2174</div><div class="line">2175</div><div class="line">2176</div><div class="line">2177</div><div class="line">2178</div><div class="line">2179</div><div class="line">2180</div><div class="line">2181</div><div class="line">2182</div><div class="line">2183</div><div class="line">2184</div><div class="line">2185</div><div class="line">2186</div><div class="line">2187</div><div class="line">2188</div><div class="line">2189</div><div class="line">2190</div><div class="line">2191</div><div class="line">2192</div><div class="line">2193</div><div class="line">2194</div><div class="line">2195</div><div class="line">2196</div><div class="line">2197</div><div class="line">2198</div><div class="line">2199</div><div class="line">2200</div><div class="line">2201</div><div class="line">2202</div><div class="line">2203</div><div class="line">2204</div><div class="line">2205</div><div class="line">2206</div><div class="line">2207</div><div class="line">2208</div><div class="line">2209</div><div class="line">2210</div><div class="line">2211</div><div class="line">2212</div><div class="line">2213</div><div class="line">2214</div><div class="line">2215</div><div class="line">2216</div><div class="line">2217</div><div class="line">2218</div><div class="line">2219</div><div class="line">2220</div><div class="line">2221</div><div class="line">2222</div><div class="line">2223</div><div class="line">2224</div><div class="line">2225</div><div class="line">2226</div><div class="line">2227</div><div class="line">2228</div><div class="line">2229</div><div class="line">2230</div><div class="line">2231</div><div class="line">2232</div><div class="line">2233</div><div class="line">2234</div><div class="line">2235</div><div class="line">2236</div><div class="line">2237</div><div class="line">2238</div><div class="line">2239</div><div class="line">2240</div><div class="line">2241</div><div class="line">2242</div><div class="line">2243</div><div class="line">2244</div><div class="line">2245</div><div class="line">2246</div><div class="line">2247</div><div class="line">2248</div><div class="line">2249</div><div class="line">2250</div><div class="line">2251</div><div class="line">2252</div><div class="line">2253</div><div class="line">2254</div><div class="line">2255</div><div class="line">2256</div><div class="line">2257</div><div class="line">2258</div><div class="line">2259</div><div class="line">2260</div><div class="line">2261</div><div class="line">2262</div><div class="line">2263</div><div class="line">2264</div><div class="line">2265</div><div class="line">2266</div><div class="line">2267</div><div class="line">2268</div><div class="line">2269</div><div class="line">2270</div><div class="line">2271</div><div class="line">2272</div><div class="line">2273</div><div class="line">2274</div><div class="line">2275</div><div class="line">2276</div><div class="line">2277</div><div class="line">2278</div><div class="line">2279</div><div class="line">2280</div><div class="line">2281</div><div class="line">2282</div><div class="line">2283</div><div class="line">2284</div><div class="line">2285</div><div class="line">2286</div><div class="line">2287</div><div class="line">2288</div><div class="line">2289</div><div class="line">2290</div><div class="line">2291</div><div class="line">2292</div><div class="line">2293</div><div class="line">2294</div><div class="line">2295</div><div class="line">2296</div><div class="line">2297</div><div class="line">2298</div><div class="line">2299</div><div class="line">2300</div><div class="line">2301</div><div class="line">2302</div><div class="line">2303</div><div class="line">2304</div><div class="line">2305</div><div class="line">2306</div><div class="line">2307</div><div class="line">2308</div><div class="line">2309</div><div class="line">2310</div><div class="line">2311</div><div class="line">2312</div><div class="line">2313</div><div class="line">2314</div><div class="line">2315</div><div class="line">2316</div><div class="line">2317</div><div class="line">2318</div><div class="line">2319</div><div class="line">2320</div><div class="line">2321</div><div class="line">2322</div><div class="line">2323</div><div class="line">2324</div><div class="line">2325</div><div class="line">2326</div><div class="line">2327</div><div class="line">2328</div><div class="line">2329</div><div class="line">2330</div><div class="line">2331</div><div class="line">2332</div><div class="line">2333</div><div class="line">2334</div><div class="line">2335</div><div class="line">2336</div><div class="line">2337</div><div class="line">2338</div><div class="line">2339</div><div class="line">2340</div><div class="line">2341</div><div class="line">2342</div><div class="line">2343</div><div class="line">2344</div><div class="line">2345</div><div class="line">2346</div><div class="line">2347</div><div class="line">2348</div><div class="line">2349</div><div class="line">2350</div><div class="line">2351</div><div class="line">2352</div><div class="line">2353</div><div class="line">2354</div><div class="line">2355</div><div class="line">2356</div><div class="line">2357</div><div class="line">2358</div><div class="line">2359</div><div class="line">2360</div><div class="line">2361</div><div class="line">2362</div><div class="line">2363</div><div class="line">2364</div><div class="line">2365</div><div class="line">2366</div><div class="line">2367</div><div class="line">2368</div><div class="line">2369</div><div class="line">2370</div><div class="line">2371</div><div class="line">2372</div><div class="line">2373</div><div class="line">2374</div><div class="line">2375</div><div class="line">2376</div><div class="line">2377</div><div class="line">2378</div><div class="line">2379</div><div class="line">2380</div><div class="line">2381</div><div class="line">2382</div><div class="line">2383</div><div class="line">2384</div><div class="line">2385</div><div class="line">2386</div><div class="line">2387</div><div class="line">2388</div><div class="line">2389</div><div class="line">2390</div><div class="line">2391</div><div class="line">2392</div><div class="line">2393</div><div class="line">2394</div><div class="line">2395</div><div class="line">2396</div><div class="line">2397</div><div class="line">2398</div><div class="line">2399</div><div class="line">2400</div><div class="line">2401</div><div class="line">2402</div><div class="line">2403</div><div class="line">2404</div><div class="line">2405</div><div class="line">2406</div><div class="line">2407</div><div class="line">2408</div><div class="line">2409</div><div class="line">2410</div><div class="line">2411</div><div class="line">2412</div><div class="line">2413</div><div class="line">2414</div><div class="line">2415</div><div class="line">2416</div><div class="line">2417</div><div class="line">2418</div><div class="line">2419</div><div class="line">2420</div><div class="line">2421</div><div class="line">2422</div><div class="line">2423</div><div class="line">2424</div><div class="line">2425</div><div class="line">2426</div><div class="line">2427</div><div class="line">2428</div><div class="line">2429</div><div class="line">2430</div><div class="line">2431</div><div class="line">2432</div><div class="line">2433</div><div class="line">2434</div><div class="line">2435</div><div class="line">2436</div><div class="line">2437</div><div class="line">2438</div><div class="line">2439</div><div class="line">2440</div><div class="line">2441</div><div class="line">2442</div><div class="line">2443</div><div class="line">2444</div><div class="line">2445</div><div class="line">2446</div><div class="line">2447</div><div class="line">2448</div><div class="line">2449</div><div class="line">2450</div><div class="line">2451</div><div class="line">2452</div><div class="line">2453</div><div class="line">2454</div><div class="line">2455</div><div class="line">2456</div><div class="line">2457</div><div class="line">2458</div><div class="line">2459</div><div class="line">2460</div><div class="line">2461</div><div class="line">2462</div><div class="line">2463</div><div class="line">2464</div><div class="line">2465</div><div class="line">2466</div><div class="line">2467</div><div class="line">2468</div><div class="line">2469</div><div class="line">2470</div><div class="line">2471</div><div class="line">2472</div><div class="line">2473</div><div class="line">2474</div><div class="line">2475</div><div class="line">2476</div><div class="line">2477</div><div class="line">2478</div><div class="line">2479</div><div class="line">2480</div><div class="line">2481</div><div class="line">2482</div><div class="line">2483</div><div class="line">2484</div><div class="line">2485</div><div class="line">2486</div><div class="line">2487</div><div class="line">2488</div><div class="line">2489</div><div class="line">2490</div><div class="line">2491</div><div class="line">2492</div><div class="line">2493</div><div class="line">2494</div><div class="line">2495</div><div class="line">2496</div><div class="line">2497</div><div class="line">2498</div><div class="line">2499</div><div class="line">2500</div><div class="line">2501</div><div class="line">2502</div><div class="line">2503</div><div class="line">2504</div><div class="line">2505</div><div class="line">2506</div><div class="line">2507</div><div class="line">2508</div><div class="line">2509</div><div class="line">2510</div><div class="line">2511</div><div class="line">2512</div><div class="line">2513</div><div class="line">2514</div><div class="line">2515</div><div class="line">2516</div><div class="line">2517</div><div class="line">2518</div><div class="line">2519</div><div class="line">2520</div><div class="line">2521</div><div class="line">2522</div><div class="line">2523</div><div class="line">2524</div><div class="line">2525</div><div class="line">2526</div><div class="line">2527</div><div class="line">2528</div><div class="line">2529</div><div class="line">2530</div><div class="line">2531</div><div class="line">2532</div><div class="line">2533</div><div class="line">2534</div><div class="line">2535</div><div class="line">2536</div><div class="line">2537</div><div class="line">2538</div><div class="line">2539</div><div class="line">2540</div><div class="line">2541</div><div class="line">2542</div><div class="line">2543</div><div class="line">2544</div><div class="line">2545</div><div class="line">2546</div><div class="line">2547</div><div class="line">2548</div><div class="line">2549</div><div class="line">2550</div><div class="line">2551</div><div class="line">2552</div><div class="line">2553</div><div class="line">2554</div><div class="line">2555</div><div class="line">2556</div><div class="line">2557</div><div class="line">2558</div><div class="line">2559</div><div class="line">2560</div><div class="line">2561</div><div class="line">2562</div><div class="line">2563</div><div class="line">2564</div><div class="line">2565</div><div class="line">2566</div><div class="line">2567</div><div class="line">2568</div><div class="line">2569</div><div class="line">2570</div><div class="line">2571</div><div class="line">2572</div><div class="line">2573</div><div class="line">2574</div><div class="line">2575</div><div class="line">2576</div><div class="line">2577</div><div class="line">2578</div><div class="line">2579</div><div class="line">2580</div><div class="line">2581</div><div class="line">2582</div><div class="line">2583</div><div class="line">2584</div><div class="line">2585</div><div class="line">2586</div><div class="line">2587</div><div class="line">2588</div><div class="line">2589</div><div class="line">2590</div><div class="line">2591</div><div class="line">2592</div><div class="line">2593</div><div class="line">2594</div><div class="line">2595</div><div class="line">2596</div><div class="line">2597</div><div class="line">2598</div><div class="line">2599</div><div class="line">2600</div><div class="line">2601</div><div class="line">2602</div><div class="line">2603</div><div class="line">2604</div><div class="line">2605</div><div class="line">2606</div><div class="line">2607</div><div class="line">2608</div><div class="line">2609</div><div class="line">2610</div><div class="line">2611</div><div class="line">2612</div><div class="line">2613</div><div class="line">2614</div><div class="line">2615</div><div class="line">2616</div><div class="line">2617</div><div class="line">2618</div><div class="line">2619</div><div class="line">2620</div><div class="line">2621</div><div class="line">2622</div><div class="line">2623</div><div class="line">2624</div><div class="line">2625</div><div class="line">2626</div><div class="line">2627</div><div class="line">2628</div><div class="line">2629</div><div class="line">2630</div><div class="line">2631</div><div class="line">2632</div><div class="line">2633</div><div class="line">2634</div><div class="line">2635</div><div class="line">2636</div><div class="line">2637</div><div class="line">2638</div><div class="line">2639</div><div class="line">2640</div><div class="line">2641</div><div class="line">2642</div><div class="line">2643</div><div class="line">2644</div><div class="line">2645</div><div class="line">2646</div><div class="line">2647</div><div class="line">2648</div><div class="line">2649</div><div class="line">2650</div><div class="line">2651</div><div class="line">2652</div><div class="line">2653</div><div class="line">2654</div><div class="line">2655</div><div class="line">2656</div><div class="line">2657</div><div class="line">2658</div><div class="line">2659</div><div class="line">2660</div></pre></td><td class="code"><pre><div class="line">name: <span class="string">"ResNet-50"</span></div><div class="line">layer &#123;</div><div class="line">    name: <span class="string">"data"</span></div><div class="line">    type: <span class="string">"ImageData"</span></div><div class="line">    top: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"label"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TRAIN</div><div class="line">    &#125;</div><div class="line">    transform_param &#123;</div><div class="line">        mirror: <span class="literal">true</span></div><div class="line">        #crop_size: <span class="number">224</span></div><div class="line">        mean_value: <span class="number">104</span></div><div class="line">        mean_value: <span class="number">117</span></div><div class="line">        mean_value: <span class="number">123</span></div><div class="line">    &#125;</div><div class="line">    image_data_param &#123;</div><div class="line">        source: <span class="string">"/home/satisfie/imagenet/train_new.txt"</span></div><div class="line">        batch_size: <span class="number">4</span></div><div class="line">        new_height: <span class="number">224</span></div><div class="line">        new_width: <span class="number">224</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">    name: <span class="string">"data"</span></div><div class="line">    type: <span class="string">"Data"</span></div><div class="line">    top: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"label"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">    transform_param &#123;</div><div class="line">        mirror: <span class="literal">false</span></div><div class="line">        #crop_size: <span class="number">224</span></div><div class="line">        mean_value: <span class="number">104</span></div><div class="line">        mean_value: <span class="number">117</span></div><div class="line">        mean_value: <span class="number">123</span></div><div class="line">    &#125;</div><div class="line">    image_data_param &#123;</div><div class="line">        source: <span class="string">"/home/satisfie/imagenet/val_new.txt"</span></div><div class="line">        batch_size: <span class="number">1</span></div><div class="line">        new_height: <span class="number">224</span></div><div class="line">        new_width: <span class="number">224</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"data"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"conv1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">7</span></div><div class="line">        pad: <span class="number">3</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"bn_conv1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"scale_conv1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"conv1"</span></div><div class="line">    name: <span class="string">"conv1_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"conv1"</span></div><div class="line">    top: <span class="string">"pool1"</span></div><div class="line">    name: <span class="string">"pool1"</span></div><div class="line">    type: <span class="string">"Pooling"</span></div><div class="line">    pooling_param &#123;</div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        pool: MAX</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"res2a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"bn2a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    top: <span class="string">"res2a_branch1"</span></div><div class="line">    name: <span class="string">"scale2a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool1"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"res2a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"bn2a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"scale2a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2a"</span></div><div class="line">    name: <span class="string">"res2a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2a"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"res2a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"bn2a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"scale2a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2b"</span></div><div class="line">    name: <span class="string">"res2a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2b"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"res2a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"bn2a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a_branch2c"</span></div><div class="line">    name: <span class="string">"scale2a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a_branch1"</span></div><div class="line">    bottom: <span class="string">"res2a_branch2c"</span></div><div class="line">    top: <span class="string">"res2a"</span></div><div class="line">    name: <span class="string">"res2a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    top: <span class="string">"res2a"</span></div><div class="line">    name: <span class="string">"res2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"res2b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"bn2b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"scale2b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2a"</span></div><div class="line">    name: <span class="string">"res2b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2a"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"res2b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"bn2b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"scale2b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2b"</span></div><div class="line">    name: <span class="string">"res2b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2b"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"res2b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"bn2b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b_branch2c"</span></div><div class="line">    name: <span class="string">"scale2b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2a"</span></div><div class="line">    bottom: <span class="string">"res2b_branch2c"</span></div><div class="line">    top: <span class="string">"res2b"</span></div><div class="line">    name: <span class="string">"res2b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    top: <span class="string">"res2b"</span></div><div class="line">    name: <span class="string">"res2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"res2c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"bn2c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"scale2c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2a"</span></div><div class="line">    name: <span class="string">"res2c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2a"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"res2c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">64</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"bn2c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"scale2c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2b"</span></div><div class="line">    name: <span class="string">"res2c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2b"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"res2c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"bn2c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c_branch2c"</span></div><div class="line">    name: <span class="string">"scale2c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2b"</span></div><div class="line">    bottom: <span class="string">"res2c_branch2c"</span></div><div class="line">    top: <span class="string">"res2c"</span></div><div class="line">    name: <span class="string">"res2c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res2c"</span></div><div class="line">    name: <span class="string">"res2c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"res3a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"bn3a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    top: <span class="string">"res3a_branch1"</span></div><div class="line">    name: <span class="string">"scale3a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"res3a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"bn3a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"scale3a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2a"</span></div><div class="line">    name: <span class="string">"res3a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2a"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"res3a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"bn3a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"scale3a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2b"</span></div><div class="line">    name: <span class="string">"res3a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2b"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"res3a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"bn3a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a_branch2c"</span></div><div class="line">    name: <span class="string">"scale3a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a_branch1"</span></div><div class="line">    bottom: <span class="string">"res3a_branch2c"</span></div><div class="line">    top: <span class="string">"res3a"</span></div><div class="line">    name: <span class="string">"res3a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    top: <span class="string">"res3a"</span></div><div class="line">    name: <span class="string">"res3a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"res3b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"bn3b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"scale3b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2a"</span></div><div class="line">    name: <span class="string">"res3b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2a"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"res3b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"bn3b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"scale3b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2b"</span></div><div class="line">    name: <span class="string">"res3b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2b"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"res3b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"bn3b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b_branch2c"</span></div><div class="line">    name: <span class="string">"scale3b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3a"</span></div><div class="line">    bottom: <span class="string">"res3b_branch2c"</span></div><div class="line">    top: <span class="string">"res3b"</span></div><div class="line">    name: <span class="string">"res3b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    top: <span class="string">"res3b"</span></div><div class="line">    name: <span class="string">"res3b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"res3c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"bn3c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"scale3c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2a"</span></div><div class="line">    name: <span class="string">"res3c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2a"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"res3c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"bn3c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"scale3c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2b"</span></div><div class="line">    name: <span class="string">"res3c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2b"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"res3c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"bn3c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c_branch2c"</span></div><div class="line">    name: <span class="string">"scale3c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3b"</span></div><div class="line">    bottom: <span class="string">"res3c_branch2c"</span></div><div class="line">    top: <span class="string">"res3c"</span></div><div class="line">    name: <span class="string">"res3c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    top: <span class="string">"res3c"</span></div><div class="line">    name: <span class="string">"res3c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"res3d_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"bn3d_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"scale3d_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2a"</span></div><div class="line">    name: <span class="string">"res3d_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2a"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"res3d_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">128</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"bn3d_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"scale3d_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2b"</span></div><div class="line">    name: <span class="string">"res3d_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2b"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"res3d_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"bn3d_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d_branch2c"</span></div><div class="line">    name: <span class="string">"scale3d_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3c"</span></div><div class="line">    bottom: <span class="string">"res3d_branch2c"</span></div><div class="line">    top: <span class="string">"res3d"</span></div><div class="line">    name: <span class="string">"res3d"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res3d"</span></div><div class="line">    name: <span class="string">"res3d_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"res4a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"bn4a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    top: <span class="string">"res4a_branch1"</span></div><div class="line">    name: <span class="string">"scale4a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res3d"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"res4a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"bn4a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"scale4a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2a"</span></div><div class="line">    name: <span class="string">"res4a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2a"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"res4a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"bn4a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"scale4a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2b"</span></div><div class="line">    name: <span class="string">"res4a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2b"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"res4a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"bn4a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a_branch2c"</span></div><div class="line">    name: <span class="string">"scale4a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a_branch1"</span></div><div class="line">    bottom: <span class="string">"res4a_branch2c"</span></div><div class="line">    top: <span class="string">"res4a"</span></div><div class="line">    name: <span class="string">"res4a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    top: <span class="string">"res4a"</span></div><div class="line">    name: <span class="string">"res4a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"res4b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"bn4b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"scale4b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2a"</span></div><div class="line">    name: <span class="string">"res4b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2a"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"res4b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"bn4b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"scale4b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2b"</span></div><div class="line">    name: <span class="string">"res4b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2b"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"res4b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"bn4b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b_branch2c"</span></div><div class="line">    name: <span class="string">"scale4b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4a"</span></div><div class="line">    bottom: <span class="string">"res4b_branch2c"</span></div><div class="line">    top: <span class="string">"res4b"</span></div><div class="line">    name: <span class="string">"res4b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    top: <span class="string">"res4b"</span></div><div class="line">    name: <span class="string">"res4b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"res4c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"bn4c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"scale4c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2a"</span></div><div class="line">    name: <span class="string">"res4c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2a"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"res4c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"bn4c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"scale4c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2b"</span></div><div class="line">    name: <span class="string">"res4c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2b"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"res4c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"bn4c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c_branch2c"</span></div><div class="line">    name: <span class="string">"scale4c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4b"</span></div><div class="line">    bottom: <span class="string">"res4c_branch2c"</span></div><div class="line">    top: <span class="string">"res4c"</span></div><div class="line">    name: <span class="string">"res4c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    top: <span class="string">"res4c"</span></div><div class="line">    name: <span class="string">"res4c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"res4d_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"bn4d_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"scale4d_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2a"</span></div><div class="line">    name: <span class="string">"res4d_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2a"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"res4d_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"bn4d_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"scale4d_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2b"</span></div><div class="line">    name: <span class="string">"res4d_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2b"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"res4d_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"bn4d_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d_branch2c"</span></div><div class="line">    name: <span class="string">"scale4d_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4c"</span></div><div class="line">    bottom: <span class="string">"res4d_branch2c"</span></div><div class="line">    top: <span class="string">"res4d"</span></div><div class="line">    name: <span class="string">"res4d"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    top: <span class="string">"res4d"</span></div><div class="line">    name: <span class="string">"res4d_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"res4e_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"bn4e_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"scale4e_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2a"</span></div><div class="line">    name: <span class="string">"res4e_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2a"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"res4e_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"bn4e_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"scale4e_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2b"</span></div><div class="line">    name: <span class="string">"res4e_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2b"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"res4e_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"bn4e_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e_branch2c"</span></div><div class="line">    name: <span class="string">"scale4e_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4d"</span></div><div class="line">    bottom: <span class="string">"res4e_branch2c"</span></div><div class="line">    top: <span class="string">"res4e"</span></div><div class="line">    name: <span class="string">"res4e"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    top: <span class="string">"res4e"</span></div><div class="line">    name: <span class="string">"res4e_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"res4f_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"bn4f_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"scale4f_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2a"</span></div><div class="line">    name: <span class="string">"res4f_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2a"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"res4f_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">256</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"bn4f_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"scale4f_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2b"</span></div><div class="line">    name: <span class="string">"res4f_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2b"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"res4f_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">1024</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"bn4f_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f_branch2c"</span></div><div class="line">    name: <span class="string">"scale4f_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4e"</span></div><div class="line">    bottom: <span class="string">"res4f_branch2c"</span></div><div class="line">    top: <span class="string">"res4f"</span></div><div class="line">    name: <span class="string">"res4f"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res4f"</span></div><div class="line">    name: <span class="string">"res4f_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"res5a_branch1"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"bn5a_branch1"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    top: <span class="string">"res5a_branch1"</span></div><div class="line">    name: <span class="string">"scale5a_branch1"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res4f"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"res5a_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">2</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"bn5a_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"scale5a_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2a"</span></div><div class="line">    name: <span class="string">"res5a_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2a"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"res5a_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"bn5a_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"scale5a_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2b"</span></div><div class="line">    name: <span class="string">"res5a_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2b"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"res5a_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"bn5a_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a_branch2c"</span></div><div class="line">    name: <span class="string">"scale5a_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a_branch1"</span></div><div class="line">    bottom: <span class="string">"res5a_branch2c"</span></div><div class="line">    top: <span class="string">"res5a"</span></div><div class="line">    name: <span class="string">"res5a"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    top: <span class="string">"res5a"</span></div><div class="line">    name: <span class="string">"res5a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"res5b_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"bn5b_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"scale5b_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2a"</span></div><div class="line">    name: <span class="string">"res5b_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2a"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"res5b_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"bn5b_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"scale5b_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2b"</span></div><div class="line">    name: <span class="string">"res5b_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2b"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"res5b_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"bn5b_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b_branch2c"</span></div><div class="line">    name: <span class="string">"scale5b_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5a"</span></div><div class="line">    bottom: <span class="string">"res5b_branch2c"</span></div><div class="line">    top: <span class="string">"res5b"</span></div><div class="line">    name: <span class="string">"res5b"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    top: <span class="string">"res5b"</span></div><div class="line">    name: <span class="string">"res5b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"res5c_branch2a"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"bn5c_branch2a"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"scale5c_branch2a"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2a"</span></div><div class="line">    name: <span class="string">"res5c_branch2a_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2a"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"res5c_branch2b"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">512</span></div><div class="line">        kernel_size: <span class="number">3</span></div><div class="line">        pad: <span class="number">1</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"bn5c_branch2b"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"scale5c_branch2b"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2b"</span></div><div class="line">    name: <span class="string">"res5c_branch2b_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2b"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"res5c_branch2c"</span></div><div class="line">    type: <span class="string">"Convolution"</span></div><div class="line">    convolution_param &#123;</div><div class="line">        num_output: <span class="number">2048</span></div><div class="line">        kernel_size: <span class="number">1</span></div><div class="line">        pad: <span class="number">0</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"msra"</span></div><div class="line">        &#125;</div><div class="line">        bias_term: <span class="literal">false</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"bn5c_branch2c"</span></div><div class="line">    type: <span class="string">"BatchNorm"</span></div><div class="line">    batch_norm_param &#123;</div><div class="line">        use_global_stats: <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c_branch2c"</span></div><div class="line">    name: <span class="string">"scale5c_branch2c"</span></div><div class="line">    type: <span class="string">"Scale"</span></div><div class="line">    scale_param &#123;</div><div class="line">        bias_term: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5b"</span></div><div class="line">    bottom: <span class="string">"res5c_branch2c"</span></div><div class="line">    top: <span class="string">"res5c"</span></div><div class="line">    name: <span class="string">"res5c"</span></div><div class="line">    type: <span class="string">"Eltwise"</span></div><div class="line">    eltwise_param &#123;</div><div class="line">        operation: SUM</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c"</span></div><div class="line">    top: <span class="string">"res5c"</span></div><div class="line">    name: <span class="string">"res5c_relu"</span></div><div class="line">    type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"res5c"</span></div><div class="line">    top: <span class="string">"pool5"</span></div><div class="line">    name: <span class="string">"pool5"</span></div><div class="line">    type: <span class="string">"Pooling"</span></div><div class="line">    pooling_param &#123;</div><div class="line">        kernel_size: <span class="number">7</span></div><div class="line">        stride: <span class="number">1</span></div><div class="line">        pool: AVE</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"pool5"</span></div><div class="line">    top: <span class="string">"fc1000"</span></div><div class="line">    name: <span class="string">"fc1000"</span></div><div class="line">    type: <span class="string">"InnerProduct"</span></div><div class="line">    param &#123;</div><div class="line">        lr_mult: <span class="number">1</span></div><div class="line">        decay_mult: <span class="number">1</span></div><div class="line">    &#125;</div><div class="line">    param &#123;</div><div class="line">        lr_mult: <span class="number">2</span></div><div class="line">        decay_mult: <span class="number">1</span></div><div class="line">    &#125;</div><div class="line">    inner_product_param &#123;</div><div class="line">        num_output: <span class="number">1000</span></div><div class="line">        weight_filler &#123;</div><div class="line">            type: <span class="string">"xavier"</span></div><div class="line">        &#125;</div><div class="line">        bias_filler &#123;</div><div class="line">            type: <span class="string">"constant"</span></div><div class="line">            value: <span class="number">0</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    name: <span class="string">"loss"</span></div><div class="line">    type: <span class="string">"SoftmaxWithLoss"</span></div><div class="line">    top: <span class="string">"loss"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    top: <span class="string">"acc/top-1"</span></div><div class="line">    name: <span class="string">"acc/top-1"</span></div><div class="line">    type: <span class="string">"Accuracy"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">layer &#123;</div><div class="line">    bottom: <span class="string">"fc1000"</span></div><div class="line">    bottom: <span class="string">"label"</span></div><div class="line">    top: <span class="string">"acc/top-5"</span></div><div class="line">    name: <span class="string">"acc/top-5"</span></div><div class="line">    type: <span class="string">"Accuracy"</span></div><div class="line">    include &#123;</div><div class="line">        phase: TEST</div><div class="line">    &#125;</div><div class="line">    accuracy_param &#123;</div><div class="line">        top_k: <span class="number">5</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/19/VGG训练ImageNet/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/19/VGG训练ImageNet/" itemprop="url">
                  VGG训练ImageNet
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-08-19T19:32:07+08:00">
              2016-08-19
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-02-15T18:08:45+08:00">
              2017-02-15
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/19/VGG训练ImageNet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/08/19/VGG训练ImageNet/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/08/19/VGG训练ImageNet/" class="leancloud_visitors" data-flag-title="VGG训练ImageNet">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>以前的笔记，放上来吧</p>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><p>具体官网地址，请点击这里</p>
<p><a href="http://www.image-net.org/signup.php?next=download-images" target="_blank" rel="external">ImageNet官网</a></p>
<p>训练数据集：<a href="http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar" target="_blank" rel="external">ILSVRC2012_img_train.tar</a></p>
<p>验证数据集：<a href="http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar" target="_blank" rel="external">ILSVRC2012_img_val.tar</a></p>
<h2 id="数据解压"><a href="#数据解压" class="headerlink" title="数据解压"></a>数据解压</h2><p><code>sudo tar –xvf  ILSVRC2012_img_train.tar -C ./train</code><br><code>sudo tar –xvf ILSVRC2012_img_val.tar -C ./val</code></p>
<p>对于<code>val</code>数据集，解压以后是所有的验证集图片，共50000张，大约6.3G。</p>
<p>对于<code>train</code>数据集，解压后是1000个tar文件，每个tar文件表示1000类里的一个类，共138G，对于1000个子tar，需要再次解压，解压脚本<code>unzip.sh</code>如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dir=/home/satisfie/imagenet/train <span class="comment">#satisfie 是我的用户名</span></div><div class="line"><span class="keyword">for</span> x <span class="keyword">in</span> `ls *.tar`</div><div class="line"><span class="keyword">do</span></div><div class="line">	filename=`basename <span class="variable">$x</span> .tar`   <span class="comment">#注意空格</span></div><div class="line">	mkdir <span class="variable">$filename</span></div><div class="line">	tar -xvf <span class="variable">$x</span> -C ./<span class="variable">$filename</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure>
<p>i7 6700K配合我的500G固态硬盘解压超快，到这原始数据就准备好了，分别放在</p>
<ul>
<li><p><code>/home/satisfie/imagenet/train</code>：里面有1000个文件夹，每个文件夹下为JPG图片</p>
</li>
<li><p><code>/home/satisfie/imagenet/val</code> ：里面有验证集的50000张图片</p>
</li>
</ul>
<p>接下来下载标签等其他说明数据~~~</p>
<h2 id="下载其他数据"><a href="#下载其他数据" class="headerlink" title="下载其他数据"></a>下载其他数据</h2><p>进入大<code>caffe</code>根目录，执行<code>/data/ilsvrc12/get_ilsvrc_aux.sh</code>下载其他数据，包括</p>
<ul>
<li><code>det_synset_words.txt</code></li>
<li><code>synset_words.txt</code>— 1000个类别的文件夹名称及真是物体的名称，比如 “n01440764 tench Tinca tinca”,在训练中，这些都当做一个类别。</li>
<li><code>synsets.txt</code>   — 1000个类别的文件夹名称,比如”n01440764”…</li>
<li><code>train.txt</code> — 1000个类别每张图片的名字及其标签，比如 “n01440764/n01440764_10026.JPEG  0”  共有1281167张图片</li>
<li><code>val.txt</code>   — 同上，总共有50000张。比如“ILSVRC2012_val_00000001.JPEG 65”</li>
<li><code>test.txt</code>  — 同上，为测试集合,总有100000张</li>
<li><code>imagenet_mean.binaryproto</code> — 模型图片的各个通道均值</li>
<li><code>imagenet.bet.pickle</code></li>
</ul>
<h1 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h1><h2 id="训练数据准备"><a href="#训练数据准备" class="headerlink" title="训练数据准备"></a>训练数据准备</h2><p>由于转化为lmdb数据库格式需要耗费较大的空间，且不支持shuffle等操作，所以这里直接读取原图片，使用的类型是<code>ImageData</code>，具体看下面的prototxt</p>
<p>其中的<code>train_new.txt</code>中对每张图片的加上了绝对值路径，这样才能被读取。<br>使用<code>sed</code>命令即可，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sed <span class="string">'s/^/\/home\/satisfie\/imagenet\/val\/&amp;/g'</span> val.txt &gt;val_new.txt</div></pre></td></tr></table></figure>
<h2 id="VGG-train-val-prototxt"><a href="#VGG-train-val-prototxt" class="headerlink" title="VGG_train_val.prototxt"></a>VGG_train_val.prototxt</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div></pre></td><td class="code"><pre><div class="line">name: <span class="string">"VGG_ILSVRC_16_layers"</span></div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"data"</span></div><div class="line">  type: <span class="string">"ImageData"</span></div><div class="line">  include &#123;</div><div class="line">    phase: TRAIN</div><div class="line">  &#125;</div><div class="line"> transform_param &#123;</div><div class="line">    #crop_size: <span class="number">224</span></div><div class="line">    mean_value: <span class="number">104</span></div><div class="line">    mean_value: <span class="number">117</span></div><div class="line">    mean_value: <span class="number">123</span></div><div class="line">    mirror: <span class="literal">true</span></div><div class="line"> &#125;</div><div class="line"> image_data_param &#123;</div><div class="line">    source: <span class="string">"/home/satisfie/imagenet/train_new.txt"</span></div><div class="line">    batch_size: <span class="number">8</span></div><div class="line">    new_height: <span class="number">224</span></div><div class="line">    new_width: <span class="number">224</span></div><div class="line">  &#125;</div><div class="line">  top: <span class="string">"data"</span></div><div class="line">  top: <span class="string">"label"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"data"</span></div><div class="line">  type: <span class="string">"ImageData"</span></div><div class="line">  include &#123;</div><div class="line">    phase: TEST</div><div class="line">  &#125;</div><div class="line"> transform_param &#123;</div><div class="line">    #crop_size: <span class="number">224</span></div><div class="line">    mean_value: <span class="number">104</span></div><div class="line">    mean_value: <span class="number">117</span></div><div class="line">    mean_value: <span class="number">123</span></div><div class="line">    mirror: <span class="literal">false</span></div><div class="line"> &#125;</div><div class="line"> image_data_param &#123;</div><div class="line">    source: <span class="string">"/home/satisfie/imagenet/val_new.txt"</span></div><div class="line">    batch_size: <span class="number">4</span></div><div class="line">    new_height: <span class="number">224</span></div><div class="line">    new_width: <span class="number">224</span></div><div class="line">  &#125;</div><div class="line">  top: <span class="string">"data"</span></div><div class="line">  top: <span class="string">"label"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"data"</span></div><div class="line">  top: <span class="string">"conv1_1"</span></div><div class="line">  name: <span class="string">"conv1_1"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">64</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv1_1"</span></div><div class="line">  top: <span class="string">"conv1_1"</span></div><div class="line">  name: <span class="string">"relu1_1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv1_1"</span></div><div class="line">  top: <span class="string">"conv1_2"</span></div><div class="line">  name: <span class="string">"conv1_2"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">64</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv1_2"</span></div><div class="line">  top: <span class="string">"conv1_2"</span></div><div class="line">  name: <span class="string">"relu1_2"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv1_2"</span></div><div class="line">  top: <span class="string">"pool1"</span></div><div class="line">  name: <span class="string">"pool1"</span></div><div class="line">  type: <span class="string">"Pooling"</span></div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: <span class="number">2</span></div><div class="line">    stride: <span class="number">2</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"pool1"</span></div><div class="line">  top: <span class="string">"conv2_1"</span></div><div class="line">  name: <span class="string">"conv2_1"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">128</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv2_1"</span></div><div class="line">  top: <span class="string">"conv2_1"</span></div><div class="line">  name: <span class="string">"relu2_1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv2_1"</span></div><div class="line">  top: <span class="string">"conv2_2"</span></div><div class="line">  name: <span class="string">"conv2_2"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">128</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv2_2"</span></div><div class="line">  top: <span class="string">"conv2_2"</span></div><div class="line">  name: <span class="string">"relu2_2"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv2_2"</span></div><div class="line">  top: <span class="string">"pool2"</span></div><div class="line">  name: <span class="string">"pool2"</span></div><div class="line">  type: <span class="string">"Pooling"</span></div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: <span class="number">2</span></div><div class="line">    stride: <span class="number">2</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"pool2"</span></div><div class="line">  top: <span class="string">"conv3_1"</span></div><div class="line">  name: <span class="string">"conv3_1"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">256</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_1"</span></div><div class="line">  top: <span class="string">"conv3_1"</span></div><div class="line">  name: <span class="string">"relu3_1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_1"</span></div><div class="line">  top: <span class="string">"conv3_2"</span></div><div class="line">  name: <span class="string">"conv3_2"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">256</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_2"</span></div><div class="line">  top: <span class="string">"conv3_2"</span></div><div class="line">  name: <span class="string">"relu3_2"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_2"</span></div><div class="line">  top: <span class="string">"conv3_3"</span></div><div class="line">  name: <span class="string">"conv3_3"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">256</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_3"</span></div><div class="line">  top: <span class="string">"conv3_3"</span></div><div class="line">  name: <span class="string">"relu3_3"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv3_3"</span></div><div class="line">  top: <span class="string">"pool3"</span></div><div class="line">  name: <span class="string">"pool3"</span></div><div class="line">  type: <span class="string">"Pooling"</span></div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: <span class="number">2</span></div><div class="line">    stride: <span class="number">2</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"pool3"</span></div><div class="line">  top: <span class="string">"conv4_1"</span></div><div class="line">  name: <span class="string">"conv4_1"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_1"</span></div><div class="line">  top: <span class="string">"conv4_1"</span></div><div class="line">  name: <span class="string">"relu4_1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_1"</span></div><div class="line">  top: <span class="string">"conv4_2"</span></div><div class="line">  name: <span class="string">"conv4_2"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_2"</span></div><div class="line">  top: <span class="string">"conv4_2"</span></div><div class="line">  name: <span class="string">"relu4_2"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_2"</span></div><div class="line">  top: <span class="string">"conv4_3"</span></div><div class="line">  name: <span class="string">"conv4_3"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_3"</span></div><div class="line">  top: <span class="string">"conv4_3"</span></div><div class="line">  name: <span class="string">"relu4_3"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv4_3"</span></div><div class="line">  top: <span class="string">"pool4"</span></div><div class="line">  name: <span class="string">"pool4"</span></div><div class="line">  type: <span class="string">"Pooling"</span></div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: <span class="number">2</span></div><div class="line">    stride: <span class="number">2</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"pool4"</span></div><div class="line">  top: <span class="string">"conv5_1"</span></div><div class="line">  name: <span class="string">"conv5_1"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_1"</span></div><div class="line">  top: <span class="string">"conv5_1"</span></div><div class="line">  name: <span class="string">"relu5_1"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_1"</span></div><div class="line">  top: <span class="string">"conv5_2"</span></div><div class="line">  name: <span class="string">"conv5_2"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_2"</span></div><div class="line">  top: <span class="string">"conv5_2"</span></div><div class="line">  name: <span class="string">"relu5_2"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_2"</span></div><div class="line">  top: <span class="string">"conv5_3"</span></div><div class="line">  name: <span class="string">"conv5_3"</span></div><div class="line">  type: <span class="string">"Convolution"</span></div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: <span class="number">512</span></div><div class="line">    pad: <span class="number">1</span></div><div class="line">    kernel_size: <span class="number">3</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_3"</span></div><div class="line">  top: <span class="string">"conv5_3"</span></div><div class="line">  name: <span class="string">"relu5_3"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"conv5_3"</span></div><div class="line">  top: <span class="string">"pool5"</span></div><div class="line">  name: <span class="string">"pool5"</span></div><div class="line">  type: <span class="string">"Pooling"</span></div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: <span class="number">2</span></div><div class="line">    stride: <span class="number">2</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"pool5"</span></div><div class="line">  top: <span class="string">"fc6"</span></div><div class="line">  name: <span class="string">"fc6"</span></div><div class="line">  type: <span class="string">"InnerProduct"</span></div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: <span class="number">4096</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"fc6"</span></div><div class="line">  top: <span class="string">"fc6"</span></div><div class="line">  name: <span class="string">"relu6"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"fc6"</span></div><div class="line">  top: <span class="string">"fc6"</span></div><div class="line">  name: <span class="string">"drop6"</span></div><div class="line">  type: <span class="string">"Dropout"</span></div><div class="line">  dropout_param &#123;</div><div class="line">    dropout_ratio: <span class="number">0.5</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"fc6"</span></div><div class="line">  top: <span class="string">"fc7"</span></div><div class="line">  name: <span class="string">"fc7"</span></div><div class="line">  type: <span class="string">"InnerProduct"</span></div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: <span class="number">4096</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"fc7"</span></div><div class="line">  top: <span class="string">"fc7"</span></div><div class="line">  name: <span class="string">"relu7"</span></div><div class="line">  type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  bottom: <span class="string">"fc7"</span></div><div class="line">  top: <span class="string">"fc7"</span></div><div class="line">  name: <span class="string">"drop7"</span></div><div class="line">  type: <span class="string">"Dropout"</span></div><div class="line">  dropout_param &#123;</div><div class="line">    dropout_ratio: <span class="number">0.5</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"fc8"</span></div><div class="line">  bottom: <span class="string">"fc7"</span></div><div class="line">  top: <span class="string">"fc8"</span></div><div class="line">  type: <span class="string">"InnerProduct"</span></div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: <span class="number">1000</span></div><div class="line">    weight_filler &#123;</div><div class="line">      type: <span class="string">"xavier"</span></div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: <span class="string">"constant"</span></div><div class="line">      value: <span class="number">0</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">1</span></div><div class="line">    decay_mult :<span class="number">1</span></div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: <span class="number">2</span></div><div class="line">    decay_mult: <span class="number">0</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"loss"</span></div><div class="line">  type: <span class="string">"SoftmaxWithLoss"</span></div><div class="line">  bottom: <span class="string">"fc8"</span></div><div class="line">  bottom: <span class="string">"label"</span></div><div class="line">  top: <span class="string">"loss/loss"</span></div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"accuracy/top1"</span></div><div class="line">  type: <span class="string">"Accuracy"</span></div><div class="line">  bottom: <span class="string">"fc8"</span></div><div class="line">  bottom: <span class="string">"label"</span></div><div class="line">  top: <span class="string">"accuracy@1"</span></div><div class="line">  include: &#123; phase: TEST &#125;</div><div class="line">  accuracy_param &#123;</div><div class="line">    top_k: <span class="number">1</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"accuracy/top5"</span></div><div class="line">  type: <span class="string">"Accuracy"</span></div><div class="line">  bottom: <span class="string">"fc8"</span></div><div class="line">  bottom: <span class="string">"label"</span></div><div class="line">  top: <span class="string">"accuracy@5"</span></div><div class="line">  include: &#123; phase: TEST &#125;</div><div class="line">  accuracy_param &#123;</div><div class="line">    top_k: <span class="number">5</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="solver-prototxt"><a href="#solver-prototxt" class="headerlink" title="solver.prototxt"></a>solver.prototxt</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">net: <span class="string">"models/vgg/train_val.prototxt"</span></div><div class="line">test_iter: <span class="number">10000</span></div><div class="line">test_interval: <span class="number">40000</span></div><div class="line">test_initialization: <span class="literal">false</span></div><div class="line">display: <span class="number">200</span></div><div class="line">base_lr: <span class="number">0.0001</span></div><div class="line">lr_policy: <span class="string">"step"</span></div><div class="line">stepsize: <span class="number">320000</span></div><div class="line">gamma: <span class="number">0.96</span></div><div class="line">max_iter: <span class="number">10000000</span></div><div class="line">momentum: <span class="number">0.9</span></div><div class="line">weight_decay: <span class="number">0.0005</span></div><div class="line">snapshot: <span class="number">800000</span></div><div class="line">snapshot_prefix: <span class="string">"models/vgg/vgg"</span></div><div class="line">solver_mode: GPU</div></pre></td></tr></table></figure>
<h2 id="finetuning"><a href="#finetuning" class="headerlink" title="finetuning"></a>finetuning</h2><p>模型太大，试了下，在GTX980的4G显存下，batchsize只能设置为8或者16这么小。。。<br>大模型还是得服务器并行，直接在原有的模型上finetuning</p>
<p><a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md" target="_blank" rel="external">VGG_ILSVRC_16_layers_deploy.prototxt</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/usr/bin/env sh</span></div><div class="line"><span class="built_in">set</span> <span class="_">-e</span></div><div class="line"></div><div class="line">TOOLS=./build/tools</div><div class="line"></div><div class="line">GLOG_logtostderr=0 GLOG_log_dir=models/vgg/Log/ \</div><div class="line"><span class="variable">$TOOLS</span>/caffe train \</div><div class="line">    --solver=models/vgg/solver.prototxt \</div><div class="line">    --weights models/vgg/VGG_ILSVRC_16_layers.caffemodel</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/10/Caffe解读6-Net/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/10/Caffe解读6-Net/" itemprop="url">
                  Caffe解读6 -- Net
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-07-10T12:56:33+08:00">
              2016-07-10
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-08T16:59:52+08:00">
              2017-03-08
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/10/Caffe解读6-Net/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/07/10/Caffe解读6-Net/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/07/10/Caffe解读6-Net/" class="leancloud_visitors" data-flag-title="Caffe解读6 -- Net">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><code>Net</code>将各个层组成了一个无回路有向图。在解读<code>Net</code>的过程中，同样需要先看一下其在<code>caffe.proto</code>中的定义<code>message NetParameter</code>以及对应生成的hpp和cpp。然后主要是<code>net.hpp</code>和<code>net.cpp</code>。</p>
<h1 id="message-NetParameter"><a href="#message-NetParameter" class="headerlink" title="message NetParameter"></a><code>message NetParameter</code></h1><ul>
<li><code>optional string name:</code> 网络对应的名字</li>
<li><code>repeated string input:</code> 网络输入的blobs</li>
<li><code>repeated BlobShape input_shape:</code>网络输入blobs对应的shape</li>
<li><code>repeated int32 input_dim:</code>输入维度</li>
<li><code>optional bool force_backward:</code>是否强制回传，默认为false,根据网络结构和学习率进行回传</li>
<li><p><code>optional NetState state:</code> 包括网络的<code>phase</code>,<code>level</code>和<code>stage</code></p>
  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">message NetState &#123;</div><div class="line">	optional Phase phase = <span class="number">1</span> [<span class="keyword">default</span> = TEST];</div><div class="line"> 	optional int32 level = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">0</span>];</div><div class="line"> 	repeated <span class="built_in">string</span> stage = <span class="number">3</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p><code>optional bool debug_info:</code>决定是否打印debug信息</p>
</li>
<li><code>repeated LayerParameter layer:</code>定义网络中每层的参数</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">message NetParameter &#123;</div><div class="line">  optional <span class="built_in">string</span> name = <span class="number">1</span>; <span class="comment">// consider giving the network a name</span></div><div class="line">  <span class="comment">// DEPRECATED. See InputParameter. The input blobs to the network.</span></div><div class="line">  repeated <span class="built_in">string</span> input = <span class="number">3</span>;</div><div class="line">  <span class="comment">// DEPRECATED. See InputParameter. The shape of the input blobs.</span></div><div class="line">  repeated BlobShape input_shape = <span class="number">8</span>;</div><div class="line"></div><div class="line">  <span class="comment">// 4D input dimensions -- deprecated.  Use "input_shape" instead.</span></div><div class="line">  <span class="comment">// If specified, for each input blob there should be four</span></div><div class="line">  <span class="comment">// values specifying the num, channels, height and width of the input blob.</span></div><div class="line">  <span class="comment">// Thus, there should be a total of (4 * #input) numbers.</span></div><div class="line">  repeated int32 input_dim = <span class="number">4</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Whether the network will force every layer to carry out backward operation.</span></div><div class="line">  <span class="comment">// If set False, then whether to carry out backward is determined</span></div><div class="line">  <span class="comment">// automatically according to the net structure and learning rates.</span></div><div class="line">  optional <span class="keyword">bool</span> force_backward = <span class="number">5</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line">  <span class="comment">// The current "state" of the network, including the phase, level, and stage.</span></div><div class="line">  <span class="comment">// Some layers may be included/excluded depending on this state and the states</span></div><div class="line">  <span class="comment">// specified in the layers' include and exclude fields.</span></div><div class="line">  optional NetState state = <span class="number">6</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Print debugging information about results while running Net::Forward,</span></div><div class="line">  <span class="comment">// Net::Backward, and Net::Update.</span></div><div class="line">  optional <span class="keyword">bool</span> debug_info = <span class="number">7</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line"></div><div class="line">  <span class="comment">// The layers that make up the net.  Each of their configurations, including</span></div><div class="line">  <span class="comment">// connectivity and behavior, is specified as a LayerParameter.</span></div><div class="line">  repeated LayerParameter layer = <span class="number">100</span>;  <span class="comment">// ID 100 so layers are printed last.</span></div><div class="line"></div><div class="line">  <span class="comment">// DEPRECATED: use 'layer' instead.</span></div><div class="line">  repeated V1LayerParameter layers = <span class="number">2</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><code>net.hpp</code> 和 <code>net.cpp</code></p>
<h2 id="protected变量"><a href="#protected变量" class="headerlink" title="protected变量"></a>protected变量</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief The network name</span></div><div class="line"><span class="built_in">string</span> name_;  <span class="comment">//网络对应的名字</span></div><div class="line"><span class="comment">/// @brief The phase: TRAIN or TEST</span></div><div class="line">Phase phase_;  <span class="comment">//对应的阶段TRAIN或者TEST</span></div><div class="line"><span class="comment">/// @brief Individual layers in the net</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_; <span class="comment">//定义各层</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; layer_names_; <span class="comment">//各层的名字</span></div><div class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; layer_names_index_; <span class="comment">//各层对应的名字和index</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; layer_need_backward_; <span class="comment">//各层是否需要回传</span></div><div class="line"></div><div class="line"><span class="comment">/// @brief the blobs storing intermediate results between the layer.</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_; <span class="comment">//各层之间存储中间结果</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; blob_names_; <span class="comment">//各个blob的名字</span></div><div class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; blob_names_index_; <span class="comment">//各个blob名字及对应的index</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; blob_need_backward_;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/// bottom_vecs stores the vectors containing the input for each layer.</span></div><div class="line"><span class="comment">/// They don't actually host the blobs (blobs_ does), so we simply store</span></div><div class="line"><span class="comment">/// pointers.</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_; <span class="comment">//存放各个层的输入，vector的vector，每层的输入是一个vector&lt;Blob&lt;Dtype&gt;*&gt;</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; bottom_id_vecs_; <span class="comment">//存放各个层的输入blob对应的id</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; &gt; bottom_need_backward_;</div><div class="line"></div><div class="line"><span class="comment">/// top_vecs stores the vectors containing the output for each layer</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_; <span class="comment">//存放各个层的输出，每个层是一个vector&lt;Blob&lt;Dtype&gt;*&gt;</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; top_id_vecs_; <span class="comment">//存放各个层输出的blob的id</span></div><div class="line"></div><div class="line"><span class="comment">/// Vector of weight in the loss (or objective) function of each net blob,</span></div><div class="line"><span class="comment">/// indexed by blob_id.</span></div><div class="line"><span class="built_in">vector</span>&lt;Dtype&gt; blob_loss_weights_;  <span class="comment">//loss blob的权重</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; param_id_vecs_; <span class="comment">// </span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; param_owners_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; param_display_names_;</div><div class="line"><span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; &gt; param_layer_indices_;</div><div class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; param_names_index_;</div><div class="line"></div><div class="line"><span class="comment">/// blob indices for the input and the output of the net</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; net_input_blob_indices_;   <span class="comment">//网络输入blob的index</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; net_output_blob_indices_;  <span class="comment">//网络输出blob的index</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; net_input_blobs_; <span class="comment">//网络输入的blobs,存放的是指针</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; net_output_blobs_;<span class="comment">//网络输出的blobs，存放的是指针</span></div><div class="line"></div><div class="line"><span class="comment">/// The parameters in the network.</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; params_; <span class="comment">//网络的参数</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_; <span class="comment">//网络学习参数</span></div><div class="line"><span class="comment">/**</span></div><div class="line"> * The mapping from params_ -&gt; learnable_params_: we have</div><div class="line"> * learnable_param_ids_.size() == params_.size(),</div><div class="line"> * and learnable_params_[learnable_param_ids_[i]] == params_[i].get()</div><div class="line"> * if and only if params_[i] is an "owner"; otherwise, params_[i] is a sharer</div><div class="line"> * and learnable_params_[learnable_param_ids_[i]] gives its owner.</div><div class="line"> */</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; learnable_param_ids_;</div><div class="line"><span class="comment">/// the learning rate multipliers for learnable_params_</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; params_lr_; <span class="comment">//参数的学习率</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; has_params_lr_; <span class="comment">//是否有学习参数</span></div><div class="line"><span class="comment">/// the weight decay multipliers for learnable_params_</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; params_weight_decay_; <span class="comment">//权重衰减</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; has_params_decay_;     <span class="comment">//是否有权重衰减</span></div><div class="line"><span class="comment">/// The bytes of memory used by this net</span></div><div class="line"><span class="keyword">size_t</span> memory_used_;    <span class="comment">//网络使用的内存</span></div><div class="line"><span class="comment">/// Whether to compute and display debug info for the net.</span></div><div class="line"><span class="keyword">bool</span> debug_info_;</div><div class="line"><span class="comment">/// The root net that actually holds the shared layers in data parallelism</span></div><div class="line"><span class="keyword">const</span> Net* <span class="keyword">const</span> root_net_;</div><div class="line">DISABLE_COPY_AND_ASSIGN(Net);</div></pre></td></tr></table></figure>
<h2 id="protected-方法"><a href="#protected-方法" class="headerlink" title="protected 方法"></a>protected 方法</h2><ul>
<li><code>AppendTop():</code>    给网络增加新的top blob</li>
<li><code>AppendBottom():</code> 给网络增加新的bottom blob</li>
<li><code>AppendParam():</code>  给网络增加新的parameter blob</li>
<li><code>ForwardDebugInfo():</code>  前向时打印debug信息</li>
<li><code>BackwardDebugInfo():</code> 反向传播时打印debug信息</li>
<li><code>UpdataDebugInfo():</code>   更新时打印debug信息</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Helpers for Init.</span></div><div class="line"><span class="comment">/// @brief Append a new top blob to the net.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">AppendTop</span><span class="params">(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</span></span></div><div class="line">               <span class="keyword">const</span> <span class="keyword">int</span> top_id, <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* available_blobs,</div><div class="line">               <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;* blob_name_to_idx);</div><div class="line"><span class="comment">/// @brief Append a new bottom blob to the net.</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">AppendBottom</span><span class="params">(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</span></span></div><div class="line">                 <span class="keyword">const</span> <span class="keyword">int</span> bottom_id, <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* available_blobs,</div><div class="line">                 <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;* blob_name_to_idx);</div><div class="line"><span class="comment">/// @brief Append a new parameter blob to the net.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">AppendParam</span><span class="params">(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</span></span></div><div class="line">                 <span class="keyword">const</span> <span class="keyword">int</span> param_id);</div><div class="line"></div><div class="line"><span class="comment">/// @brief Helper for displaying debug info in Forward.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ForwardDebugInfo</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> layer_id)</span></span>;</div><div class="line"><span class="comment">/// @brief Helper for displaying debug info in Backward.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">BackwardDebugInfo</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> layer_id)</span></span>;</div><div class="line"><span class="comment">/// @brief Helper for displaying debug info in Update.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">UpdateDebugInfo</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id)</span></span>;</div></pre></td></tr></table></figure>
<h3 id="AppendTop"><a href="#AppendTop" class="headerlink" title="AppendTop()"></a><code>AppendTop()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Helper for Net::Init: add a new top blob to the net.</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::AppendTop(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</div><div class="line">                           <span class="keyword">const</span> <span class="keyword">int</span> top_id, <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* available_blobs,</div><div class="line">                           <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;* blob_name_to_idx) &#123;</div><div class="line">  <span class="built_in">shared_ptr</span>&lt;LayerParameter&gt; layer_param(</div><div class="line">      <span class="keyword">new</span> LayerParameter(param.layer(layer_id)));</div><div class="line">  <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = (layer_param-&gt;top_size() &gt; top_id) ?</div><div class="line">      layer_param-&gt;top(top_id) : <span class="string">"(automatic)"</span>;</div><div class="line">  <span class="comment">// Check if we are doing in-place computation</span></div><div class="line">  <span class="keyword">if</span> (blob_name_to_idx &amp;&amp; layer_param-&gt;bottom_size() &gt; top_id &amp;&amp;</div><div class="line">      blob_name == layer_param-&gt;bottom(top_id)) &#123;</div><div class="line">    <span class="comment">// In-place computation</span></div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; layer_param-&gt;name() &lt;&lt; <span class="string">" -&gt; "</span> &lt;&lt; blob_name &lt;&lt; <span class="string">" (in-place)"</span>;</div><div class="line">    top_vecs_[layer_id].push_back(blobs_[(*blob_name_to_idx)[blob_name]].get());</div><div class="line">    top_id_vecs_[layer_id].push_back((*blob_name_to_idx)[blob_name]);</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (blob_name_to_idx &amp;&amp;</div><div class="line">             blob_name_to_idx-&gt;find(blob_name) != blob_name_to_idx-&gt;end()) &#123;</div><div class="line">    <span class="comment">// If we are not doing in-place computation but have duplicated blobs,</span></div><div class="line">    <span class="comment">// raise an error.</span></div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Top blob '"</span> &lt;&lt; blob_name</div><div class="line">               &lt;&lt; <span class="string">"' produced by multiple sources."</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// Normal output.</span></div><div class="line">    <span class="keyword">if</span> (Caffe::root_solver()) &#123;</div><div class="line">      LOG(INFO) &lt;&lt; layer_param-&gt;name() &lt;&lt; <span class="string">" -&gt; "</span> &lt;&lt; blob_name;</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; blob_pointer(<span class="keyword">new</span> Blob&lt;Dtype&gt;());</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size();</div><div class="line">    blobs_.push_back(blob_pointer);</div><div class="line">    blob_names_.push_back(blob_name);</div><div class="line">    blob_need_backward_.push_back(<span class="literal">false</span>);</div><div class="line">    <span class="keyword">if</span> (blob_name_to_idx) &#123; (*blob_name_to_idx)[blob_name] = blob_id; &#125;</div><div class="line">    top_id_vecs_[layer_id].push_back(blob_id);</div><div class="line">    top_vecs_[layer_id].push_back(blob_pointer.get());</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (available_blobs) &#123; available_blobs-&gt;insert(blob_name); &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="AppendBottom"><a href="#AppendBottom" class="headerlink" title="AppendBottom()"></a><code>AppendBottom()</code></h3><p>这个函数不难，我们来分析一下，主要参数包括：</p>
<ul>
<li><code>param:</code> <code>NetParameter</code>类型的引用</li>
<li><code>layer_id:</code> 需要添加bottom 的layer的id</li>
<li><code>bottom_id:</code> bottom对应的id</li>
<li><code>availabel_blobs：</code> 可用的blobs, vector<string></string></li>
<li><code>blob_name_to_idx:</code> blob名字和idx对应的map</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Helper for Net::Init: add a new bottom blob to the net.</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">int</span> Net&lt;Dtype&gt;::AppendBottom(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> <span class="keyword">int</span> layer_id,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> bottom_id, <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;* available_blobs,</div><div class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;* blob_name_to_idx) &#123;</div><div class="line">  <span class="keyword">const</span> LayerParameter&amp; layer_param = param.layer(layer_id);</div><div class="line">  <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = layer_param.bottom(bottom_id);</div><div class="line">  <span class="keyword">if</span> (available_blobs-&gt;find(blob_name) == available_blobs-&gt;end()) &#123;</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown bottom blob '"</span> &lt;&lt; blob_name &lt;&lt; <span class="string">"' (layer '"</span></div><div class="line">               &lt;&lt; layer_param.name() &lt;&lt; <span class="string">"', bottom index "</span> &lt;&lt; bottom_id &lt;&lt; <span class="string">")"</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> blob_id = (*blob_name_to_idx)[blob_name];</div><div class="line">  LOG_IF(INFO, Caffe::root_solver())</div><div class="line">      &lt;&lt; layer_names_[layer_id] &lt;&lt; <span class="string">" &lt;- "</span> &lt;&lt; blob_name;</div><div class="line">  bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());</div><div class="line">  bottom_id_vecs_[layer_id].push_back(blob_id);</div><div class="line">  available_blobs-&gt;erase(blob_name);</div><div class="line">  <span class="keyword">bool</span> need_backward = blob_need_backward_[blob_id];</div><div class="line">  <span class="comment">// Check if the backpropagation on bottom_id should be skipped</span></div><div class="line">  <span class="keyword">if</span> (layer_param.propagate_down_size() &gt; <span class="number">0</span>) &#123;</div><div class="line">    need_backward = layer_param.propagate_down(bottom_id);</div><div class="line">  &#125;</div><div class="line">  bottom_need_backward_[layer_id].push_back(need_backward);</div><div class="line">  <span class="keyword">return</span> blob_id;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="AppendParam"><a href="#AppendParam" class="headerlink" title="AppendParam()"></a><code>AppendParam()</code></h3><p>在网络中增加层对应的bottom的 param</p>
<p>参数</p>
<ul>
<li><code>param:</code> <code>NetParameter</code>引用</li>
<li><code>layer_id:</code> 对应层的id</li>
<li><code>param_id:</code> param对应的id</li>
</ul>
<h2 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h2><p>在构造函数里最主要的是调用了<code>Init()</code>函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Net&lt;Dtype&gt;::Net(<span class="keyword">const</span> NetParameter&amp; param, <span class="keyword">const</span> Net* root_net)</div><div class="line">    : root_net_(root_net) &#123;</div><div class="line">  Init(param);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Net&lt;Dtype&gt;::Net(<span class="keyword">const</span> <span class="built_in">string</span>&amp; param_file, Phase phase,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> level, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;* stages,</div><div class="line">    <span class="keyword">const</span> Net* root_net)</div><div class="line">    : root_net_(root_net) &#123;</div><div class="line">  NetParameter param;</div><div class="line">  ReadNetParamsFromTextFileOrDie(param_file, &amp;param);</div><div class="line">  <span class="comment">// Set phase, stages and level</span></div><div class="line">  param.mutable_state()-&gt;set_phase(phase);</div><div class="line">  <span class="keyword">if</span> (stages != <span class="literal">NULL</span>) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stages-&gt;size(); i++) &#123;</div><div class="line">      param.mutable_state()-&gt;add_stage((*stages)[i]);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  param.mutable_state()-&gt;set_level(level);</div><div class="line">  Init(param);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="FilterNet-函数"><a href="#FilterNet-函数" class="headerlink" title="FilterNet()函数"></a><code>FilterNet()</code>函数</h2><p>使用其他param 来初始化本网络的<code>param</code>,其中考虑了include 和exclude 的状态。即滤出一些不包含状态下的层</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">template &lt;typename Dtype&gt;</div><div class="line">void Net&lt;Dtype&gt;::FilterNet(const NetParameter&amp; param,</div><div class="line">    NetParameter* param_filtered) &#123;</div><div class="line">  NetState net_state(param.state());</div><div class="line">  param_filtered-&gt;CopyFrom(param);</div><div class="line">  param_filtered-&gt;clear_layer();</div><div class="line">  for (int i = 0; i &lt; param.layer_size(); ++i) &#123;</div><div class="line">    const LayerParameter&amp; layer_param = param.layer(i);</div><div class="line">    const string&amp; layer_name = layer_param.name();</div><div class="line">    CHECK(layer_param.include_size() == 0 || layer_param.exclude_size() == 0)</div><div class="line">          &lt;&lt; "Specify either include rules or exclude rules; not both.";</div><div class="line">    // If no include rules are specified, the layer is included by default and</div><div class="line">    // only excluded if it meets one of the exclude rules.</div><div class="line">    bool layer_included = (layer_param.include_size() == 0);</div><div class="line">    for (int j = 0; layer_included &amp;&amp; j &lt; layer_param.exclude_size(); ++j) &#123;</div><div class="line">      if (StateMeetsRule(net_state, layer_param.exclude(j), layer_name)) &#123;</div><div class="line">        layer_included = false;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    for (int j = 0; !layer_included &amp;&amp; j &lt; layer_param.include_size(); ++j) &#123;</div><div class="line">      if (StateMeetsRule(net_state, layer_param.include(j), layer_name)) &#123;</div><div class="line">        layer_included = true;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    if (layer_included) &#123;</div><div class="line">      param_filtered-&gt;add_layer()-&gt;CopyFrom(layer_param);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Init-函数"><a href="#Init-函数" class="headerlink" title="Init()函数"></a><code>Init()</code>函数</h2><ul>
<li>作用：使用<code>NetParameter</code>参数对网络进行初始化。</li>
<li>code 略长,很重要</li>
<li>分析技巧：每个<code>Net</code>由<code>Layer</code>类型的vector组成，每个<code>layer</code>都有对应的bottom vector输入和top vector输出，<br>所以一个<code>net</code>中有很多的<code>vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt;&gt;</code> 组成, 需要对其赋值操作细看。不明白的地方先看proto</li>
<li>主要的逻辑是对<code>Net</code>中一层一层进行初始化，解决每层的输入数据，参数设定等，计算需要的内存等</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Init(<span class="keyword">const</span> NetParameter&amp; in_param) &#123;</div><div class="line">  CHECK(Caffe::root_solver() || root_net_)</div><div class="line">      &lt;&lt; <span class="string">"root_net_ needs to be set for all non-root solvers"</span>;</div><div class="line">  <span class="comment">// Set phase from the state.</span></div><div class="line">  phase_ = in_param.state().phase();</div><div class="line">  <span class="comment">// Filter layers based on their include/exclude rules and</span></div><div class="line">  <span class="comment">// the current NetState.</span></div><div class="line">  NetParameter filtered_param;</div><div class="line">  FilterNet(in_param, &amp;filtered_param);</div><div class="line">  LOG_IF(INFO, Caffe::root_solver())</div><div class="line">      &lt;&lt; <span class="string">"Initializing net from parameters: "</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span></div><div class="line">      &lt;&lt; filtered_param.DebugString();</div><div class="line">  <span class="comment">// Create a copy of filtered_param with splits added where necessary.</span></div><div class="line">  NetParameter param;</div><div class="line">  InsertSplits(filtered_param, &amp;param);</div><div class="line">  <span class="comment">// Basically, build all the layers and set up their connections.</span></div><div class="line">  name_ = param.name();</div><div class="line">  <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; blob_name_to_idx;</div><div class="line">  <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt; available_blobs;</div><div class="line">  memory_used_ = <span class="number">0</span>;</div><div class="line">  <span class="comment">// For each layer, set up its input and output</span></div><div class="line">  bottom_vecs_.resize(param.layer_size());</div><div class="line">  top_vecs_.resize(param.layer_size());</div><div class="line">  bottom_id_vecs_.resize(param.layer_size());</div><div class="line">  param_id_vecs_.resize(param.layer_size());</div><div class="line">  top_id_vecs_.resize(param.layer_size());</div><div class="line">  bottom_need_backward_.resize(param.layer_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;</div><div class="line">    <span class="comment">// For non-root solvers, whether this layer is shared from root_net_.</span></div><div class="line">    <span class="keyword">bool</span> share_from_root = !Caffe::root_solver()</div><div class="line">        &amp;&amp; root_net_-&gt;layers_[layer_id]-&gt;ShareInParallel();</div><div class="line">    <span class="comment">// Inherit phase from net if unset.</span></div><div class="line">    <span class="keyword">if</span> (!param.layer(layer_id).has_phase()) &#123;</div><div class="line">      param.mutable_layer(layer_id)-&gt;set_phase(phase_);</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// Setup layer.</span></div><div class="line">    <span class="keyword">const</span> LayerParameter&amp; layer_param = param.layer(layer_id);</div><div class="line">    <span class="keyword">if</span> (layer_param.propagate_down_size() &gt; <span class="number">0</span>) &#123;</div><div class="line">      CHECK_EQ(layer_param.propagate_down_size(),</div><div class="line">          layer_param.bottom_size())</div><div class="line">          &lt;&lt; <span class="string">"propagate_down param must be specified "</span></div><div class="line">          &lt;&lt; <span class="string">"either 0 or bottom_size times "</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (share_from_root) &#123;</div><div class="line">      LOG(INFO) &lt;&lt; <span class="string">"Sharing layer "</span> &lt;&lt; layer_param.name() &lt;&lt; <span class="string">" from root net"</span>;</div><div class="line">      layers_.push_back(root_net_-&gt;layers_[layer_id]);</div><div class="line">      layers_[layer_id]-&gt;SetShared(<span class="literal">true</span>);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</div><div class="line">    &#125;</div><div class="line">    layer_names_.push_back(layer_param.name());</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"Creating Layer "</span> &lt;&lt; layer_param.name();</div><div class="line">    <span class="keyword">bool</span> need_backward = <span class="literal">false</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Figure out this layer's input and output</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size();</div><div class="line">         ++bottom_id) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> blob_id = AppendBottom(param, layer_id, bottom_id,</div><div class="line">                                       &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// If a blob needs backward, this layer should provide it.</span></div><div class="line">      need_backward |= blob_need_backward_[blob_id];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> num_top = layer_param.top_size();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; num_top; ++top_id) &#123;</div><div class="line">      AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// Collect Input layer tops as Net inputs.</span></div><div class="line">      <span class="keyword">if</span> (layer_param.type() == <span class="string">"Input"</span>) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size() - <span class="number">1</span>;</div><div class="line">        net_input_blob_indices_.push_back(blob_id);</div><div class="line">        net_input_blobs_.push_back(blobs_[blob_id].get());</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// If the layer specifies that AutoTopBlobs() -&gt; true and the LayerParameter</span></div><div class="line">    <span class="comment">// specified fewer than the required number (as specified by</span></div><div class="line">    <span class="comment">// ExactNumTopBlobs() or MinTopBlobs()), allocate them here.</span></div><div class="line">    Layer&lt;Dtype&gt;* layer = layers_[layer_id].get();</div><div class="line">    <span class="keyword">if</span> (layer-&gt;AutoTopBlobs()) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> needed_num_top =</div><div class="line">          <span class="built_in">std</span>::max(layer-&gt;MinTopBlobs(), layer-&gt;ExactNumTopBlobs());</div><div class="line">      <span class="keyword">for</span> (; num_top &lt; needed_num_top; ++num_top) &#123;</div><div class="line">        <span class="comment">// Add "anonymous" top blobs -- do not modify available_blobs or</span></div><div class="line">        <span class="comment">// blob_name_to_idx as we don't want these blobs to be usable as input</span></div><div class="line">        <span class="comment">// to other layers.</span></div><div class="line">        AppendTop(param, layer_id, num_top, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// After this layer is connected, set it up.</span></div><div class="line">    <span class="keyword">if</span> (share_from_root) &#123;</div><div class="line">      <span class="comment">// Set up size of top blobs using root_net_</span></div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; base_top = root_net_-&gt;top_vecs_[layer_id];</div><div class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; this_top = <span class="keyword">this</span>-&gt;top_vecs_[layer_id];</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; base_top.size(); ++top_id) &#123;</div><div class="line">        this_top[top_id]-&gt;ReshapeLike(*base_top[top_id]);</div><div class="line">        LOG(INFO) &lt;&lt; <span class="string">"Created top blob "</span> &lt;&lt; top_id &lt;&lt; <span class="string">" (shape: "</span></div><div class="line">            &lt;&lt; this_top[top_id]-&gt;shape_string() &lt;&lt;  <span class="string">") for shared layer "</span></div><div class="line">            &lt;&lt; layer_param.name();</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</div><div class="line">    &#125;</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"Setting up "</span> &lt;&lt; layer_names_[layer_id];</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size(); ++top_id) &#123;</div><div class="line">      <span class="keyword">if</span> (blob_loss_weights_.size() &lt;= top_id_vecs_[layer_id][top_id]) &#123;</div><div class="line">        blob_loss_weights_.resize(top_id_vecs_[layer_id][top_id] + <span class="number">1</span>, Dtype(<span class="number">0</span>));</div><div class="line">      &#125;</div><div class="line">      blob_loss_weights_[top_id_vecs_[layer_id][top_id]] = layer-&gt;loss(top_id);</div><div class="line">      LOG_IF(INFO, Caffe::root_solver())</div><div class="line">          &lt;&lt; <span class="string">"Top shape: "</span> &lt;&lt; top_vecs_[layer_id][top_id]-&gt;shape_string();</div><div class="line">      <span class="keyword">if</span> (layer-&gt;loss(top_id)) &#123;</div><div class="line">        LOG_IF(INFO, Caffe::root_solver())</div><div class="line">            &lt;&lt; <span class="string">"    with loss weight "</span> &lt;&lt; layer-&gt;loss(top_id);</div><div class="line">      &#125;</div><div class="line">      memory_used_ += top_vecs_[layer_id][top_id]-&gt;count();</div><div class="line">    &#125;</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"Memory required for data: "</span> &lt;&lt; <span class="function">memory_used_ * <span class="title">sizeof</span><span class="params">(Dtype)</span></span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> param_size = layer_param.param_size();</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> num_param_blobs = layers_[layer_id]-&gt;blobs().size();</div><div class="line">    CHECK_LE(param_size, num_param_blobs)</div><div class="line">        &lt;&lt; <span class="string">"Too many params specified for layer "</span> &lt;&lt; layer_param.name();</div><div class="line">    ParamSpec default_param_spec;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</div><div class="line">      <span class="keyword">const</span> ParamSpec* param_spec = (param_id &lt; param_size) ?</div><div class="line">          &amp;layer_param.param(param_id) : &amp;default_param_spec;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">bool</span> param_need_backward = param_spec-&gt;lr_mult() != <span class="number">0</span>;</div><div class="line">      need_backward |= param_need_backward;</div><div class="line">      layers_[layer_id]-&gt;set_param_propagate_down(param_id,</div><div class="line">                                                  param_need_backward);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; num_param_blobs; ++param_id) &#123;</div><div class="line">      AppendParam(param, layer_id, param_id);</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// Finally, set the backward flag</span></div><div class="line">    layer_need_backward_.push_back(need_backward);</div><div class="line">    <span class="keyword">if</span> (need_backward) &#123;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top_id_vecs_[layer_id].size(); ++top_id) &#123;</div><div class="line">        blob_need_backward_[top_id_vecs_[layer_id][top_id]] = <span class="literal">true</span>;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Go through the net backwards to determine which blobs contribute to the</span></div><div class="line">  <span class="comment">// loss.  We can skip backward computation for blobs that don't contribute</span></div><div class="line">  <span class="comment">// to the loss.</span></div><div class="line">  <span class="comment">// Also checks if all bottom blobs don't need backward computation (possible</span></div><div class="line">  <span class="comment">// because the skip_propagate_down param) and so we can skip bacward</span></div><div class="line">  <span class="comment">// computation for the entire layer</span></div><div class="line">  <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt; blobs_under_loss;</div><div class="line">  <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt; blobs_skip_backp;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> layer_id = layers_.size() - <span class="number">1</span>; layer_id &gt;= <span class="number">0</span>; --layer_id) &#123;</div><div class="line">    <span class="keyword">bool</span> layer_contributes_loss = <span class="literal">false</span>;</div><div class="line">    <span class="keyword">bool</span> layer_skip_propagate_down = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size(); ++top_id) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];</div><div class="line">      <span class="keyword">if</span> (layers_[layer_id]-&gt;loss(top_id) ||</div><div class="line">          (blobs_under_loss.find(blob_name) != blobs_under_loss.end())) &#123;</div><div class="line">        layer_contributes_loss = <span class="literal">true</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) &#123;</div><div class="line">        layer_skip_propagate_down = <span class="literal">false</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (layer_contributes_loss &amp;&amp; !layer_skip_propagate_down)</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// If this layer can skip backward computation, also all his bottom blobs</span></div><div class="line">    <span class="comment">// don't need backpropagation</span></div><div class="line">    <span class="keyword">if</span> (layer_need_backward_[layer_id] &amp;&amp; layer_skip_propagate_down) &#123;</div><div class="line">      layer_need_backward_[layer_id] = <span class="literal">false</span>;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; bottom_vecs_[layer_id].size();</div><div class="line">               ++bottom_id) &#123;</div><div class="line">        bottom_need_backward_[layer_id][bottom_id] = <span class="literal">false</span>;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!layer_contributes_loss) &#123; layer_need_backward_[layer_id] = <span class="literal">false</span>; &#125;</div><div class="line">    <span class="keyword">if</span> (Caffe::root_solver()) &#123;</div><div class="line">      <span class="keyword">if</span> (layer_need_backward_[layer_id]) &#123;</div><div class="line">        LOG(INFO) &lt;&lt; layer_names_[layer_id] &lt;&lt; <span class="string">" needs backward computation."</span>;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        LOG(INFO) &lt;&lt; layer_names_[layer_id]</div><div class="line">            &lt;&lt; <span class="string">" does not need backward computation."</span>;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; bottom_vecs_[layer_id].size();</div><div class="line">         ++bottom_id) &#123;</div><div class="line">      <span class="keyword">if</span> (layer_contributes_loss) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name =</div><div class="line">            blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">        blobs_under_loss.insert(blob_name);</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        bottom_need_backward_[layer_id][bottom_id] = <span class="literal">false</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (!bottom_need_backward_[layer_id][bottom_id]) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name =</div><div class="line">                   blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">        blobs_skip_backp.insert(blob_name);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Handle force_backward if needed.</span></div><div class="line">  <span class="keyword">if</span> (param.force_backward()) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; layers_.size(); ++layer_id) &#123;</div><div class="line">      layer_need_backward_[layer_id] = <span class="literal">true</span>;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>;</div><div class="line">           bottom_id &lt; bottom_need_backward_[layer_id].size(); ++bottom_id) &#123;</div><div class="line">        bottom_need_backward_[layer_id][bottom_id] =</div><div class="line">            bottom_need_backward_[layer_id][bottom_id] ||</div><div class="line">            layers_[layer_id]-&gt;AllowForceBackward(bottom_id);</div><div class="line">        blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] =</div><div class="line">            blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] ||</div><div class="line">            bottom_need_backward_[layer_id][bottom_id];</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; layers_[layer_id]-&gt;blobs().size();</div><div class="line">           ++param_id) &#123;</div><div class="line">        layers_[layer_id]-&gt;set_param_propagate_down(param_id, <span class="literal">true</span>);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// In the end, all remaining blobs are considered output blobs.</span></div><div class="line">  <span class="keyword">for</span> (<span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;::iterator it = available_blobs.begin();</div><div class="line">      it != available_blobs.end(); ++it) &#123;</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"This network produces output "</span> &lt;&lt; *it;</div><div class="line">    net_output_blobs_.push_back(blobs_[blob_name_to_idx[*it]].get());</div><div class="line">    net_output_blob_indices_.push_back(blob_name_to_idx[*it]);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> blob_id = <span class="number">0</span>; blob_id &lt; blob_names_.size(); ++blob_id) &#123;</div><div class="line">    blob_names_index_[blob_names_[blob_id]] = blob_id;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> layer_id = <span class="number">0</span>; layer_id &lt; layer_names_.size(); ++layer_id) &#123;</div><div class="line">    layer_names_index_[layer_names_[layer_id]] = layer_id;</div><div class="line">  &#125;</div><div class="line">  ShareWeights();</div><div class="line">  debug_info_ = param.debug_info();</div><div class="line">  LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; <span class="string">"Network initialization done."</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="StateMeetsRule"><a href="#StateMeetsRule" class="headerlink" title="StateMeetsRule"></a><code>StateMeetsRule</code></h2><p>网络中每层的状态检查函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">bool</span> Net&lt;Dtype&gt;::StateMeetsRule(<span class="keyword">const</span> NetState&amp; state,</div><div class="line">    <span class="keyword">const</span> NetStateRule&amp; rule, <span class="keyword">const</span> <span class="built_in">string</span>&amp; layer_name) &#123;</div><div class="line">  <span class="comment">// Check whether the rule is broken due to phase.</span></div><div class="line">  <span class="keyword">if</span> (rule.has_phase()) &#123;</div><div class="line">      <span class="keyword">if</span> (rule.phase() != state.phase()) &#123;</div><div class="line">        LOG_IF(INFO, Caffe::root_solver())</div><div class="line">            &lt;&lt; <span class="string">"The NetState phase ("</span> &lt;&lt; state.phase()</div><div class="line">            &lt;&lt; <span class="string">") differed from the phase ("</span> &lt;&lt; rule.phase()</div><div class="line">            &lt;&lt; <span class="string">") specified by a rule in layer "</span> &lt;&lt; layer_name;</div><div class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Check whether the rule is broken due to min level.</span></div><div class="line">  <span class="keyword">if</span> (rule.has_min_level()) &#123;</div><div class="line">    <span class="keyword">if</span> (state.level() &lt; rule.min_level()) &#123;</div><div class="line">      LOG_IF(INFO, Caffe::root_solver())</div><div class="line">          &lt;&lt; <span class="string">"The NetState level ("</span> &lt;&lt; state.level()</div><div class="line">          &lt;&lt; <span class="string">") is above the min_level ("</span> &lt;&lt; rule.min_level()</div><div class="line">          &lt;&lt; <span class="string">") specified by a rule in layer "</span> &lt;&lt; layer_name;</div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Check whether the rule is broken due to max level.</span></div><div class="line">  <span class="keyword">if</span> (rule.has_max_level()) &#123;</div><div class="line">    <span class="keyword">if</span> (state.level() &gt; rule.max_level()) &#123;</div><div class="line">      LOG_IF(INFO, Caffe::root_solver())</div><div class="line">          &lt;&lt; <span class="string">"The NetState level ("</span> &lt;&lt; state.level()</div><div class="line">          &lt;&lt; <span class="string">") is above the max_level ("</span> &lt;&lt; rule.max_level()</div><div class="line">          &lt;&lt; <span class="string">") specified by a rule in layer "</span> &lt;&lt; layer_name;</div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Check whether the rule is broken due to stage. The NetState must</span></div><div class="line">  <span class="comment">// contain ALL of the rule's stages to meet it.</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rule.stage_size(); ++i) &#123;</div><div class="line">    <span class="comment">// Check that the NetState contains the rule's ith stage.</span></div><div class="line">    <span class="keyword">bool</span> has_stage = <span class="literal">false</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; !has_stage &amp;&amp; j &lt; state.stage_size(); ++j) &#123;</div><div class="line">      <span class="keyword">if</span> (rule.stage(i) == state.stage(j)) &#123; has_stage = <span class="literal">true</span>; &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!has_stage) &#123;</div><div class="line">      LOG_IF(INFO, Caffe::root_solver())</div><div class="line">          &lt;&lt; <span class="string">"The NetState did not contain stage '"</span> &lt;&lt; rule.stage(i)</div><div class="line">          &lt;&lt; <span class="string">"' specified by a rule in layer "</span> &lt;&lt; layer_name;</div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// Check whether the rule is broken due to not_stage. The NetState must</span></div><div class="line">  <span class="comment">// contain NONE of the rule's not_stages to meet it.</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; rule.not_stage_size(); ++i) &#123;</div><div class="line">    <span class="comment">// Check that the NetState contains the rule's ith not_stage.</span></div><div class="line">    <span class="keyword">bool</span> has_stage = <span class="literal">false</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; !has_stage &amp;&amp; j &lt; state.stage_size(); ++j) &#123;</div><div class="line">      <span class="keyword">if</span> (rule.not_stage(i) == state.stage(j)) &#123; has_stage = <span class="literal">true</span>; &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (has_stage) &#123;</div><div class="line">      LOG_IF(INFO, Caffe::root_solver())</div><div class="line">          &lt;&lt; <span class="string">"The NetState contained a not_stage '"</span> &lt;&lt; rule.not_stage(i)</div><div class="line">          &lt;&lt; <span class="string">"' specified by a rule in layer "</span> &lt;&lt; layer_name;</div><div class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Net中的前向函数"><a href="#Net中的前向函数" class="headerlink" title="Net中的前向函数"></a>Net中的前向函数</h2><p>网络中层之间的前向函数，指定start和end,返回的是经过的层之间的loss<br>具体是对每层调用Forward</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Net&lt;Dtype&gt;::ForwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(start, <span class="number">0</span>);</div><div class="line">  CHECK_LT(end, layers_.size());</div><div class="line">  Dtype loss = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt;= end; ++i) &#123;</div><div class="line">    <span class="comment">// LOG(ERROR) &lt;&lt; "Forwarding " &lt;&lt; layer_names_[i];</span></div><div class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);</div><div class="line">    loss += layer_loss;</div><div class="line">    <span class="keyword">if</span> (debug_info_) &#123; ForwardDebugInfo(i); &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> loss;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Net&lt;Dtype&gt;::ForwardFrom(<span class="keyword">int</span> start) &#123;</div><div class="line">  <span class="keyword">return</span> ForwardFromTo(start, layers_.size() - <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Net&lt;Dtype&gt;::ForwardTo(<span class="keyword">int</span> end) &#123;</div><div class="line">  <span class="keyword">return</span> ForwardFromTo(<span class="number">0</span>, end);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; Net&lt;Dtype&gt;::Forward(Dtype* loss) &#123;</div><div class="line">  <span class="keyword">if</span> (loss != <span class="literal">NULL</span>) &#123;</div><div class="line">    *loss = ForwardFromTo(<span class="number">0</span>, layers_.size() - <span class="number">1</span>);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    ForwardFromTo(<span class="number">0</span>, layers_.size() - <span class="number">1</span>);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> net_output_blobs_;</div><div class="line">&#125;</div><div class="line"></div><div class="line">``` </div><div class="line"></div><div class="line">## Net中的反向传播函数</div><div class="line">对每层调用Backward</div><div class="line"> </div><div class="line">```cpp</div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(end, <span class="number">0</span>);</div><div class="line">  CHECK_LT(start, layers_.size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &gt;= end; --i) &#123;</div><div class="line">    <span class="keyword">if</span> (layer_need_backward_[i]) &#123;</div><div class="line">      layers_[i]-&gt;Backward(</div><div class="line">          top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);</div><div class="line">      <span class="keyword">if</span> (debug_info_) &#123; BackwardDebugInfo(i); &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFrom(<span class="keyword">int</span> start) &#123;</div><div class="line">  BackwardFromTo(start, <span class="number">0</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardTo(<span class="keyword">int</span> end) &#123;</div><div class="line">  BackwardFromTo(layers_.size() - <span class="number">1</span>, end);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Backward() &#123;</div><div class="line">  BackwardFromTo(layers_.size() - <span class="number">1</span>, <span class="number">0</span>);</div><div class="line">  <span class="keyword">if</span> (debug_info_) &#123;</div><div class="line">    Dtype asum_data = <span class="number">0</span>, asum_diff = <span class="number">0</span>, sumsq_data = <span class="number">0</span>, sumsq_diff = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; learnable_params_.size(); ++i) &#123;</div><div class="line">      asum_data += learnable_params_[i]-&gt;asum_data();</div><div class="line">      asum_diff += learnable_params_[i]-&gt;asum_diff();</div><div class="line">      sumsq_data += learnable_params_[i]-&gt;sumsq_data();</div><div class="line">      sumsq_diff += learnable_params_[i]-&gt;sumsq_diff();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">const</span> Dtype l2norm_data = <span class="built_in">std</span>::<span class="built_in">sqrt</span>(sumsq_data);</div><div class="line">    <span class="keyword">const</span> Dtype l2norm_diff = <span class="built_in">std</span>::<span class="built_in">sqrt</span>(sumsq_diff);</div><div class="line">    LOG(ERROR) &lt;&lt; <span class="string">"    [Backward] All net params (data, diff): "</span></div><div class="line">               &lt;&lt; <span class="string">"L1 norm = ("</span> &lt;&lt; asum_data &lt;&lt; <span class="string">", "</span> &lt;&lt; asum_diff &lt;&lt; <span class="string">"); "</span></div><div class="line">               &lt;&lt; <span class="string">"L2 norm = ("</span> &lt;&lt; l2norm_data &lt;&lt; <span class="string">", "</span> &lt;&lt; l2norm_diff &lt;&lt; <span class="string">")"</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a><code>Reshape</code></h2><p>对网络中每层调用Reshape</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::Reshape() &#123;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; layers_.size(); ++i) &#123;</div><div class="line">    layers_[i]-&gt;Reshape(bottom_vecs_[i], top_vecs_[i]);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Debug信息打印函数"><a href="#Debug信息打印函数" class="headerlink" title="Debug信息打印函数"></a>Debug信息打印函数</h2><p>在网络前向，反向和更新时打印信息</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::ForwardDebugInfo(<span class="keyword">const</span> <span class="keyword">int</span> layer_id) &#123;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top_vecs_[layer_id].size(); ++top_id) &#123;</div><div class="line">    <span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; blob = *top_vecs_[layer_id][top_id];</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];</div><div class="line">    <span class="keyword">const</span> Dtype data_abs_val_mean = blob.asum_data() / blob.count();</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Forward] "</span></div><div class="line">        &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; layer_names_[layer_id]</div><div class="line">        &lt;&lt; <span class="string">", top blob "</span> &lt;&lt; blob_name</div><div class="line">        &lt;&lt; <span class="string">" data: "</span> &lt;&lt; data_abs_val_mean;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; layers_[layer_id]-&gt;blobs().size();</div><div class="line">       ++param_id) &#123;</div><div class="line">    <span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; blob = *layers_[layer_id]-&gt;blobs()[param_id];</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> net_param_id = param_id_vecs_[layer_id][param_id];</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = param_display_names_[net_param_id];</div><div class="line">    <span class="keyword">const</span> Dtype data_abs_val_mean = blob.asum_data() / blob.count();</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Forward] "</span></div><div class="line">        &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; layer_names_[layer_id]</div><div class="line">        &lt;&lt; <span class="string">", param blob "</span> &lt;&lt; blob_name</div><div class="line">        &lt;&lt; <span class="string">" data: "</span> &lt;&lt; data_abs_val_mean;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardDebugInfo(<span class="keyword">const</span> <span class="keyword">int</span> layer_id) &#123;</div><div class="line">  <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom_vec = bottom_vecs_[layer_id];</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; bottom_vec.size(); ++bottom_id) &#123;</div><div class="line">    <span class="keyword">if</span> (!bottom_need_backward_[layer_id][bottom_id]) &#123; <span class="keyword">continue</span>; &#125;</div><div class="line">    <span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; blob = *bottom_vec[bottom_id];</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];</div><div class="line">    <span class="keyword">const</span> Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Backward] "</span></div><div class="line">        &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; layer_names_[layer_id]</div><div class="line">        &lt;&lt; <span class="string">", bottom blob "</span> &lt;&lt; blob_name</div><div class="line">        &lt;&lt; <span class="string">" diff: "</span> &lt;&lt; diff_abs_val_mean;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; layers_[layer_id]-&gt;blobs().size();</div><div class="line">       ++param_id) &#123;</div><div class="line">    <span class="keyword">if</span> (!layers_[layer_id]-&gt;param_propagate_down(param_id)) &#123; <span class="keyword">continue</span>; &#125;</div><div class="line">    <span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; blob = *layers_[layer_id]-&gt;blobs()[param_id];</div><div class="line">    <span class="keyword">const</span> Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Backward] "</span></div><div class="line">        &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; layer_names_[layer_id]</div><div class="line">        &lt;&lt; <span class="string">", param blob "</span> &lt;&lt; param_id</div><div class="line">        &lt;&lt; <span class="string">" diff: "</span> &lt;&lt; diff_abs_val_mean;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::UpdateDebugInfo(<span class="keyword">const</span> <span class="keyword">int</span> param_id) &#123;</div><div class="line">  <span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; blob = *params_[param_id];</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> param_owner = param_owners_[param_id];</div><div class="line">  <span class="keyword">const</span> <span class="built_in">string</span>&amp; layer_name = layer_names_[param_layer_indices_[param_id].first];</div><div class="line">  <span class="keyword">const</span> <span class="built_in">string</span>&amp; param_display_name = param_display_names_[param_id];</div><div class="line">  <span class="keyword">const</span> Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();</div><div class="line">  <span class="keyword">if</span> (param_owner &lt; <span class="number">0</span>) &#123;</div><div class="line">    <span class="keyword">const</span> Dtype data_abs_val_mean = blob.asum_data() / blob.count();</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Update] Layer "</span> &lt;&lt; layer_name</div><div class="line">        &lt;&lt; <span class="string">", param "</span> &lt;&lt; param_display_name</div><div class="line">        &lt;&lt; <span class="string">" data: "</span> &lt;&lt; data_abs_val_mean</div><div class="line">        &lt;&lt; <span class="string">"; diff: "</span> &lt;&lt; diff_abs_val_mean;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">const</span> <span class="built_in">string</span>&amp; owner_layer_name =</div><div class="line">        layer_names_[param_layer_indices_[param_owner].first];</div><div class="line">    LOG_IF(INFO, Caffe::root_solver())</div><div class="line">        &lt;&lt; <span class="string">"    [Update] Layer "</span> &lt;&lt; layer_name</div><div class="line">        &lt;&lt; <span class="string">", param blob "</span> &lt;&lt; param_display_name</div><div class="line">        &lt;&lt; <span class="string">" (owned by layer "</span> &lt;&lt; owner_layer_name &lt;&lt; <span class="string">", "</span> &lt;&lt; <span class="string">"param "</span></div><div class="line">        &lt;&lt; param_display_names_[param_owners_[param_id]] &lt;&lt; <span class="string">")"</span></div><div class="line">        &lt;&lt; <span class="string">" diff: "</span> &lt;&lt; diff_abs_val_mean;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="从其他网络复制层"><a href="#从其他网络复制层" class="headerlink" title="从其他网络复制层"></a>从其他网络复制层</h2><ul>
<li><code>ShareTrainedLayersWith</code></li>
<li><code>CopyTrainedLayersFrom(const NetParameter&amp; param)</code></li>
<li><code>CopyTrainedLayersFrom(const string trained_filename)</code></li>
<li><code>CopyTrainedLayersFromBinaryProto()</code></li>
<li><code>CopyTrainedLayersFromHDF5</code></li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">**</div><div class="line">   * @brief For an already initialized net, implicitly copies (i.e., using no</div><div class="line">   *        additional memory) the pre-trained layers from another Net.</div><div class="line">   */</div><div class="line">  void ShareTrainedLayersWith(const Net* other);</div><div class="line">  // For an already initialized net, CopyTrainedLayersFrom() copies the already</div><div class="line">  // trained layers from another net parameter instance.</div><div class="line">  /**</div><div class="line">   * @brief For an already initialized net, copies the pre-trained layers from</div><div class="line">   *        another Net.</div><div class="line">   */</div><div class="line">  void CopyTrainedLayersFrom(const NetParameter&amp; param);</div><div class="line">  void CopyTrainedLayersFrom(const string trained_filename);</div><div class="line">  void CopyTrainedLayersFromBinaryProto(const string trained_filename);</div><div class="line">  void CopyTrainedLayersFromHDF5(const string trained_filename);</div></pre></td></tr></table></figure>
<h2 id="写到proto或者hdf5"><a href="#写到proto或者hdf5" class="headerlink" title="写到proto或者hdf5"></a>写到proto或者hdf5</h2><ul>
<li><code>ToProto(NetParameter* param, bool write_diff)</code></li>
<li><code>ToHDF5(const string&amp; filename, bool write_diff)</code></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief Writes the net to a proto.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(NetParameter* param, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span> <span class="keyword">const</span></span>;</div><div class="line"><span class="comment">/// @brief Writes the net to an HDF5 file.</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ToHDF5</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; filename, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span> <span class="keyword">const</span></span>;</div></pre></td></tr></table></figure>
<h2 id="其他的一些Get函数"><a href="#其他的一些Get函数" class="headerlink" title="其他的一些Get函数"></a>其他的一些Get函数</h2><p>简单的Get函数一般都是内联函数，用来获取定义的保护变量.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief returns the network name.</span></div><div class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">string</span>&amp; <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> name_; &#125;</div><div class="line">  <span class="comment">/// @brief returns the layer names</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; layer_names() <span class="keyword">const</span> &#123; <span class="keyword">return</span> layer_names_; &#125;</div><div class="line">  <span class="comment">/// @brief returns the blob names</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; blob_names() <span class="keyword">const</span> &#123; <span class="keyword">return</span> blob_names_; &#125;</div><div class="line">  <span class="comment">/// @brief returns the blobs</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> blobs_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the layers</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; &gt;&amp; layers() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> layers_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the phase: TRAIN or TEST</span></div><div class="line">  <span class="function"><span class="keyword">inline</span> Phase <span class="title">phase</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> phase_; &#125;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * @brief returns the bottom vecs for each layer -- usually you won't</div><div class="line">   *        need this unless you do per-layer checks such as gradients.</div><div class="line">   */</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; bottom_vecs() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> bottom_vecs_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * @brief returns the top vecs for each layer -- usually you won't</div><div class="line">   *        need this unless you do per-layer checks such as gradients.</div><div class="line">   */</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; &gt;&amp; top_vecs() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> top_vecs_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the ids of the top blobs of layer i</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp; top_ids(<span class="keyword">int</span> i) <span class="keyword">const</span> &#123;</div><div class="line">    CHECK_GE(i, <span class="number">0</span>) &lt;&lt; <span class="string">"Invalid layer id"</span>;</div><div class="line">    CHECK_LT(i, top_id_vecs_.size()) &lt;&lt; <span class="string">"Invalid layer id"</span>;</div><div class="line">    <span class="keyword">return</span> top_id_vecs_[i];</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the ids of the bottom blobs of layer i</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp; bottom_ids(<span class="keyword">int</span> i) <span class="keyword">const</span> &#123;</div><div class="line">    CHECK_GE(i, <span class="number">0</span>) &lt;&lt; <span class="string">"Invalid layer id"</span>;</div><div class="line">    CHECK_LT(i, bottom_id_vecs_.size()) &lt;&lt; <span class="string">"Invalid layer id"</span>;</div><div class="line">    <span class="keyword">return</span> bottom_id_vecs_[i];</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; &gt;&amp; bottom_need_backward() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> bottom_need_backward_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Dtype&gt;&amp; blob_loss_weights() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> blob_loss_weights_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; layer_need_backward() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> layer_need_backward_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the parameters</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> params_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; learnable_params() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> learnable_params_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief returns the learnable parameter learning rate multipliers</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; params_lr() <span class="keyword">const</span> &#123; <span class="keyword">return</span> params_lr_; &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; has_params_lr() <span class="keyword">const</span> &#123; <span class="keyword">return</span> has_params_lr_; &#125;</div><div class="line">  <span class="comment">/// @brief returns the learnable parameter decay multipliers</span></div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; params_weight_decay() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> params_weight_decay_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; has_params_decay() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> has_params_decay_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;&amp; param_names_index() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> param_names_index_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; param_owners() <span class="keyword">const</span> &#123; <span class="keyword">return</span> param_owners_; &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; param_display_names() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> param_display_names_;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">/// @brief Input and output blob numbers</span></div><div class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_inputs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> net_input_blobs_.size(); &#125;</div><div class="line">  <span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_outputs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> net_output_blobs_.size(); &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; input_blobs() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> net_input_blobs_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; output_blobs() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> net_output_blobs_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; input_blob_indices() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> net_input_blob_indices_;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; output_blob_indices() <span class="keyword">const</span> &#123;</div><div class="line">    <span class="keyword">return</span> net_output_blob_indices_;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><code>Net</code>会比<code>Layer</code>的代码复杂度稍微高一些，主要也是对一个net中的每个layer去做初始化操作，所以具体操作都是在layer上做的。重要的是<code>Init()</code>初始化函数，构造函数用它来初始化一个网络，net的<code>Forwward</code>和<code>Backward</code>其实就是调用了其中每个层的<code>Forward</code>和<code>Backward</code>。其中还有一些重要的函数是从已经训练好的网络，来根据layer的名字复制对应的内容，这个操作是比较重要的，因为模型的finetune都与此有关。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/01/Caffe解读5-Layer/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/01/Caffe解读5-Layer/" itemprop="url">
                  Caffe解读5 -- Layer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-07-01T12:02:54+08:00">
              2016-07-01
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-07T19:03:37+08:00">
              2017-03-07
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/01/Caffe解读5-Layer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/07/01/Caffe解读5-Layer/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/07/01/Caffe解读5-Layer/" class="leancloud_visitors" data-flag-title="Caffe解读5 -- Layer">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul>
<li>Caffe通过层来定义网络，Layer是一个模型的基础，也是基本的计算单元，Layer的操作对象就是<a href="https://satisfie.github.io/2016/06/15/Caffe%E8%A7%A3%E8%AF%BB4-Blob/" target="_blank" rel="external">上篇介绍的Blob</a>。</li>
<li>Caffe 的设计强调模块化，因此每个Layer完成一类特定的计算</li>
<li><p>每个layer连接bottom层当做输入，连接top层当做输出，主要定义了3个重要的计算：<code>LayerSetup</code>,<code>Forward</code>,<code>Backward</code>。</p>
<ul>
<li><code>LayerSetup</code>:   在模型初始化的时候对layer进行初始化以及它的输入输出连接</li>
<li><code>Forward</code>: 前向传递，从bottom给定的输入计算输出，输出到top层</li>
<li><p><code>Backward</code>: 反向传播误差，通过top层的误差梯度输入，计算bottom层的误差梯度 </p>
<p>如果你自己写新的layer，也必须实现这3个函数，对于<code>Forward</code>和<code>Backward</code>要实现对应的cpu版本和gpu版本。</p>
</li>
</ul>
</li>
<li><p>Caffe的Layer主要包括以下几个大类：</p>
<ul>
<li>Data Layers</li>
<li>Vision Layers</li>
<li>Recurrent Layers</li>
<li>Common Layers</li>
<li>Normalization Layers</li>
<li>Activation/Neuron Layers</li>
<li>Utility Layers</li>
<li>Loss Layers</li>
</ul>
</li>
</ul>
<p>这篇文章首先介绍对于Layer的基础定义，关于具体的layer的解读，放到后面进行。layer.hpp定义了一个基类，是各种layer的基础，所以我们本章主要对layer进行分析。主要分析了caffe.proto,layer.hpp和layer.cpp文件。</p>
<h1 id="Message-LayerParameter"><a href="#Message-LayerParameter" class="headerlink" title="Message LayerParameter"></a>Message LayerParameter</h1><p>caffe.proto中定义了层的参数<code>LayerParameter</code>,每个参数还是比较浅显易懂的~对于Protocol Buffer的了解，可以参阅前面的文章<a href="https://satisfie.github.io/2016/06/08/Caffe%E8%A7%A3%E8%AF%BB2-Protocol-Buffers/" target="_blank" rel="external">Caffe解读2 – Protocol Buffers</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// NOTE</span></div><div class="line"><span class="comment">// Update the next available ID when you add a new LayerParameter field.</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// LayerParameter next available layer-specific ID: 147 (last added: recurrent_param)</span></div><div class="line">message LayerParameter &#123;</div><div class="line">  optional <span class="built_in">string</span> name = <span class="number">1</span>; <span class="comment">// the layer name  </span></div><div class="line">  optional <span class="built_in">string</span> type = <span class="number">2</span>; <span class="comment">// the layer type  </span></div><div class="line">  repeated <span class="built_in">string</span> bottom = <span class="number">3</span>; <span class="comment">// the name of each bottom blob</span></div><div class="line">  repeated <span class="built_in">string</span> top = <span class="number">4</span>; <span class="comment">// the name of each top blob</span></div><div class="line"></div><div class="line">  <span class="comment">// The train / test phase for computation.</span></div><div class="line">  optional Phase phase = <span class="number">10</span>;</div><div class="line"></div><div class="line">  <span class="comment">// The amount of weight to assign each top blob in the objective.</span></div><div class="line">  <span class="comment">// Each layer assigns a default value, usually of either 0 or 1,</span></div><div class="line">  <span class="comment">// to each top blob.</span></div><div class="line">  repeated <span class="keyword">float</span> loss_weight = <span class="number">5</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Specifies training parameters (multipliers on global learning constants,</span></div><div class="line">  <span class="comment">// and the name and other settings used for weight sharing).</span></div><div class="line">  repeated ParamSpec param = <span class="number">6</span>;</div><div class="line"></div><div class="line">  <span class="comment">// The blobs containing the numeric parameters of the layer.</span></div><div class="line">  repeated BlobProto blobs = <span class="number">7</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Specifies whether to backpropagate to each bottom. If unspecified,</span></div><div class="line">  <span class="comment">// Caffe will automatically infer whether each input needs backpropagation</span></div><div class="line">  <span class="comment">// to compute parameter gradients. If set to true for some inputs,</span></div><div class="line">  <span class="comment">// backpropagation to those inputs is forced; if set false for some inputs,</span></div><div class="line">  <span class="comment">// backpropagation to those inputs is skipped.</span></div><div class="line">  <span class="comment">//</span></div><div class="line">  <span class="comment">// The size must be either 0 or equal to the number of bottoms.</span></div><div class="line">  repeated <span class="keyword">bool</span> propagate_down = <span class="number">11</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Rules controlling whether and when a layer is included in the network,</span></div><div class="line">  <span class="comment">// based on the current NetState.  You may specify a non-zero number of rules</span></div><div class="line">  <span class="comment">// to include OR exclude, but not both.  If no include or exclude rules are</span></div><div class="line">  <span class="comment">// specified, the layer is always included.  If the current NetState meets</span></div><div class="line">  <span class="comment">// ANY (i.e., one or more) of the specified rules, the layer is</span></div><div class="line">  <span class="comment">// included/excluded.</span></div><div class="line">  repeated NetStateRule include = <span class="number">8</span>;</div><div class="line">  repeated NetStateRule exclude = <span class="number">9</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Parameters for data pre-processing.</span></div><div class="line">  optional TransformationParameter transform_param = <span class="number">100</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Parameters shared by loss layers.</span></div><div class="line">  optional LossParameter loss_param = <span class="number">101</span>;</div><div class="line"></div><div class="line">  <span class="comment">// Layer type-specific parameters.</span></div><div class="line">  <span class="comment">//</span></div><div class="line">  <span class="comment">// Note: certain layers may have more than one computational engine</span></div><div class="line">  <span class="comment">// for their implementation. These layers include an Engine type and</span></div><div class="line">  <span class="comment">// engine parameter for selecting the implementation.</span></div><div class="line">  <span class="comment">// The default for the engine is set by the ENGINE switch at compile-time.</span></div><div class="line">  optional AccuracyParameter accuracy_param = <span class="number">102</span>;</div><div class="line">  optional ArgMaxParameter argmax_param = <span class="number">103</span>;</div><div class="line">  optional BatchNormParameter batch_norm_param = <span class="number">139</span>;</div><div class="line">  optional BiasParameter bias_param = <span class="number">141</span>;</div><div class="line">  optional ConcatParameter concat_param = <span class="number">104</span>;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = <span class="number">105</span>;</div><div class="line">  optional ConvolutionParameter convolution_param = <span class="number">106</span>;</div><div class="line">  optional CropParameter crop_param = <span class="number">144</span>;</div><div class="line">  optional DataParameter data_param = <span class="number">107</span>;</div><div class="line">  optional DropoutParameter dropout_param = <span class="number">108</span>;</div><div class="line">  optional DummyDataParameter dummy_data_param = <span class="number">109</span>;</div><div class="line">  optional EltwiseParameter eltwise_param = <span class="number">110</span>;</div><div class="line">  optional ELUParameter elu_param = <span class="number">140</span>;</div><div class="line">  optional EmbedParameter embed_param = <span class="number">137</span>;</div><div class="line">  optional ExpParameter exp_param = <span class="number">111</span>;</div><div class="line">  optional FlattenParameter flatten_param = <span class="number">135</span>;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = <span class="number">112</span>;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = <span class="number">113</span>;</div><div class="line">  optional HingeLossParameter hinge_loss_param = <span class="number">114</span>;</div><div class="line">  optional ImageDataParameter image_data_param = <span class="number">115</span>;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = <span class="number">116</span>;</div><div class="line">  optional InnerProductParameter inner_product_param = <span class="number">117</span>;</div><div class="line">  optional InputParameter input_param = <span class="number">143</span>;</div><div class="line">  optional LogParameter log_param = <span class="number">134</span>;</div><div class="line">  optional LRNParameter lrn_param = <span class="number">118</span>;</div><div class="line">  optional MemoryDataParameter memory_data_param = <span class="number">119</span>;</div><div class="line">  optional MVNParameter mvn_param = <span class="number">120</span>;</div><div class="line">  optional ParameterParameter parameter_param = <span class="number">145</span>;</div><div class="line">  optional PoolingParameter pooling_param = <span class="number">121</span>;</div><div class="line">  optional PowerParameter power_param = <span class="number">122</span>;</div><div class="line">  optional PReLUParameter prelu_param = <span class="number">131</span>;</div><div class="line">  optional PythonParameter python_param = <span class="number">130</span>;</div><div class="line">  optional RecurrentParameter recurrent_param = <span class="number">146</span>;</div><div class="line">  optional ReductionParameter reduction_param = <span class="number">136</span>;</div><div class="line">  optional ReLUParameter relu_param = <span class="number">123</span>;</div><div class="line">  optional ReshapeParameter reshape_param = <span class="number">133</span>;</div><div class="line">  optional ScaleParameter scale_param = <span class="number">142</span>;</div><div class="line">  optional SigmoidParameter sigmoid_param = <span class="number">124</span>;</div><div class="line">  optional SoftmaxParameter softmax_param = <span class="number">125</span>;</div><div class="line">  optional SPPParameter spp_param = <span class="number">132</span>;</div><div class="line">  optional SliceParameter slice_param = <span class="number">126</span>;</div><div class="line">  optional TanHParameter tanh_param = <span class="number">127</span>;</div><div class="line">  optional ThresholdParameter threshold_param = <span class="number">128</span>;</div><div class="line">  optional TileParameter tile_param = <span class="number">138</span>;</div><div class="line">  optional WindowDataParameter window_data_param = <span class="number">129</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体的layer的参数定义可以在protocal buffer中找到，例如</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">message ConvolutionParameter &#123;</div><div class="line">  optional uint32 num_output = <span class="number">1</span>; <span class="comment">// The number of outputs for the layer</span></div><div class="line">  optional <span class="keyword">bool</span> bias_term = <span class="number">2</span> [<span class="keyword">default</span> = <span class="literal">true</span>]; <span class="comment">// whether to have bias terms</span></div><div class="line"></div><div class="line">  <span class="comment">// Pad, kernel size, and stride are all given as a single value for equal</span></div><div class="line">  <span class="comment">// dimensions in all spatial dimensions, or once per spatial dimension.</span></div><div class="line">  repeated uint32 pad = <span class="number">3</span>; <span class="comment">// The padding size; defaults to 0</span></div><div class="line">  repeated uint32 kernel_size = <span class="number">4</span>; <span class="comment">// The kernel size</span></div><div class="line">  repeated uint32 stride = <span class="number">6</span>; <span class="comment">// The stride; defaults to 1</span></div><div class="line">  <span class="comment">// Factor used to dilate the kernel, (implicitly) zero-filling the resulting</span></div><div class="line">  <span class="comment">// holes. (Kernel dilation is sometimes referred to by its use in the</span></div><div class="line">  <span class="comment">// algorithme à trous from Holschneider et al. 1987.)</span></div><div class="line">  repeated uint32 dilation = <span class="number">18</span>; <span class="comment">// The dilation; defaults to 1</span></div><div class="line"></div><div class="line">  <span class="comment">// For 2D convolution only, the *_h and *_w versions may also be used to</span></div><div class="line">  <span class="comment">// specify both spatial dimensions.</span></div><div class="line">  optional uint32 pad_h = <span class="number">9</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding height (2D only)</span></div><div class="line">  optional uint32 pad_w = <span class="number">10</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding width (2D only)</span></div><div class="line">  optional uint32 kernel_h = <span class="number">11</span>; <span class="comment">// The kernel height (2D only)</span></div><div class="line">  optional uint32 kernel_w = <span class="number">12</span>; <span class="comment">// The kernel width (2D only)</span></div><div class="line">  optional uint32 stride_h = <span class="number">13</span>; <span class="comment">// The stride height (2D only)</span></div><div class="line">  optional uint32 stride_w = <span class="number">14</span>; <span class="comment">// The stride width (2D only)</span></div><div class="line"></div><div class="line">  optional uint32 group = <span class="number">5</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// The group size for group conv</span></div><div class="line"></div><div class="line">  optional FillerParameter weight_filler = <span class="number">7</span>; <span class="comment">// The filler for the weight</span></div><div class="line">  optional FillerParameter bias_filler = <span class="number">8</span>; <span class="comment">// The filler for the bias</span></div><div class="line">  <span class="keyword">enum</span> Engine &#123;</div><div class="line">    DEFAULT = <span class="number">0</span>;</div><div class="line">    CAFFE = <span class="number">1</span>;</div><div class="line">    CUDNN = <span class="number">2</span>;</div><div class="line">  &#125;</div><div class="line">  optional Engine engine = <span class="number">15</span> [<span class="keyword">default</span> = DEFAULT];</div><div class="line"></div><div class="line">  <span class="comment">// The axis to interpret as "channels" when performing convolution.</span></div><div class="line">  <span class="comment">// Preceding dimensions are treated as independent inputs;</span></div><div class="line">  <span class="comment">// succeeding dimensions are treated as "spatial".</span></div><div class="line">  <span class="comment">// With (N, C, H, W) inputs, and axis == 1 (the default), we perform</span></div><div class="line">  <span class="comment">// N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</span></div><div class="line">  <span class="comment">// groups g&gt;1) filters across the spatial axes (H, W) of the input.</span></div><div class="line">  <span class="comment">// With (N, C, D, H, W) inputs, and axis == 1, we perform</span></div><div class="line">  <span class="comment">// N independent 3D convolutions, sliding (C/g)-channels</span></div><div class="line">  <span class="comment">// filters across the spatial axes (D, H, W) of the input.</span></div><div class="line">  optional int32 axis = <span class="number">16</span> [<span class="keyword">default</span> = <span class="number">1</span>];</div><div class="line"></div><div class="line">  <span class="comment">// Whether to force use of the general ND convolution, even if a specific</span></div><div class="line">  <span class="comment">// implementation for blobs of the appropriate number of spatial dimensions</span></div><div class="line">  <span class="comment">// is available. (Currently, there is only a 2D-specific convolution</span></div><div class="line">  <span class="comment">// implementation; for input blobs with num_axes != 2, this option is</span></div><div class="line">  <span class="comment">// ignored and the ND implementation will be used.)</span></div><div class="line">  optional <span class="keyword">bool</span> force_nd_im2col = <span class="number">17</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Layer类的具体实现"><a href="#Layer类的具体实现" class="headerlink" title="Layer类的具体实现"></a>Layer类的具体实现</h1><h2 id="protected变量和private变量"><a href="#protected变量和private变量" class="headerlink" title="protected变量和private变量"></a><code>protected</code>变量和<code>private</code>变量</h2><ul>
<li>layer_param_:存储了层的参数</li>
<li>phase: 指明当前网络是train阶段还是test阶段</li>
<li>blobs: Blob指针的vector,每个指针都存储可学习的参数</li>
<li>param_propagate_down_: 用来指明该层的梯度是否继续反向传播</li>
<li>loss_: 用来指示是否top层在目标函数中的非零权重</li>
<li>is_shared_: 该层是否被其他网络共享</li>
<li>forward_mutex_: 若该layer被shared，则需要这个mutex序列保持forward过程的正常运行</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="comment">/** The protobuf that stores the layer parameters */</span></div><div class="line"> LayerParameter layer_param_;</div><div class="line"> <span class="comment">/** The phase: TRAIN or TEST */</span></div><div class="line"> Phase phase_;</div><div class="line"> <span class="comment">/** The vector that stores the learnable parameters as a set of blobs. */</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;</div><div class="line"> <span class="comment">/** Vector indicating whether to compute the diff of each param blob. */</span></div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; param_propagate_down_;</div><div class="line"></div><div class="line"> <span class="comment">/** The vector that indicates whether each top blob has a non-zero weight in</span></div><div class="line">  *  the objective function. */</div><div class="line"> <span class="built_in">vector</span>&lt;Dtype&gt; loss_;</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line"> <span class="comment">/** Whether this layer is actually shared by other nets*/</span></div><div class="line"> <span class="keyword">bool</span> is_shared_;</div><div class="line"></div><div class="line"> <span class="comment">/** The mutex for sequential forward if this layer is shared */</span></div><div class="line"> <span class="built_in">shared_ptr</span>&lt;boost::mutex&gt; forward_mutex_;</div></pre></td></tr></table></figure>
<h2 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">explicit Layer(const LayerParameter&amp; param)</div><div class="line">  : layer_param_(param), is_shared_(false) &#123;</div><div class="line">    // Set phase and copy blobs (if there are any).</div><div class="line">    phase_ = param.phase();</div><div class="line">    if (layer_param_.blobs_size() &gt; 0) &#123;</div><div class="line">      blobs_.resize(layer_param_.blobs_size());</div><div class="line">      for (int i = 0; i &lt; layer_param_.blobs_size(); ++i) &#123;</div><div class="line">        blobs_[i].reset(new Blob&lt;Dtype&gt;());</div><div class="line">        blobs_[i]-&gt;FromProto(layer_param_.blobs(i));</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>首先，设置网络当前的phase，是train还是test,然后用layer_param_中的参数对blob_指针初始化。</p>
<h2 id="SetUp函数"><a href="#SetUp函数" class="headerlink" title="SetUp函数"></a><code>SetUp</code>函数</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Implements common layer setup functionality.</div><div class="line"> *</div><div class="line"> * @param bottom the preshaped input blobs</div><div class="line"> * @param top</div><div class="line"> *     the allocated but unshaped output blobs, to be shaped by Reshape</div><div class="line"> *</div><div class="line"> * Checks that the number of bottom and top blobs is correct.</div><div class="line"> * Calls LayerSetUp to do special layer setup for individual layer types,</div><div class="line"> * followed by Reshape to set up sizes of top blobs and internal buffers.</div><div class="line"> * Sets up the loss weight multiplier blobs for any non-zero loss weights.</div><div class="line"> * This method may not be overridden.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  InitMutex();</div><div class="line">  CheckBlobCounts(bottom, top);</div><div class="line">  LayerSetUp(bottom, top);</div><div class="line">  Reshape(bottom, top);</div><div class="line">  SetLossWeights(top);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在<code>SetUp</code>函数中，实现了通用层的功能设置，bottom是已经reshaped后的输入，top是分配了内存但没有reshape的输出blobs。首先核对bottom和top的维度，然后调用<code>LayerSetUp</code>进行层的初始化，然后调用<br><code>Reshape</code>对top层和中间buffer层进行reshape,最后设置loss权重。</p>
<h2 id="LayerSetUp虚函数"><a href="#LayerSetUp虚函数" class="headerlink" title="LayerSetUp虚函数"></a><code>LayerSetUp</code>虚函数</h2><p><code>LayerSetUp</code>的实现，这个函数专门用来初始化层，在前向传递前，读取相应的layer_param_<br>参数，使用Reshape对top层和中间buffers层进行reshape。这是一个抽象虚函数，具体的实现要在子类中体现。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Does layer-specific setup: your layer should implement this function</div><div class="line"> *        as well as Reshape.</div><div class="line"> *</div><div class="line"> * @param bottom</div><div class="line"> *     the preshaped input blobs, whose data fields store the input data for</div><div class="line"> *     this layer</div><div class="line"> * @param top</div><div class="line"> *     the allocated but unshaped output blobs</div><div class="line"> *</div><div class="line"> * This method should do one-time layer specific setup. This includes reading</div><div class="line"> * and processing relevent parameters from the &lt;code&gt;layer_param_&lt;/code&gt;.</div><div class="line"> * Setting up the shapes of top blobs and internal buffers should be done in</div><div class="line"> * &lt;code&gt;Reshape&lt;/code&gt;, which will be called before the forward pass to</div><div class="line"> * adjust the top blob sizes.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">LayerSetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape虚函数"><a href="#Reshape虚函数" class="headerlink" title="Reshape虚函数"></a><code>Reshape</code>虚函数</h2><p><code>Reshape</code>函数也是虚函数，用来对top层和中间buffers层进行reshape。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Adjust the shapes of top blobs and internal buffers to accommodate</div><div class="line"> *        the shapes of the bottom blobs.</div><div class="line"> *</div><div class="line"> * @param bottom the input blobs, with the requested input shapes</div><div class="line"> * @param top the top blobs, which should be reshaped as needed</div><div class="line"> *</div><div class="line"> * This method should reshape top blobs as needed according to the shapes</div><div class="line"> * of the bottom (input) blobs, as well as reshaping any internal buffers</div><div class="line"> * and making any other necessary adjustments so that the layer can</div><div class="line"> * accommodate the bottom blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Reshape</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = <span class="number">0</span>;</div></pre></td></tr></table></figure>
<h2 id="Forward函数"><a href="#Forward函数" class="headerlink" title="Forward函数"></a><code>Forward</code>函数</h2><p><code>Forward</code>函数进行前向传播，计算top blobs和loss，这是一个wrapper函数，调用相应的<code>Forward_cpu</code>和<code>Forward_gpu</code>函数。<br>不同的层应该实现对应的<code>Forward_cpu</code>和<code>Forward_gpu</code>，而不需要改变<code>Forward</code>函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Forward and backward wrappers. You should implement the cpu and</span></div><div class="line"><span class="comment">// gpu specific implementations instead, and should not change these</span></div><div class="line"><span class="comment">// functions.</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">inline</span> Dtype Layer&lt;Dtype&gt;::Forward(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">// Lock during forward to ensure sequential forward</span></div><div class="line">  Lock();</div><div class="line">  Dtype loss = <span class="number">0</span>;</div><div class="line">  Reshape(bottom, top);</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    Forward_cpu(bottom, top);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</div><div class="line">      <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;loss(top_id)) &#123; <span class="keyword">continue</span>; &#125;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = top[top_id]-&gt;count();</div><div class="line">      <span class="keyword">const</span> Dtype* data = top[top_id]-&gt;cpu_data();</div><div class="line">      <span class="keyword">const</span> Dtype* loss_weights = top[top_id]-&gt;cpu_diff();</div><div class="line">      loss += caffe_cpu_dot(count, data, loss_weights);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    Forward_gpu(bottom, top);</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</div><div class="line">      <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;loss(top_id)) &#123; <span class="keyword">continue</span>; &#125;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = top[top_id]-&gt;count();</div><div class="line">      <span class="keyword">const</span> Dtype* data = top[top_id]-&gt;gpu_data();</div><div class="line">      <span class="keyword">const</span> Dtype* loss_weights = top[top_id]-&gt;gpu_diff();</div><div class="line">      Dtype blob_loss = <span class="number">0</span>;</div><div class="line">      caffe_gpu_dot(count, data, loss_weights, &amp;blob_loss);</div><div class="line">      loss += blob_loss;</div><div class="line">    &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">  Unlock();</div><div class="line">  <span class="keyword">return</span> loss;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** @brief Using the CPU device, compute the layer output. */</span></div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = <span class="number">0</span>;</div><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Using the GPU device, compute the layer output.</div><div class="line"> *        Fall back to Forward_cpu() if unavailable.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Forward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></div><div class="line">  <span class="keyword">return</span> Forward_cpu(bottom, top);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Backward函数"><a href="#Backward函数" class="headerlink" title="Backward函数"></a><code>Backward</code>函数</h2><p><code>Backward</code>函数根据给定的top层误差梯度，计算bottom层的误差梯度，实现梯度反向传播。内部调用相应的<code>Backward_cpu</code>和<code>Backward_gpu</code>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> Layer&lt;Dtype&gt;::Backward(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    Backward_cpu(top, propagate_down, bottom);</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    Backward_gpu(top, propagate_down, bottom);</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对应的<code>Backward_cpu</code>和<code>Backward_gpu</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Using the CPU device, compute the gradients for any parameters and</div><div class="line"> *        for the bottom blobs if propagate_down is true.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_cpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) = <span class="number">0</span>;</div><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Using the GPU device, compute the gradients for any parameters and</div><div class="line"> *        for the bottom blobs if propagate_down is true.</div><div class="line"> *        Fall back to Backward_cpu() if unavailable.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Backward_gpu</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="comment">// LOG(WARNING) &lt;&lt; "Using CPU code as backup.";</span></div><div class="line">  Backward_cpu(top, propagate_down, bottom);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h2><h3 id="blobs"><a href="#blobs" class="headerlink" title="blobs()"></a><code>blobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the vector of learnable parameter blobs.</div><div class="line"> */</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blobs() &#123;</div><div class="line">  <span class="keyword">return</span> blobs_;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="layer-param"><a href="#layer-param" class="headerlink" title="layer_param()"></a><code>layer_param()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the layer parameter.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">const</span> LayerParameter&amp; <span class="title">layer_param</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> layer_param_; &#125;</div></pre></td></tr></table></figure>
<h3 id="type"><a href="#type" class="headerlink" title="type()"></a><code>type()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the layer type.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">""</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="ToProto"><a href="#ToProto" class="headerlink" title="ToProto()"></a><code>ToProto()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Writes the layer parameter to a protocol buffer</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(LayerParameter* param, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span></span>;</div></pre></td></tr></table></figure>
<h3 id="loss-int-top-index"><a href="#loss-int-top-index" class="headerlink" title="loss(int top_index)"></a><code>loss(int top_index)</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the scalar loss associated with a top blob at a given index.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> (loss_.size() &gt; top_index) ? loss_[top_index] : Dtype(<span class="number">0</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="set-loss-const-int-top-index-const-Dtype-value"><a href="#set-loss-const-int-top-index-const-Dtype-value" class="headerlink" title="set_loss(const int top_index, const Dtype value)"></a>set_loss(const int top_index, const Dtype value)</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Sets the loss associated with a top blob at a given index.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_loss</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> top_index, <span class="keyword">const</span> Dtype value)</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (loss_.size() &lt;= top_index) &#123;</div><div class="line">    loss_.resize(top_index + <span class="number">1</span>, Dtype(<span class="number">0</span>));</div><div class="line">  &#125;</div><div class="line">  loss_[top_index] = value;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="type-1"><a href="#type-1" class="headerlink" title="type()"></a><code>type()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the layer type.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">const</span> <span class="keyword">char</span>* <span class="title">type</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="string">""</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="ExactNumBottomBlobs"><a href="#ExactNumBottomBlobs" class="headerlink" title="ExactNumBottomBlobs()"></a><code>ExactNumBottomBlobs()</code></h3><p>返回具体的bottom blob数目</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the exact number of bottom blobs required by the layer,</div><div class="line"> *        or -1 if no exact number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some exact number of bottom blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="MinBottomBlobs"><a href="#MinBottomBlobs" class="headerlink" title="MinBottomBlobs()"></a><code>MinBottomBlobs()</code></h3><p>返回layer需要的最小的bottom blob个数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the minimum number of bottom blobs required by the layer,</div><div class="line"> *        or -1 if no minimum number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some minimum number of bottom blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="MaxBottomBlobs"><a href="#MaxBottomBlobs" class="headerlink" title="MaxBottomBlobs()"></a><code>MaxBottomBlobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the maximum number of bottom blobs required by the layer,</div><div class="line"> *        or -1 if no maximum number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some maximum number of bottom blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxBottomBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="ExactNumTopBlobs"><a href="#ExactNumTopBlobs" class="headerlink" title="ExactNumTopBlobs()"></a><code>ExactNumTopBlobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the exact number of top blobs required by the layer,</div><div class="line"> *        or -1 if no exact number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some exact number of top blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">ExactNumTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="MinTopBlobs"><a href="#MinTopBlobs" class="headerlink" title="MinTopBlobs()"></a><code>MinTopBlobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the minimum number of top blobs required by the layer,</div><div class="line"> *        or -1 if no minimum number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some minimum number of top blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MinTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="MaxTopBlobs"><a href="#MaxTopBlobs" class="headerlink" title="MaxTopBlobs()"></a><code>MaxTopBlobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns the maximum number of top blobs required by the layer,</div><div class="line"> *        or -1 if no maximum number is required.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return a non-negative value if your</div><div class="line"> * layer expects some maximum number of top blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">MaxTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="number">-1</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="EqualNumBottomTopBlobs"><a href="#EqualNumBottomTopBlobs" class="headerlink" title="EqualNumBottomTopBlobs()"></a><code>EqualNumBottomTopBlobs()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Returns true if the layer requires an equal number of bottom and</div><div class="line"> *        top blobs.</div><div class="line"> *</div><div class="line"> * This method should be overridden to return true if your layer expects an</div><div class="line"> * equal number of bottom and top blobs.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">EqualNumBottomTopBlobs</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</div></pre></td></tr></table></figure>
<h3 id="param-propagate-down"><a href="#param-propagate-down" class="headerlink" title="param_propagate_down()"></a><code>param_propagate_down()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Specifies whether the layer should compute gradients w.r.t. a</div><div class="line"> *        parameter at a particular index given by param_id.</div><div class="line"> *</div><div class="line"> * You can safely ignore false values and always compute gradients</div><div class="line"> * for all parameters, but possibly with wasteful computation.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> (param_propagate_down_.size() &gt; param_id) ?</div><div class="line">      param_propagate_down_[param_id] : <span class="literal">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="set-param-propagate-down"><a href="#set-param-propagate-down" class="headerlink" title="set_param_propagate_down()"></a><code>set_param_propagate_down()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief Sets whether the layer should compute gradients w.r.t. a</div><div class="line"> *        parameter at a particular index given by param_id.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">set_param_propagate_down</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> param_id, <span class="keyword">const</span> <span class="keyword">bool</span> value)</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (param_propagate_down_.size() &lt;= param_id) &#123;</div><div class="line">    param_propagate_down_.resize(param_id + <span class="number">1</span>, <span class="literal">true</span>);</div><div class="line">  &#125;</div><div class="line">  param_propagate_down_[param_id] = value;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="SetLossWeights函数"><a href="#SetLossWeights函数" class="headerlink" title="SetLossWeights函数"></a><code>SetLossWeights</code>函数</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Called by SetUp to initialize the weights associated with any top blobs in</div><div class="line"> * the loss function. Store non-zero loss weights in the diff blob.</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">SetLossWeights</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)</span> </span>&#123;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> num_loss_weights = layer_param_.loss_weight_size();</div><div class="line">  <span class="keyword">if</span> (num_loss_weights) &#123;</div><div class="line">    CHECK_EQ(top.size(), num_loss_weights) &lt;&lt; <span class="string">"loss_weight must be "</span></div><div class="line">        <span class="string">"unspecified or specified once per top blob."</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; top.size(); ++top_id) &#123;</div><div class="line">      <span class="keyword">const</span> Dtype loss_weight = layer_param_.loss_weight(top_id);</div><div class="line">      <span class="keyword">if</span> (loss_weight == Dtype(<span class="number">0</span>)) &#123; <span class="keyword">continue</span>; &#125;</div><div class="line">      <span class="keyword">this</span>-&gt;set_loss(top_id, loss_weight);</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> count = top[top_id]-&gt;count();</div><div class="line">      Dtype* loss_multiplier = top[top_id]-&gt;mutable_cpu_diff();</div><div class="line">      caffe_set(count, loss_weight, loss_multiplier);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="loss-weight分析："><a href="#loss-weight分析：" class="headerlink" title="loss_weight分析："></a>loss_weight分析：</h3><p>loss_weight一般只对带有loss的层有意义。一个网络可以有多个loss层，而且每个loss层可以计算多个loss（每个loss放在单独的top blob中，这种情况在caffe中没出现过，caffe的loss层一般只有一个loss，放在top[0]对应的blob里）。整个网络的损失就是这些loss的和，loss_weight指的就是求和过程中每个loss的权重，可以在定义layer的时候加上loss_weight这个参数来指定，设置的时候该层loss_weight的个数需要与top的个数相同，否则就不设置。默认情况下，普通层没有loss_weight, loss层的loss_weight为1。<br>对于caffe中的loss层，计算出的loss放在top[0]指向的blob里，该blob里的diff存放的就是loss对应的loss_weight，传递过程在Layer的SetUp里通过调用SetLossWeights(top)完成</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/06/15/Caffe解读4-Blob/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/15/Caffe解读4-Blob/" itemprop="url">
                  Caffe解读4 -- Blob
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-06-15T11:53:20+08:00">
              2016-06-15
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-07T11:54:13+08:00">
              2017-03-07
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/15/Caffe解读4-Blob/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/06/15/Caffe解读4-Blob/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/06/15/Caffe解读4-Blob/" class="leancloud_visitors" data-flag-title="Caffe解读4 -- Blob">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul>
<li><p><code>Blob</code>在Caffe中是对于SyncedMemory的封装，Blob是Caffe中保存数据的类，是在各个Layer,Net,Solver之间传递的基本计算单元。Caffe内部数据存储和通讯都是通过Blob来完成，Blob提供统一的存储操作接口，可用来保存训练数据和模型参数等。</p>
</li>
<li><p>在Blob中主要定义了关于数据<code>data_</code>和梯度<code>diff_</code>以及相关的一系列方法,使用的变量都是<code>SyncedMemory</code>的智能指针，所以在解读Blob之前，需要先看上一篇的</p>
<p>  <a href="https://satisfie.github.io/2016/06/12/Caffe%E8%A7%A3%E8%AF%BB3-SyncedMemory/" target="_blank" rel="external">Caffe解读3 – SyncedMemory</a></p>
</li>
</ul>
<h1 id="模块说明"><a href="#模块说明" class="headerlink" title="模块说明"></a>模块说明</h1><p>Blob是一个N维连续数组。批处理图像数据时通常使用4维Blob，Blob的维度可以表示为(N, K, H, W)，每个维度的意思分别是：</p>
<ul>
<li>N：数据的个数，例如batch_size的大小</li>
<li>K：如果是图像，可以理解为通道数量，网络中间可以理解为feature map的数量</li>
<li>H,W: 图像或者滤波器的高度和宽度</li>
</ul>
<p>Blob中数据是row-major存储的，W是变化最快的维度，例如在(n, k, h, w)处的数据，其物理偏移量计算方式为：</p>
<p>$$<br>((n<em>K+k)</em>H+h)*W+w<br>$$</p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="1-私有保护变量"><a href="#1-私有保护变量" class="headerlink" title="1.私有保护变量"></a>1.私有保护变量</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span>:</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</div><div class="line"> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</div><div class="line"> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_;</div><div class="line"> <span class="keyword">int</span> count_;</div><div class="line"> <span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure>
<p>shared_ptr是智能指针，使用引用计数，当计数为0时，自动释放内存。其中</p>
<ul>
<li>data_ : 用来存放正向传播时的数据</li>
<li>diff_ : 用来存放反向传播时的梯度</li>
<li>shape_data_: 用来存储Blob的形状数据的SyncedMemor智能指针</li>
<li>shape_ : 用来存储Blob的形状数据</li>
<li>count_ : 表示Blob中的元素个数，等于 $num \times channels \times height \times width$</li>
<li>capacity_ : 表示Blob的容量</li>
</ul>
<h2 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h2><p>总共声明和实现了3种构造函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Blob(): data_(), diff_(), count_(<span class="number">0</span>), capacity_(<span class="number">0</span>) &#123;&#125;</div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width);   </div><div class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</div></pre></td></tr></table></figure>
<p>explict关键字可以防止构造函数的隐式转换,构造函数的实现主要是调用了<code>Reshape</code>函数来完成data_和diff_的共享内存对象SyncedMemory的申请</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(num, channels, height, width);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</div><div class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></div><div class="line">  : capacity_(<span class="number">0</span>) &#123;</div><div class="line">  Reshape(shape);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Reshape函数"><a href="#Reshape函数" class="headerlink" title="Reshape函数"></a><code>Reshape</code>函数</h2><p><code>Reshape</code>能够被调用用来实现 （1）内存的初始化分配；（2）在<code>Layer::Reshape</code>或者<code>Layer::Forward</code>时 用来调节top blob的维度。 当改变blob的size时，只有当原来的内存已经不够了才会重新分配，而创建后多余的内存是不会释放的。需要注意的是，当对输入blob进行<code>reshape</code>时，不能马上调用<code>Net::Backward</code>,因为需要在进行<br><code>reshape</code>后需要调用<code>Net::Forward</code>或者<code>Net::Reshape</code>将新的输入shape传递到更高的层。</p>
<p>Reshape成员函数有4种:</p>
<ul>
<li><code>void Reshape(const int num, const int channels, const int height,const int width);</code></li>
<li><code>void Reshape(const vector&lt;int&gt;&amp; shape);</code></li>
<li><code>void Reshape(const BlobShape&amp; shape);</code></li>
<li><code>void ReshapeLike(const Blob&amp; other);</code></li>
</ul>
<h3 id="Reshape成员函数1"><a href="#Reshape成员函数1" class="headerlink" title="Reshape成员函数1"></a><code>Reshape成员函数1</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接用num,channels,height,width来完成reshape,调用了Reshape(const vector&lt;int&gt; &amp;shape)</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width) &#123;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape(<span class="number">4</span>);</div><div class="line">  shape[<span class="number">0</span>] = num;</div><div class="line">  shape[<span class="number">1</span>] = channels;</div><div class="line">  shape[<span class="number">2</span>] = height;</div><div class="line">  shape[<span class="number">3</span>] = width;</div><div class="line">  Reshape(shape); <span class="comment">//调用了类型2的Reshape函数</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Reshape成员函数2"><a href="#Reshape成员函数2" class="headerlink" title="Reshape成员函数2"></a><code>Reshape成员函数2</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape) &#123;</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * CHECK_LE 位于logging.h文件 767行</div><div class="line">   * 定义为：</div><div class="line">   * #define CHECK_LE(val1, val2) CHECK_OP(_LE, &lt;=, val1, val2)</div><div class="line">   * 用来检查val1&lt;=val2, 用到了GLOG日志库</div><div class="line">   */</div><div class="line">  CHECK_LE(shape.size(), kMaxBlobAxes); <span class="comment">//kMaxBlobAxes定义为shape参数最大的个数，设定为32</span></div><div class="line">  count_ = <span class="number">1</span>; <span class="comment">//开始初始化时赋值为1，因为后面要乘以shape_中的每个元素值</span></div><div class="line">  shape_.resize(shape.size());<span class="comment">//shape_ 开始初始化</span></div><div class="line"> </div><div class="line">  <span class="comment">//shape_data_的初始化和赋值，它是一个SyncedMemory类指针</span></div><div class="line">  <span class="keyword">if</span> (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)) &#123;</div><div class="line">    shape_data_.reset(<span class="keyword">new</span> SyncedMemory(shape.size() * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">int</span>* shape_data = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>*&gt;(shape_data_-&gt;mutable_cpu_data());<span class="comment">//获得shape_data_的cpu内存地址</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.size(); ++i) &#123;</div><div class="line">    CHECK_GE(shape[i], <span class="number">0</span>); <span class="comment">//检查shape中该参数是否为0</span></div><div class="line">    <span class="keyword">if</span> (count_ != <span class="number">0</span>) &#123;    </div><div class="line">      <span class="comment">//检查乘以shape[i]后，count_是否会超过INT_MAX</span></div><div class="line">      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; <span class="string">"blob size exceeds INT_MAX"</span>; </div><div class="line">    &#125;</div><div class="line">    count_ *= shape[i]; <span class="comment">//统计Blob元素个数= num*channels*height*width</span></div><div class="line">    shape_[i] = shape[i]; <span class="comment">//给成员变量shape_ 赋值</span></div><div class="line">    shape_data[i] = shape[i]; <span class="comment">//给shape_data_赋值</span></div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="comment">//超过容量，设定容量为count_</span></div><div class="line">  <span class="keyword">if</span> (count_ &gt; capacity_) &#123;</div><div class="line">    capacity_ = count_;</div><div class="line">    data_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//data_ 进行内存分配</span></div><div class="line">    diff_.reset(<span class="keyword">new</span> SyncedMemory(capacity_ * <span class="keyword">sizeof</span>(Dtype)));<span class="comment">//diff_ 进行内存分配</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Reshape成员函数3"><a href="#Reshape成员函数3" class="headerlink" title="Reshape成员函数3"></a><code>Reshape成员函数3</code></h3><p><code>Reshape(const BlobShape&amp; shape)</code> 用<code>BlobShape</code>来进行Reshape, <code>BlobShape</code>是在caffe.proto中定义的，用来定义Blob的shape维度的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Specifies the shape (dimensions) of a Blob.</span></div><div class="line">message BlobShape &#123;</div><div class="line">  repeated int64 dim = <span class="number">1</span> [packed = <span class="literal">true</span>];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Reshape(<span class="keyword">const</span> BlobShape&amp; shape) &#123;</div><div class="line">  CHECK_LE(shape.dim_size(), kMaxBlobAxes);</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_vec(shape.dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape.dim_size(); ++i) &#123;</div><div class="line">    shape_vec[i] = shape.dim(i);</div><div class="line">  &#125;</div><div class="line">  Reshape(shape_vec); <span class="comment">//同样调用了Reshape函数2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Reshape成员函数4"><a href="#Reshape成员函数4" class="headerlink" title="Reshape成员函数4"></a><code>Reshape成员函数4</code></h3><p><code>ReshapeLike(const Blob&lt;Dtype&gt;&amp; other)</code>，用其他的Blob参数进行Reshape</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ReshapeLike(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; other) &#123;</div><div class="line">  Reshape(other.shape()); <span class="comment">//同样调用了Reshape函数2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="shape-数据输出函数"><a href="#shape-数据输出函数" class="headerlink" title="shape 数据输出函数"></a><code>shape 数据输出函数</code></h3><p>两个内联函数用来输出shape的形状数据</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="built_in">string</span> <span class="title">shape_string</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="built_in">ostringstream</span> stream;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; shape_.size(); ++i) &#123;</div><div class="line">    stream &lt;&lt; shape_[i] &lt;&lt; <span class="string">" "</span>;</div><div class="line">  &#125;</div><div class="line">  stream &lt;&lt; <span class="string">"("</span> &lt;&lt; count_ &lt;&lt; <span class="string">")"</span>;</div><div class="line">  <span class="keyword">return</span> stream.str();</div><div class="line">&#125;</div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape() <span class="keyword">const</span> &#123; <span class="keyword">return</span> shape_; &#125;</div></pre></td></tr></table></figure>
<h2 id="一些变量访问函数"><a href="#一些变量访问函数" class="headerlink" title="一些变量访问函数"></a>一些变量访问函数</h2><ul>
<li><code>num_axes()</code>: 返回shape_ 的size</li>
<li><code>count()</code>:返回count_</li>
<li><code>CanonicalAxisIndex(int axis_index)</code>：返回规范化的坐标，支持负坐标的访问</li>
<li><code>shape(int index)</code>:返回shape_索引处的值</li>
<li><code>LegacyShape(int index)</code>:内部调用shape(intdex)，多了一些合法性检查</li>
<li><code>num()</code>,<code>channels()</code>,<code>height()</code>,<code>width()</code>:分别返回对应的值</li>
</ul>
<h3 id="num-axes"><a href="#num-axes" class="headerlink" title="num_axes()"></a><code>num_axes()</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num_axes</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> shape_.size(); &#125;</div></pre></td></tr></table></figure>
<h3 id="count"><a href="#count" class="headerlink" title="count()"></a><code>count()</code></h3><p>3种形式的<code>count()</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//直接返回count_</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> count_; &#125; </div><div class="line"><span class="comment">//计算一个片内的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis, <span class="keyword">int</span> end_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">CHECK_LE(start_axis, end_axis);</div><div class="line">CHECK_GE(start_axis, <span class="number">0</span>);</div><div class="line">CHECK_GE(end_axis, <span class="number">0</span>);</div><div class="line">CHECK_LE(start_axis, num_axes());</div><div class="line">CHECK_LE(end_axis, num_axes());</div><div class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = start_axis; i &lt; end_axis; ++i) &#123;</div><div class="line">  count *= shape(i);</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> count;</div><div class="line">&#125;</div><div class="line"><span class="comment">//给定一个起始，计算到最后片的元素个数</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">(<span class="keyword">int</span> start_axis)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">   <span class="keyword">return</span> count(start_axis, num_axes());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="CanonicalAxisIndex"><a href="#CanonicalAxisIndex" class="headerlink" title="CanonicalAxisIndex()"></a><code>CanonicalAxisIndex()</code></h3><p>用来进行坐标的规范化，和Python一样，支持负数的访问。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">CanonicalAxisIndex</span><span class="params">(<span class="keyword">int</span> axis_index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_GE(axis_index, -num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  CHECK_LT(axis_index, num_axes())</div><div class="line">      &lt;&lt; <span class="string">"axis "</span> &lt;&lt; axis_index &lt;&lt; <span class="string">" out of range for "</span> &lt;&lt; num_axes()</div><div class="line">      &lt;&lt; <span class="string">"-D Blob with shape "</span> &lt;&lt; shape_string();</div><div class="line">  <span class="keyword">if</span> (axis_index &lt; <span class="number">0</span>) &#123;</div><div class="line">    <span class="keyword">return</span> axis_index + num_axes();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> axis_index;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="shape-int-index"><a href="#shape-int-index" class="headerlink" title="shape(int index)"></a><code>shape(int index)</code></h3><p>返回索引处shape的值</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">shape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> shape_[CanonicalAxisIndex(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LegacyShape-int-index"><a href="#LegacyShape-int-index" class="headerlink" title="LegacyShape(int index)"></a><code>LegacyShape(int index)</code></h3><p>这个是只针对4维Blob的shape数据访问版本。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">LegacyShape</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(num_axes(), <span class="number">4</span>)</div><div class="line">      &lt;&lt; <span class="string">"Cannot use legacy accessors on Blobs with &gt; 4 axes."</span>;</div><div class="line">  CHECK_LT(index, <span class="number">4</span>);</div><div class="line">  CHECK_GE(index, <span class="number">-4</span>);</div><div class="line">  <span class="keyword">if</span> (index &gt;= num_axes() || index &lt; -num_axes()) &#123;</div><div class="line">    <span class="comment">// Axis is out of range, but still in [0, 3] (or [-4, -1] for reverse</span></div><div class="line">    <span class="comment">// indexing) -- this special case simulates the one-padding used to fill</span></div><div class="line">    <span class="comment">// extraneous axes of legacy blobs.</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape(index);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="其他变量访问函数"><a href="#其他变量访问函数" class="headerlink" title="其他变量访问函数"></a>其他变量访问函数</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor num: use shape(0) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">num</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">0</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor channels: use shape(1) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">channels</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">1</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor height: use shape(2) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">2</span>); &#125;</div><div class="line"><span class="comment">/// @brief Deprecated legacy shape accessor width: use shape(3) instead.</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">width</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> LegacyShape(<span class="number">3</span>); &#125;</div></pre></td></tr></table></figure>
<h2 id="CPU和GPU中数据的获得"><a href="#CPU和GPU中数据的获得" class="headerlink" title="CPU和GPU中数据的获得"></a>CPU和GPU中数据的获得</h2><p>以下是一些get和set函数</p>
<ul>
<li><code>const Dtype* cpu_data() const;</code></li>
<li><code>void set_cpu_data(Dtype* data);</code></li>
<li><code>const int* gpu_shape() const;</code></li>
<li><code>const Dtype* gpu_data() const;</code></li>
<li><code>const Dtype* cpu_diff() const;</code></li>
<li><code>const Dtype* gpu_diff() const;</code></li>
<li><code>Dtype* mutable_cpu_data();</code></li>
<li><code>Dtype* mutable_gpu_data();</code></li>
<li><code>Dtype* mutable_cpu_diff();</code></li>
<li><code>Dtype* mutable_gpu_diff();</code></li>
</ul>
<h3 id="cpu-data"><a href="#cpu-data" class="headerlink" title="cpu_data()"></a><code>cpu_data()</code></h3><p>返回数据<code>data_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="set-cpu-data"><a href="#set-cpu-data" class="headerlink" title="set_cpu_data()"></a><code>set_cpu_data()</code></h3><p>设定<code>data_</code>在cpu中的数据，直接用指针替换的方式</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) &#123;</div><div class="line">  CHECK(data);</div><div class="line">  data_-&gt;set_cpu_data(data); <span class="comment">//在Syscedmem.cpp中实现，用cpu_ptr_ = data实现替换</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="gpu-shape"><a href="#gpu-shape" class="headerlink" title="gpu_shape"></a><code>gpu_shape</code></h3><p>返回的在gpu中存储的shape_data_，即gpu中存储的shape形状变量</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span>* Blob&lt;Dtype&gt;::gpu_shape() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(shape_data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">int</span>*)shape_data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="gpu-data"><a href="#gpu-data" class="headerlink" title="gpu_data()"></a><code>gpu_data()</code></h3><p>返回数据<code>data_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)data_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="cpu-diff"><a href="#cpu-diff" class="headerlink" title="cpu_diff()"></a><code>cpu_diff()</code></h3><p>返回梯度<code>diff_</code>在cpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::cpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;cpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="gpu-diff"><a href="#gpu-diff" class="headerlink" title="gpu_diff()"></a><code>gpu_diff()</code></h3><p>返回梯度<code>diff_</code>在gpu内存中的地址指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">const</span> Dtype* Blob&lt;Dtype&gt;::gpu_diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> Dtype*)diff_-&gt;gpu_data();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="mutable版本"><a href="#mutable版本" class="headerlink" title="mutable版本"></a>mutable版本</h3><p>下面这4个函数与上面类似，不同之处在于mutable，可以对其进行修改，而上面的函数返回形式是const，不可修改的</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="具体offset位置处的访问"><a href="#具体offset位置处的访问" class="headerlink" title="具体offset位置处的访问"></a>具体offset位置处的访问</h2><p>先需要用<code>offset()</code>函数计算具体的位置index，然后对<code>data_</code>和<code>diff_</code>具体index处进行访问,主要的函数有</p>
<ul>
<li><code>offset(const int n, const int c = 0, const int h = 0, const int w = 0)</code></li>
<li><code>offset(const vector&lt;int&gt;&amp; indices)</code></li>
<li><code>data_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>diff_at(const int n, const int c, const int h, const int w)</code></li>
<li><code>data_at(const vector&lt;int&gt;&amp; index)</code></li>
<li><code>diff_at(const vector&lt;int&gt;&amp; index)</code></li>
</ul>
<h3 id="offset"><a href="#offset" class="headerlink" title="offset()"></a><code>offset()</code></h3><p><code>offset()</code>函数有两个实现的版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c = <span class="number">0</span>, <span class="keyword">const</span> <span class="keyword">int</span> h = <span class="number">0</span>,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w = <span class="number">0</span>) <span class="keyword">const</span> &#123;</div><div class="line">  CHECK_GE(n, <span class="number">0</span>);</div><div class="line">  CHECK_LE(n, num());</div><div class="line">  CHECK_GE(channels(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(c, channels());</div><div class="line">  CHECK_GE(height(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(h, height());</div><div class="line">  CHECK_GE(width(), <span class="number">0</span>);</div><div class="line">  CHECK_LE(w, width());</div><div class="line">  <span class="keyword">return</span> ((n * channels() + c) * height() + h) * width() + w;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//用[n,c,h,w]的vector向量实现</span></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">offset</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; indices)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  CHECK_LE(indices.size(), num_axes());</div><div class="line">  <span class="keyword">int</span> offset = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_axes(); ++i) &#123;</div><div class="line">    offset *= shape(i);</div><div class="line">    <span class="keyword">if</span> (indices.size() &gt; i) &#123;</div><div class="line">      CHECK_GE(indices[i], <span class="number">0</span>);</div><div class="line">      CHECK_LT(indices[i], shape(i));</div><div class="line">      offset += indices[i]; </div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> offset;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="data-at-和diff-data-函数的访问"><a href="#data-at-和diff-data-函数的访问" class="headerlink" title="data_at()和diff_data()函数的访问"></a><code>data_at()</code>和<code>diff_data()</code>函数的访问</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">int</span> c, <span class="keyword">const</span> <span class="keyword">int</span> h,</span></span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> w) <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(n, c, h, w)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">data_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_data()[offset(index)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> Dtype <span class="title">diff_at</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index)</span> <span class="keyword">const</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> cpu_diff()[offset(index)];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="data-和diff-指针"><a href="#data-和diff-指针" class="headerlink" title="data_和diff_指针"></a><code>data_</code>和<code>diff_</code>指针</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; data() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(data_);</div><div class="line">  <span class="keyword">return</span> data_;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">inline</span> <span class="keyword">const</span> <span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt;&amp; diff() <span class="keyword">const</span> &#123;</div><div class="line">  CHECK(diff_);</div><div class="line">  <span class="keyword">return</span> diff_;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="data-的更新"><a href="#data-的更新" class="headerlink" title="data_的更新"></a><code>data_</code>的更新</h2><p>对于<code>data_</code>的更新，一般是对其减去反向传播的<code>diff_</code>乘以相应的系数，这里主要用到的函数有</p>
<ul>
<li><code>void Update()</code>: 用来对<code>data_</code>进行更新</li>
<li><code>Dtype asum_data() const</code>: 对<code>data_</code>求绝对值之和，即L1范数</li>
<li><code>Dtype asum_diff() const</code>: 对<code>diff_</code>求L1范数</li>
<li><code>Dtype sumsq_data() const</code>:对<code>data_</code>求平方和之和，即L2范数</li>
<li><code>Dtype sumsq_diff() const</code>:对<code>diff_</code>求L2范数</li>
<li><code>void scale_data(Dtype scale_factor)</code>:对<code>data_</code>乘以相应的标量</li>
<li><code>void scale_diff(Dtype scale_factor)</code>:对<code>diff_</code>乘以相应的标量</li>
</ul>
<h3 id="Update-方法"><a href="#Update-方法" class="headerlink" title="Update()方法"></a><code>Update()</code>方法</h3><p><code>Updata()</code>方法组要是用来对<code>data_</code>进行<code>diff_</code>的更新，主要是封装了cblas和cublas中的版本，其中里面分别有针对<code>float</code>和<code>double</code>版本的,因此没有实现<code>int</code>版本和<code>unsigned int</code>版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::Update() &#123; NOT_IMPLEMENTED; &#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::Update() &#123;</div><div class="line">  <span class="comment">// We will perform update based on where the data is located.</span></div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:  <span class="comment">//对于cpu中的数据,调用caffe_axpy()</span></div><div class="line">    <span class="comment">// perform computation on CPU</span></div><div class="line">    caffe_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;cpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY                     </span></div><div class="line">    <span class="comment">// perform computation on GPU  //对于gpu中的数据调用caffe_gpu_axpy()</span></div><div class="line">    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(<span class="number">-1</span>),</div><div class="line">        <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> Dtype*&gt;(diff_-&gt;gpu_data()),</div><div class="line">        <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;     <span class="comment">//log报错                   </span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Syncedmem not initialized."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在<code>Update</code>函数中调用了<code>caffe_axpy</code>,分别封装了cpu实现版本<code>cblas_saxpy</code>,主要是调用了cblas中的函数；<br>gpu实现版本<code>caffe_gpu_axpy</code>，主要是调用了cublas中的函数。</p>
<p>两者的声明在/caffe/util/math_function.hpp中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_axpy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, <span class="keyword">const</span> Dtype* X,</span></span></div><div class="line">    Dtype* Y);</div></pre></td></tr></table></figure>
<p>cpu版本的实现只有一种，位于/caffe/util/math_function.cpp中</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123; cblas_saxpy(N, alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>); &#125;</div></pre></td></tr></table></figure>
<p>在<code>cblas_saxpy</code>函数中，N是这个向量的元素个数，在Blob中就是<code>count_</code>。alpha是X前面的系数，X,Y 是输入的矢量。其中的1和1分别是X,Y的步进，这里每个元素都要更新，所以是1。函数实现的功能是</p>
<p>\begin{equation}<br>Y=alpha * X+Y<br>\end{equation}</p>
<p>gpu版本的实现有两种,位于/caffe/util/math_function.cu中：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* X,</div><div class="line">    <span class="keyword">float</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_axpy&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">const</span> <span class="keyword">double</span>* X,</div><div class="line">    <span class="keyword">double</span>* Y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDaxpy(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>, Y, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="求L1范数"><a href="#求L1范数" class="headerlink" title="求L1范数"></a>求L1范数</h3><p><code>asum_data()</code>函数用来计算data数据的绝对值之和(L1范数),<code>asum_diff()</code>用来计算梯度数据diff的L1范数。<br>主要是调用了cpu版本的<code>caffe_cpu_asum</code>和gpu版本的<code>caffe_gpu_asum</code>，这两个函数同样分别对cblas和cublas中的函数进行了封装。</p>
<p><code>asum_data()</code>的实现如下，<code>asum_diff()</code>的实现类似，无非是将<code>data_</code>指针换成了<code>diff_</code>指针：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::asum_data() <span class="keyword">const</span> &#123;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    <span class="keyword">return</span> caffe_cpu_asum(count_, cpu_data());</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  &#123;</div><div class="line">    Dtype asum;</div><div class="line">    caffe_gpu_asum(count_, gpu_data(), &amp;asum);</div><div class="line">    <span class="keyword">return</span> asum;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中,<code>caffe_cpu_asum</code>和<code>caffe_gpu_asum</code>的声明如下，位于/caffe/util/math_function.hpp</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_asum</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, Dtype* y)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_asum</code>的实现在/caffe/util/math_function.cpp中,针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sasum(n, x, <span class="number">1</span>);<span class="comment">//用来计算向量x的和，共有n个元素，1是stride,每个元素都用所以取1</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_dasum(n, x, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_asum</code>的实现在/caffe/util/math_function.cu中，针对<code>float</code>和<code>double</code>有两个版本</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//gpu的计算使用cublas</span></div><div class="line"><span class="keyword">template</span> &lt;&gt; </div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">float</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_asum&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">double</span>* y) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDasum(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h3><p><code>sumq_data()</code>函数用来计算data数据的平方和(L2范数),<code>sumsq_diff()</code> 用来计算梯度数据diff的L2范数。<br>主要是调用了cpu版本的<code>caffe_cpu_dout</code>和gpu版本的<code>caffe_gpu_dot</code>函数。</p>
<p><code>sumq_data()</code>的实现如下，<code>sumsq_diff()</code>与之类似：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">unsigned</span> <span class="keyword">int</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">int</span> Blob&lt;<span class="keyword">int</span>&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Blob&lt;Dtype&gt;::sumsq_data() <span class="keyword">const</span> &#123;</div><div class="line">  Dtype sumsq;</div><div class="line">  <span class="keyword">const</span> Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = cpu_data();</div><div class="line">    sumsq = caffe_cpu_dot(count_, data, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = gpu_data();</div><div class="line">    caffe_gpu_dot(count_, data, data, &amp;sumsq); <span class="comment">//gpu版本</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> sumsq;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中<code>caffe_cpu_dout</code>和<code>caffe_gpu_dot</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span></span>;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_dot</code>的实现调用了</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> caffe_cpu_strided_dot(n, x, <span class="number">1</span>, y, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_cpu_strided_dot</code>是一个模板函数，对<code>cblas_sdot</code>和<code>cblas_ddot</code>进行了封装，其声明和实现分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function">Dtype <span class="title">caffe_cpu_strided_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</span></span></div><div class="line">    <span class="keyword">const</span> Dtype* y, <span class="keyword">const</span> <span class="keyword">int</span> incy);</div><div class="line">    </div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">float</span> caffe_cpu_strided_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">int</span> incx,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_sdot(n, x, incx, y, incy);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">double</span> caffe_cpu_strided_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x,</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> incx, <span class="keyword">const</span> <span class="keyword">double</span>* y, <span class="keyword">const</span> <span class="keyword">int</span> incy) &#123;</div><div class="line">  <span class="keyword">return</span> cblas_ddot(n, x, incx, y, incy);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_dot</code>是一个模板函数，它的声明如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_dot</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> Dtype* x, <span class="keyword">const</span> Dtype* y, Dtype* out)</span></span>;</div></pre></td></tr></table></figure>
<p>该函数有<code>float</code>和<code>double</code>两个实现版本,位于math_functions.cu中，实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* x, <span class="keyword">const</span> <span class="keyword">float</span>* y,</div><div class="line">    <span class="keyword">float</span>* out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_dot&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">double</span>* x, <span class="keyword">const</span> <span class="keyword">double</span>* y,</div><div class="line">    <span class="keyword">double</span> * out) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDdot(Caffe::cublas_handle(), n, x, <span class="number">1</span>, y, <span class="number">1</span>, out));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="scale-data和scale-diff"><a href="#scale-data和scale-diff" class="headerlink" title="scale_data和scale_diff"></a><code>scale_data</code>和<code>scale_diff</code></h3><p><code>scale_data</code>函数和<code>scale_diff</code>函数主要是对Blob内的<code>data_</code>向量或者<code>diff_</code>向量乘以一个标量。<br>主要调用的两个函数cpu版本的<code>caffe_scal()</code>和gpu版本的<code>caffe_gpu_scal()</code></p>
<p><code>scale_data</code>的实现如下,<code>scale_diff</code>与之类似。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>&gt;::scale_data(<span class="keyword">unsigned</span> <span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt; <span class="keyword">void</span> Blob&lt;<span class="keyword">int</span>&gt;::scale_data(<span class="keyword">int</span> scale_factor) &#123;</div><div class="line">  NOT_IMPLEMENTED;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) &#123;</div><div class="line">  Dtype* data;</div><div class="line">  <span class="keyword">if</span> (!data_) &#123; <span class="keyword">return</span>; &#125;</div><div class="line">  <span class="keyword">switch</span> (data_-&gt;head()) &#123;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_CPU:</div><div class="line">    data = mutable_cpu_data();</div><div class="line">    caffe_scal(count_, scale_factor, data); <span class="comment">//cpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">case</span> SyncedMemory::HEAD_AT_GPU:</div><div class="line">  <span class="keyword">case</span> SyncedMemory::SYNCED:</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">    data = mutable_gpu_data();</div><div class="line">    caffe_gpu_scal(count_, scale_factor, data); <span class="comment">//gpu版本</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="keyword">case</span> SyncedMemory::UNINITIALIZED:</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown SyncedMemory head state: "</span> &lt;&lt; data_-&gt;head();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_scal</code>的模板函数声明为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p>对应的<code>float</code>和<code>double</code>实现版本如下，分别是对<code>cblas_sscal</code>和<code>cblas_dscal</code>函数的调用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  cblas_sscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  cblas_dscal(N, alpha, X, <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_gpu_scal</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype *X)</span></span>;</div></pre></td></tr></table></figure>
<p><code>caffe_gpu_scal()</code>的实现同样有两个版本：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasSscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;&gt;</div><div class="line"><span class="keyword">void</span> caffe_gpu_scal&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span> *X) &#123;</div><div class="line">  CUBLAS_CHECK(cublasDscal(Caffe::cublas_handle(), N, &amp;alpha, X, <span class="number">1</span>));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="其他的一些函数"><a href="#其他的一些函数" class="headerlink" title="其他的一些函数"></a>其他的一些函数</h2><ul>
<li><code>ShareData</code></li>
<li><code>ShareDiff</code></li>
<li><code>ShapeEquals</code></li>
<li><code>CopyFrom</code></li>
<li><code>FromProto</code></li>
<li><code>ToProto</code></li>
</ul>
<h3 id="ShareData和ShareDiff"><a href="#ShareData和ShareDiff" class="headerlink" title="ShareData和ShareDiff"></a><code>ShareData</code>和<code>ShareDiff</code></h3><p><code>ShareData</code>和<code>ShareDiff</code>实现方式是直接将<code>data_</code>和<code>diff_</code>的指针替换成其他类中的指针。这可以简化Layer中前向传递时只是简单的copy的情况。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareData(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  data_ = other.data();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::ShareDiff(<span class="keyword">const</span> Blob&amp; other) &#123;</div><div class="line">  CHECK_EQ(count_, other.count());</div><div class="line">  diff_ = other.diff();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ShapeEquals"><a href="#ShapeEquals" class="headerlink" title="ShapeEquals"></a><code>ShapeEquals</code></h3><p>Blob的shape是否相同的检查</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">bool</span> Blob&lt;Dtype&gt;::ShapeEquals(<span class="keyword">const</span> BlobProto&amp; other) &#123;</div><div class="line">  <span class="keyword">if</span> (other.has_num() || other.has_channels() ||</div><div class="line">      other.has_height() || other.has_width()) &#123;</div><div class="line">    <span class="comment">// Using deprecated 4D Blob dimensions --</span></div><div class="line">    <span class="comment">// shape is (num, channels, height, width).</span></div><div class="line">    <span class="comment">// Note: we do not use the normal Blob::num(), Blob::channels(), etc.</span></div><div class="line">    <span class="comment">// methods as these index from the beginning of the blob shape, where legacy</span></div><div class="line">    <span class="comment">// parameter blobs were indexed from the end of the blob shape (e.g., bias</span></div><div class="line">    <span class="comment">// Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).</span></div><div class="line">    <span class="keyword">return</span> shape_.size() &lt;= <span class="number">4</span> &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-4</span>) == other.num() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-3</span>) == other.channels() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-2</span>) == other.height() &amp;&amp;</div><div class="line">           LegacyShape(<span class="number">-1</span>) == other.width();</div><div class="line">  &#125;</div><div class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; other_shape(other.shape().dim_size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; other.shape().dim_size(); ++i) &#123;</div><div class="line">    other_shape[i] = other.shape().dim(i);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> shape_ == other_shape;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="CopyFrom"><a href="#CopyFrom" class="headerlink" title="CopyFrom"></a><code>CopyFrom</code></h3><p><code>CopyFrom</code>的声明如下，其中<code>copy_diff</code>为<code>false</code>,则拷贝的是data,否则拷贝diff</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyFrom</span><span class="params">(<span class="keyword">const</span> Blob&lt;Dtype&gt;&amp; source, <span class="keyword">bool</span> copy_diff = <span class="literal">false</span>,</span></span></div><div class="line">    <span class="keyword">bool</span> reshape = <span class="literal">false</span>);</div></pre></td></tr></table></figure>
<p>实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Blob&lt;Dtype&gt;::CopyFrom(<span class="keyword">const</span> Blob&amp; source, <span class="keyword">bool</span> copy_diff, <span class="keyword">bool</span> reshape) &#123;</div><div class="line">  <span class="comment">//先做size检查</span></div><div class="line">  <span class="keyword">if</span> (source.count() != count_ || source.shape() != shape_) &#123;</div><div class="line">    <span class="keyword">if</span> (reshape) &#123;</div><div class="line">      ReshapeLike(source);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Trying to copy blobs of different sizes."</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">switch</span> (Caffe::mode()) &#123;</div><div class="line">  <span class="keyword">case</span> Caffe::GPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.gpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.gpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> Caffe::CPU:</div><div class="line">    <span class="keyword">if</span> (copy_diff) &#123;</div><div class="line">      caffe_copy(count_, source.cpu_diff(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      caffe_copy(count_, source.cpu_data(),</div><div class="line">          <span class="keyword">static_cast</span>&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">default</span>:</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Unknown caffe mode."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中，<code>caffe_copy</code>封装了cpu内存之间的内存copy和gpu内存的copy</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_copy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (X != Y) &#123;</div><div class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">      <span class="comment">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></div><div class="line">      CUDA_CHECK(cudaMemcpy(Y, X, <span class="keyword">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">      NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">memcpy</span>(Y, X, <span class="keyword">sizeof</span>(Dtype) * N);  <span class="comment">// NOLINT(caffe/alt_fn)</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="FromProto和ToProto"><a href="#FromProto和ToProto" class="headerlink" title="FromProto和ToProto"></a><code>FromProto</code>和<code>ToProto</code></h3><p><code>FromProto</code>用proto文件来实现Blob的初始化,<code>ToProto</code>是将Blob的内容写入到proto文件</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总而言之，Blob相对而言是一个比较简单的类，在看懂<code>SyncedMemory</code>以后，就比较容易看懂这个类了，里面的各种调用数学函数的声明都是在/caffe/util/math_functions.hpp中，实现分别是在math_functions.cpp和math_functions.cu中。</p>
<p>在类的最后可以看到 <code>DISABLE_COPY_AND_ASSIGN(Blob)</code>,这是一个宏函数，可以看到它的实现,主要是禁止这个类的拷贝和赋值操作,是为了防止两个大型数据结构内容的复制和赋值。如果要使用，应该是使用指针和引用来指向。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Disable the copy and assignment operator for a class.</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> DISABLE_COPY_AND_ASSIGN(classname) \</span></div><div class="line">private:\</div><div class="line">  classname(const classname&amp;);\</div><div class="line">  classname&amp; operator=(const classname&amp;)</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/06/12/Caffe解读3-SyncedMemory/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="John Doe">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Keson's blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Keson's blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/12/Caffe解读3-SyncedMemory/" itemprop="url">
                  Caffe解读3 -- SyncedMemory
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-06-12T12:50:13+08:00">
              2016-06-12
            </time>

            &nbsp;|&nbsp;

            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-check-o"></i>
            </span>
            <time title="Post modified" itemprop="dateModified" datetime="2017-03-06T19:00:07+08:00">
              2017-03-06
            </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/12/Caffe解读3-SyncedMemory/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="http://yoursite.com/2016/06/12/Caffe解读3-SyncedMemory/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          



          
          
             <span id="/2016/06/12/Caffe解读3-SyncedMemory/" class="leancloud_visitors" data-flag-title="Caffe解读3 -- SyncedMemory">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>首先推荐一个阅读Caffe代码结构的网站<br><a href="http://caffe.berkeleyvision.org/doxygen" target="_blank" rel="external">http://caffe.berkeleyvision.org/doxygen</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>这个类主要用来做内存的分配和同步，代码量较少，包含<code>syncedmem.hpp</code> 和 <code>syncedmem.cpp</code>，这个类相对比较简单易懂~  但是也很重要，因为牵扯到了最底层的cpu和gpu数据的同步等问题。</p>
<p>首先简单介绍一下Pinned Memory和Non-Pinned Memory,详细的介绍可以参见我上一篇博客<a href="https://satisfie.github.io/2016/06/06/Caffe%E8%A7%A3%E8%AF%BB1-Pinned-Memory-Vs-Non-Pinned-Memory/" target="_blank" rel="external">Caffe解读1-Pinned-Memory-Vs-Non-Pinned-Memory/</a></p>
<p>为了在CPU和GPU之间传输内存，关于CPU的内存分配和释放方式有以下两种：</p>
<ul>
<li>通过C标准库中的<code>malloc</code>函数完成内存分配，<code>free</code>进行内存释放</li>
<li>调用CUDA中的<code>cudaMallocHost</code>函数完成内存分配，<code>cudaFreeHost</code>进行内存释放</li>
</ul>
<p><code>malloc</code>和<code>free</code>的优点是分配和释放的耗时少，缺点是CPU和GPU之间的传输相比而言比较慢。<code>cudaMallocHost</code>和<code>cudaFreeHost</code>正好相反，优点是CPU和GPU之间的传输快，缺点是分配和释放内存比较耗时。</p>
<p>在Caffe中，让在GPU模式和CUDA可用的情况下，采用Pinned Memory方式，即使用<code>cudaMallocHost</code>和<code>cudaFreeHost</code>来进行CPU内存的分配和释放。这在单个GPU的时候可能效果并不明显，但是更利于并行训练，更重要的是，在多个GPU上，这种方式更加稳定。</p>
<h2 id="头文件syncedmem-hpp"><a href="#头文件syncedmem-hpp" class="headerlink" title="头文件syncedmem.hpp"></a>头文件<code>syncedmem.hpp</code></h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line">#ifndef CAFFE_SYNCEDMEM_HPP_</div><div class="line">#define CAFFE_SYNCEDMEM_HPP_</div><div class="line"></div><div class="line">#include &lt;cstdlib&gt;</div><div class="line"></div><div class="line">#include "caffe/common.hpp"</div><div class="line"></div><div class="line">namespace caffe &#123;</div><div class="line"></div><div class="line">// If CUDA is available and in GPU mode, host memory will be allocated pinned,</div><div class="line">// using cudaMallocHost. It avoids dynamic pinning for transfers (DMA).</div><div class="line">// The improvement in performance seems negligible in the single GPU case,</div><div class="line">// but might be more significant for parallel training. Most importantly,</div><div class="line">// it improved stability for large models on many GPUs.</div><div class="line">inline void CaffeMallocHost(void** ptr, size_t size, bool* use_cuda) &#123;</div><div class="line">#ifndef CPU_ONLY</div><div class="line">  if (Caffe::mode() == Caffe::GPU) &#123;</div><div class="line">    CUDA_CHECK(cudaMallocHost(ptr, size));</div><div class="line">    *use_cuda = true;</div><div class="line">    return;</div><div class="line">  &#125;</div><div class="line">#endif</div><div class="line">  *ptr = malloc(size);</div><div class="line">  *use_cuda = false;</div><div class="line">  CHECK(*ptr) &lt;&lt; "host allocation of size " &lt;&lt; size &lt;&lt; " failed";</div><div class="line">&#125;</div><div class="line"></div><div class="line">inline void CaffeFreeHost(void* ptr, bool use_cuda) &#123;</div><div class="line">#ifndef CPU_ONLY</div><div class="line">  if (use_cuda) &#123;</div><div class="line">    CUDA_CHECK(cudaFreeHost(ptr));</div><div class="line">    return;</div><div class="line">  &#125;</div><div class="line">#endif</div><div class="line">  free(ptr);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">/**</div><div class="line"> * @brief Manages memory allocation and synchronization between the host (CPU)</div><div class="line"> *        and device (GPU).</div><div class="line"> *</div><div class="line"> * TODO(dox): more thorough description.</div><div class="line"> */</div><div class="line">class SyncedMemory &#123;</div><div class="line"> public:</div><div class="line">  SyncedMemory()</div><div class="line">      : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(0), head_(UNINITIALIZED),</div><div class="line">        own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">        gpu_device_(-1) &#123;&#125;</div><div class="line">  explicit SyncedMemory(size_t size)</div><div class="line">      : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(size), head_(UNINITIALIZED),</div><div class="line">        own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">        gpu_device_(-1) &#123;&#125;</div><div class="line">  ~SyncedMemory();</div><div class="line">  </div><div class="line">  const void* cpu_data(); //获取cpu数据指针</div><div class="line">  void set_cpu_data(void* data); //设置cpu数据</div><div class="line">  const void* gpu_data();   //获得gpu数据指针</div><div class="line">  void set_gpu_data(void* data);  //设置gpu数据</div><div class="line">  void* mutable_cpu_data();  //获取可以更改cpu数据的指针</div><div class="line">  void* mutable_gpu_data();  //获取可以更改gpu数据的指针</div><div class="line">  enum SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</div><div class="line">  SyncedHead head() &#123; return head_; &#125;</div><div class="line">  size_t size() &#123; return size_; &#125;</div><div class="line"></div><div class="line">#ifndef CPU_ONLY</div><div class="line">  void async_gpu_push(const cudaStream_t&amp; stream);</div><div class="line">#endif</div><div class="line"></div><div class="line"> private:</div><div class="line">  void to_cpu();   //数据状态转移函数</div><div class="line">  void to_gpu();   //数据状态转移函数</div><div class="line">  void* cpu_ptr_;  //cpu数据内存指针</div><div class="line">  void* gpu_ptr_;  //gpu数据内存指针</div><div class="line">  size_t size_;    //size</div><div class="line">  SyncedHead head_; //用来指明状态</div><div class="line">  bool own_cpu_data_; //共享标记，是否使用的是自己的cpu数据</div><div class="line">  bool cpu_malloc_use_cuda_;</div><div class="line">  bool own_gpu_data_; //共享标记，是否使用的是自己的gpu数据</div><div class="line">  int gpu_device_;</div><div class="line"></div><div class="line">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</div><div class="line">&#125;;  // class SyncedMemory</div><div class="line"></div><div class="line">&#125;  // namespace caffe</div><div class="line"></div><div class="line">#endif  // CAFFE_SYNCEDMEM_HPP_</div></pre></td></tr></table></figure>
<h2 id="CPU内存的分配和释放"><a href="#CPU内存的分配和释放" class="headerlink" title="CPU内存的分配和释放"></a>CPU内存的分配和释放</h2><p>进行了最简单的封装</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeMallocHost</span><span class="params">(<span class="keyword">void</span>** ptr, <span class="keyword">size_t</span> size, <span class="keyword">bool</span>* use_cuda)</span> </span>&#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123; <span class="comment">//GPU模式下使用cudaMallocHost</span></div><div class="line">    CUDA_CHECK(cudaMallocHost(ptr, size));</div><div class="line">    *use_cuda = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  *ptr = <span class="built_in">malloc</span>(size);    <span class="comment">//CPU模式下使用malloc</span></div><div class="line">  *use_cuda = <span class="literal">false</span>;</div><div class="line">  CHECK(*ptr) &lt;&lt; <span class="string">"host allocation of size "</span> &lt;&lt; size &lt;&lt; <span class="string">" failed"</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">CaffeFreeHost</span><span class="params">(<span class="keyword">void</span>* ptr, <span class="keyword">bool</span> use_cuda)</span> </span>&#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY         <span class="comment">//GPU模式下使用cudaFreeHost</span></span></div><div class="line">  <span class="keyword">if</span> (use_cuda) &#123;</div><div class="line">    CUDA_CHECK(cudaFreeHost(ptr));</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  <span class="built_in">free</span>(ptr);            <span class="comment">//CPU模式下使用free</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="SyncedMemory类的私有成员变量"><a href="#SyncedMemory类的私有成员变量" class="headerlink" title="SyncedMemory类的私有成员变量"></a><code>SyncedMemory</code>类的私有成员变量</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span>* cpu_ptr_;   <span class="comment">//数据在cpu的指针</span></div><div class="line"><span class="keyword">void</span>* gpu_ptr_;   <span class="comment">//数据在gpu的指针</span></div><div class="line"><span class="keyword">size_t</span> size_;     <span class="comment">//数据的大小</span></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"> *用来表示数据的同步状态，有4种状态，分别是未初始化的，数据在cpu中，数据在gpu中，cpu和gpu都有。</div><div class="line"> * /</div><div class="line">enum SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;; </div><div class="line">SyncedHead head_;     //用来指明状态</div><div class="line">bool own_cpu_data_;   //是否是自己的cpu数据</div><div class="line">bool cpu_malloc_use_cuda_; //</div><div class="line">bool own_gpu_data_;   //是否有gpu数据</div><div class="line">int  gpu_device_;     //gpu卡的标志</div></pre></td></tr></table></figure>
<h2 id="CPU-GPU-内存的状态"><a href="#CPU-GPU-内存的状态" class="headerlink" title="CPU/GPU 内存的状态"></a>CPU/GPU 内存的状态</h2><p><code>enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };</code></p>
<ul>
<li><p><code>UNINITIALIZED</code><br>这个状态是未初始化状态，也就是SyncedMemory最早的状态，这时候内存和显存都没有被分配，当cpu或者gpu申请内存时该状态终结。</p>
</li>
<li><p><code>HEAD_AT_CPU</code><br>这个状态表明最近一次数据修改是由cpu引起的。此时cpu和gpu的数据还没有同步，也就是cpu和gpu的数据可能不同。</p>
</li>
<li><p><code>HEAD_AT_GPU</code><br>这个状态表明最近一次数据修改是由gpu引起的。此时cpu和gpu的数据还没有同步，也就是cpu和gpu的数据可能不同。</p>
</li>
<li><p><code>SYNCED</code><br>同步状态，这个状态表明此时cpu和gpu的数据一致。这个状态可以使得CPU和GPU数据相同时减少不必要的复制。</p>
</li>
</ul>
<h1 id="具体的状态转移和实现函数"><a href="#具体的状态转移和实现函数" class="headerlink" title="具体的状态转移和实现函数"></a>具体的状态转移和实现函数</h1><h2 id="构造函数和析构函数："><a href="#构造函数和析构函数：" class="headerlink" title="构造函数和析构函数："></a>构造函数和析构函数：</h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">SyncedMemory()</div><div class="line">    : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(0), head_(UNINITIALIZED),</div><div class="line">      own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">      gpu_device_(-1) &#123;&#125;</div><div class="line">explicit SyncedMemory(size_t size)</div><div class="line">    : cpu_ptr_(NULL), gpu_ptr_(NULL), size_(size), head_(UNINITIALIZED),</div><div class="line">      own_cpu_data_(false), cpu_malloc_use_cuda_(false), own_gpu_data_(false),</div><div class="line">      gpu_device_(-1) &#123;&#125;</div><div class="line">~SyncedMemory();</div></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SyncedMemory::~SyncedMemory() &#123;</div><div class="line">  <span class="keyword">if</span> (cpu_ptr_ &amp;&amp; own_cpu_data_) &#123;</div><div class="line">    CaffeFreeHost(cpu_ptr_, cpu_malloc_use_cuda_);</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">if</span> (gpu_ptr_ &amp;&amp; own_gpu_data_) &#123;</div><div class="line">    <span class="keyword">int</span> initial_device;</div><div class="line">    cudaGetDevice(&amp;initial_device);</div><div class="line">    <span class="keyword">if</span> (gpu_device_ != <span class="number">-1</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaSetDevice(gpu_device_));</div><div class="line">    &#125;</div><div class="line">    CUDA_CHECK(cudaFree(gpu_ptr_));</div><div class="line">    cudaSetDevice(initial_device);</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// CPU_ONLY</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="两个内敛私有函数to-cpu-和to-gpu"><a href="#两个内敛私有函数to-cpu-和to-gpu" class="headerlink" title="两个内敛私有函数to_cpu()和to_gpu()"></a>两个内敛私有函数<code>to_cpu()</code>和<code>to_gpu()</code></h3><p><code>to_cpu()</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_cpu() &#123;</div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED: <span class="comment">//如果是未初始化状态，则进行cpu内存分配</span></div><div class="line">    CaffeMallocHost(&amp;cpu_ptr_, size_, &amp;cpu_malloc_use_cuda_);</div><div class="line">    caffe_memset(size_, <span class="number">0</span>, cpu_ptr_);</div><div class="line">    head_ = HEAD_AT_CPU;   <span class="comment">//设置状态为“数据在cpu”</span></div><div class="line">    own_cpu_data_ = <span class="literal">true</span>;  <span class="comment">//cpu拥有数据置为真</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:  <span class="comment">//如果是gpu拥有数据 </span></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY      <span class="comment">//如果有GPU存在</span></span></div><div class="line">    <span class="keyword">if</span> (cpu_ptr_ == <span class="literal">NULL</span>) &#123; <span class="comment">//如果cpu还未分配内存空间，则进行分配</span></div><div class="line">      CaffeMallocHost(&amp;cpu_ptr_, size_, &amp;cpu_malloc_use_cuda_);</div><div class="line">      own_cpu_data_ = <span class="literal">true</span>; <span class="comment">//cpu拥有数据置为真</span></div><div class="line">    &#125;</div><div class="line">    caffe_gpu_memcpy(size_, gpu_ptr_, cpu_ptr_); <span class="comment">//内存从gpu拷贝到cpu</span></div><div class="line">    head_ = SYNCED;         <span class="comment">//设置状态为"数据在cpu和gpu都拥有"</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    NO_GPU;       <span class="comment">//否则log报错，这是宏定义</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:     <span class="comment">//CPU 已经拥有数据，则直接跳过</span></div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>to_gpu()</code>:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">inline</span> <span class="keyword">void</span> SyncedMemory::to_gpu() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">switch</span> (head_) &#123;</div><div class="line">  <span class="keyword">case</span> UNINITIALIZED:                          <span class="comment">//如果状态是未初始化</span></div><div class="line">    CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); <span class="comment">//分配GPU显存</span></div><div class="line">    caffe_gpu_memset(size_, <span class="number">0</span>, gpu_ptr_);</div><div class="line">    head_ = HEAD_AT_GPU;                      <span class="comment">//设置状态为在GPU</span></div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_CPU:                           <span class="comment">//如果状态是在CPU</span></div><div class="line">    <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); <span class="comment">//分配GPU显存  </span></div><div class="line">      own_gpu_data_ = <span class="literal">true</span>;            </div><div class="line">    &#125;</div><div class="line">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);   <span class="comment">//将数据从CPU传输到GPU</span></div><div class="line">    head_ = SYNCED;   						           <span class="comment">//状态设置为CPU和GPU同步</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">case</span> HEAD_AT_GPU:                            <span class="comment">//如果GPU已经拥有数据，则跳过</span></div><div class="line">  <span class="keyword">case</span> SYNCED:</div><div class="line">    <span class="keyword">break</span>;</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;                         <span class="comment">//如果没有GPU，则log报错</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="获取cpu和gpu内存地址的方法"><a href="#获取cpu和gpu内存地址的方法" class="headerlink" title="获取cpu和gpu内存地址的方法"></a>获取cpu和gpu内存地址的方法</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</div></pre></td></tr></table></figure>
<p>实现分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">void</span>* SyncedMemory::cpu_data() &#123;</div><div class="line">  to_cpu();        <span class="comment">//保证cpu内存存在数据</span></div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">void</span>*)cpu_ptr_; <span class="comment">//返回数据地址指针</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">void</span>* SyncedMemory::gpu_data() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY     <span class="comment">//如果是GPU模式</span></span></div><div class="line">  to_gpu();          <span class="comment">//保证GPU内存中存在数据</span></div><div class="line">  <span class="keyword">return</span> (<span class="keyword">const</span> <span class="keyword">void</span>*)gpu_ptr_;    <span class="comment">//返回GPU中数据地址指针</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;            <span class="comment">//GPU不存在则log报错</span></div><div class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;      </div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="mutable-cpu-data-和mutable-gpu-data"><a href="#mutable-cpu-data-和mutable-gpu-data" class="headerlink" title="mutable_cpu_data()和mutable_gpu_data()"></a><code>mutable_cpu_data()</code>和<code>mutable_gpu_data()</code></h2><p>与上面两个函数类似,多了状态的设定和返回可以修改数据的指针</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span>* SyncedMemory::mutable_cpu_data() &#123;</div><div class="line">  to_cpu();</div><div class="line">  head_ = HEAD_AT_CPU;      <span class="comment">//设置装填为数据在CPU</span></div><div class="line">  <span class="keyword">return</span> cpu_ptr_;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">void</span>* SyncedMemory::mutable_gpu_data() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  to_gpu();</div><div class="line">  head_ = HEAD_AT_GPU;     <span class="comment">//设置状态为数据在GPU</span></div><div class="line">  <span class="keyword">return</span> gpu_ptr_;</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;</div><div class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="set-cpu-data-void-data-和set-gpu-data-void-data"><a href="#set-cpu-data-void-data-和set-gpu-data-void-data" class="headerlink" title="set_cpu_data(void* data)和set_gpu_data(void* data)"></a><code>set_cpu_data(void* data)</code>和<code>set_gpu_data(void* data)</code></h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">//cpu的data指针指向一个新的区域由data指针传入，并且将原来申请的内存释放</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::set_cpu_data(<span class="keyword">void</span>* data) &#123;</div><div class="line">  CHECK(data);</div><div class="line">  <span class="keyword">if</span> (own_cpu_data_) &#123;       <span class="comment">//如果cpu内存有数据，则释放</span></div><div class="line">    CaffeFreeHost(cpu_ptr_, cpu_malloc_use_cuda_);</div><div class="line">  &#125;</div><div class="line">  cpu_ptr_ = data;         <span class="comment">//设置</span></div><div class="line">  head_ = HEAD_AT_CPU;     <span class="comment">//设置状态为数据在CPU</span></div><div class="line">  own_cpu_data_ = <span class="literal">false</span>;  <span class="comment">//表明当前使用的宿主的数据</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们可以看到<code>set_cpu_data</code>释放了当前的cpu内存，把指针指向data所指的内存中，<code>own_cpu_data_</code>设置为了false；表明当前使用的是宿主(data)的内存。</p>
<p>我们对<code>own_cpu_data_</code>进行标记是有必要的，因为当使用的是宿主的内存的时候，当这个类被释放而调用析构函数时，需要检查共享标记，不能释放宿主的内存。这样可以保证自己申请的内存只能由自己释放。</p>
<p>析构函数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">SyncedMemory::~SyncedMemory() &#123;</div><div class="line">  <span class="keyword">if</span> (cpu_ptr_ &amp;&amp; own_cpu_data_) &#123;</div><div class="line">    CaffeFreeHost(cpu_ptr_, cpu_malloc_use_cuda_);</div><div class="line">  &#125;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  <span class="keyword">if</span> (gpu_ptr_ &amp;&amp; own_gpu_data_) &#123;</div><div class="line">    <span class="keyword">int</span> initial_device;</div><div class="line">    cudaGetDevice(&amp;initial_device);</div><div class="line">    <span class="keyword">if</span> (gpu_device_ != <span class="number">-1</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaSetDevice(gpu_device_));</div><div class="line">    &#125;</div><div class="line">    CUDA_CHECK(cudaFree(gpu_ptr_));</div><div class="line">    cudaSetDevice(initial_device);</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// CPU_ONLY</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>set_gpu_data(void *data)</code></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::set_gpu_data(<span class="keyword">void</span>* data) &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  CHECK(data);</div><div class="line">  <span class="keyword">if</span> (own_gpu_data_) &#123;</div><div class="line">    <span class="keyword">int</span> initial_device;</div><div class="line">    cudaGetDevice(&amp;initial_device);</div><div class="line">    <span class="keyword">if</span> (gpu_device_ != <span class="number">-1</span>) &#123;</div><div class="line">      CUDA_CHECK(cudaSetDevice(gpu_device_));</div><div class="line">    &#125;</div><div class="line">    CUDA_CHECK(cudaFree(gpu_ptr_));</div><div class="line">    cudaSetDevice(initial_device);</div><div class="line">  &#125;</div><div class="line">  gpu_ptr_ = data;</div><div class="line">  head_ = HEAD_AT_GPU;</div><div class="line">  own_gpu_data_ = <span class="literal">false</span>;  </div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">  NO_GPU;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="async-gpu-push-const-cudaStream-t-amp-stream"><a href="#async-gpu-push-const-cudaStream-t-amp-stream" class="headerlink" title="async_gpu_push(const cudaStream_t&amp; stream)"></a><code>async_gpu_push(const cudaStream_t&amp; stream)</code></h2><p>最后还有一个异步传输的函数,cuda拷贝的异步传输，从数据从cpu拷贝到gpu,实现如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line"><span class="keyword">void</span> SyncedMemory::async_gpu_push(<span class="keyword">const</span> cudaStream_t&amp; stream) &#123;</div><div class="line">  CHECK(head_ == HEAD_AT_CPU);</div><div class="line">  <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</div><div class="line">    CUDA_CHECK(cudaGetDevice(&amp;gpu_device_));</div><div class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</div><div class="line">    own_gpu_data_ = <span class="literal">true</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">const</span> cudaMemcpyKind put = cudaMemcpyHostToDevice;</div><div class="line">  CUDA_CHECK(cudaMemcpyAsync(gpu_ptr_, cpu_ptr_, size_, put, stream));</div><div class="line">  <span class="comment">// Assume caller will synchronize on the stream before use</span></div><div class="line">  head_ = SYNCED;</div><div class="line">&#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="John Doe" />
          <p class="site-author-name" itemprop="name">John Doe</p>
          <p class="site-description motion-element" itemprop="description">Stay hungry</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>







        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
</html>
